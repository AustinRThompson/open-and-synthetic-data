---
title: "Open & Synthetic Data"
output:
  html_notebook:
    toc: yes
    toc_float: true
    toc_collapsed: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

# Packages
Here, we load the packages that we will use for data wrangling, as well as the `synthpop` package, which we will use to generate synthetic data.
```{r}
library(tidyverse) # data wrangling
library(synthpop) # install.packages('synthpop')
library(ggeasy) # install.packages('ggeasy')
library(janitor) # install.packages('janitor')
library(gtsummary) # install.packages('gtsummary')
library(labelled) # install.packages('labelled')
```

# Data Synthesis {.tabset}
## 01 - Swallowing
These data were reported in "Normative Reference Values for FEES and VASES: Preliminary Data From 39 Nondysphagic, Community-Dwelling Adults" by Curtis et al. (2023) (https://doi.org/10.1044/2023_JSLHR-23-00132) and were retrieved from https://osf.io/4anzm/.

### Loading Data
```{r}
#Load data
swallowing_data <-
  read.csv(here::here("Data/01_Swallowing/norms_ratings.csv")) |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables
  dplyr::select(c(study_id, bolus_consistency, laryngeal_vestibule_severity_rating)) |> 
  mutate(
    # create binary variable for 'absent' when 0 and present when > 0
    lv_zero = if_else(laryngeal_vestibule_severity_rating == 0, "Absent", "Present"),
    # create variables for 'amount of residue when present'
    lv_present = ifelse(
      laryngeal_vestibule_severity_rating > 0,
      laryngeal_vestibule_severity_rating,
      NA
    )
  )

# Visualize distribution
break_trans = scales::trans_new('clip_range',
                                \(x) ifelse(x <= 50, x, x - 350),
                                \(x) ifelse(x <= 50, x, x + 350))

ggplot(swallowing_data,
       aes(x = laryngeal_vestibule_severity_rating, fill = lv_zero, color = lv_zero)) +
  geom_histogram(bins = 105) +
  coord_trans(y = break_trans) +
  cowplot::theme_cowplot() +
  scale_x_continuous(limits = c(-1, 100),
                     breaks = seq(from = 0, to = 100, by = 5.0)) +
  scale_y_continuous(
    limits = c(-1, 450),
    breaks = c(
      0,
      10,
      20,
      30,
      40,
      50,
      405,
      425,
      450
    )
  ) +
  geom_hline(yintercept = 50,
             linetype = "solid",
             color = "dark grey") +
  scale_fill_manual(values = c("orange", "blue")) +
  scale_color_manual(values = c("white", "white")) +
  labs(x = "Laryngeal Vestibule Residue Rating (%)",
       y = "Count") +
  ggeasy::easy_remove_legend()

# Summarise distribution
## Percentage of 'present' laryngeal vestibule residue
### Thin (IDDSI 1)
swallowing_data |> 
  filter(bolus_consistency == "Thin (IDDSI 0)") |> 
  count(lv_zero) |> 
  mutate(total = sum(n)) |> 
  mutate(perc = (n/total)*100)

### Pudding & Cracker
swallowing_data |> 
  filter(bolus_consistency != "Thin (IDDSI 0)") |> 
  count(lv_zero) |> 
  mutate(total = sum(n)) |> 
  mutate(perc = (n/total)*100)

## Median and IQR for 'present' laryngeal vestibule residue
swallowing_data |> 
  filter(bolus_consistency == "Thin (IDDSI 0)" & lv_zero == "Present") |> 
  summarise(median = median(lv_present),
            IQR_high = quantile(lv_present, 0.75),
            IQR_low = quantile(lv_present, 0.25))
```

### Generating Synthetic Data

### Comparisons

## 02 - Articulation
These data were reported in "Vowel Acoustics as Predictors of Speech Intelligibility in Dysarthria" by Thompson et al. (2023) (https://doi.org/10.1044/2022_JSLHR-22-00287) and were retrieved from: https://osf.io/hr7aj/.

### Loading Data
```{r}
data_articulation <- rio::import(
  file = here::here("Data", "02_Articulation", "data_Acoustic Measures.csv")
) |>
  
  # Remove the reliability trials that have "_rel" in the SpeakerID variable
  dplyr::filter(
    !grepl(
      pattern = "_rel",
      x = SpeakerID
  )) |>
  
  # Selecting just the variables we need
  dplyr::select(
    SpeakerID,
    Sex,
    Etiology,
    VSA_b, # VSA in Bark
    Int = Int_OT # intelligibility (orthographic transcriptions)
  )

# Examine the relationship between VSA and intelligibility
model <- lm(Int ~ VSA_b,
            data = data_articulation)
summary(model)
confint(model, level = 0.95)
```

### Generating Synthetic Data

### Comparisons

## 03 - Fluency
These data were reported in "Hypernasality associated with basal ganglia dysfunction: Evidence from Parkinson's disease and Huntington's disease" by Novotny et al. (2016) (https://doi.org/10.7717/peerj.2530) and were retrieved from https://peerj.com/articles/2530/#supp-1.

### Loading Data
```{r}
library(effectsize)

# Load data
data_fluency <- rio::import(file = here::here("Data", "03_Fluency", "data_Dyslexia Stutter Master.csv")) |> 
  dplyr::filter(Rejection == 0) |> 
  dplyr::filter(Group != "AWD") |>
  
  # Selecting just the variables that we need
  dplyr::select(
    Subject,
    Age,
    Group,
    Nonwordrepetition
  )

data_fluency |> 
  dplyr::group_by(Group) |> 
  dplyr::summarise(
    N = length(Group),
    M = mean(Nonwordrepetition),
    SD = sd(Nonwordrepetition),
    SEM = sd(Nonwordrepetition) / sqrt(length(Nonwordrepetition)),
    CIneg = M - 1.96 * SEM,
    CIpos = M + 1.96 * SEM
  )

# Examine the difference between neurotypical and stuttering in non-word repetition
t_test <- stats::t.test(Nonwordrepetition ~ Group,
                        data = data_fluency,
                        paired = FALSE,
                        var.equal = FALSE)

delta <- effectsize::glass_delta(Nonwordrepetition ~ Group,
                                 data = data_fluency,
                                 correction = FALSE,
                                 ci = 0.95)

```

## 04 - Voice/Resonance
These data were reported in "Hypernasality associated with basal ganglia dysfunction: Evidence from Parkinson's disease and Huntington's disease" by Novotny et al. (2016) (https://doi.org/10.7717/peerj.2530) and were retrieved from https://peerj.com/articles/2530/#supp-1.

### Loading Data
```{r}
# Load data
voice_data <-
  readxl::read_excel(here::here("Data/04_Voice_Resonance/RawData.xlsx"), range = "B3:Z114") |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables
  dplyr::select(c(names, efn_sd_d_b, median_of_raters))

# Examine correlation between overall perceptual rating and acoustic EFn SD parameter
cor.test(voice_data$median_of_raters, voice_data$efn_sd_d_b, method = "pearson")
```

## 05 - Hearing
These data were reported in "Ubiquitous enhancement of spatial hearing in congenitally blind people" by Battal et al. (2020) (https://doi.org/10.31234/osf.io/veh7t) and were retrieved from https://osf.io/tav35/.

### Loading Data
```{r}
# Load packages
library(nlme) # LMM

# Load data
plane.thre.out <- read.table(here::here("Data/05_Hearing/All_Thresholds_outliersmarked-500B.txt"), sep=",", header=T)

# Convert variables to factors
names(plane.thre.out)[4]<-c('head')
plane.thre.out$arm <- factor(plane.thre.out$arm)
plane.thre.out$subj <- factor(plane.thre.out$subj)
plane.thre.out$head <- factor(plane.thre.out$head)
plane.thre.out$planes <- factor(plane.thre.out$planes)
plane.thre.out$group <- ifelse(plane.thre.out$group=='EB', c('CB'), c('SC'))
plane.thre.out$group <-factor(plane.thre.out$group)

#omit all +-3SD outliers 
plane.thre.out<- subset(plane.thre.out, outlier==0)
plane.thre.out[which(plane.thre.out$thre >12),]

#divide back
horizontal.thre.out <- subset(plane.thre.out,planes=='hor')
vertical.thre.out <- subset(plane.thre.out,planes=='ver')

rownames(horizontal.thre.out) <-c(1:nrow(horizontal.thre.out)) #correct the row numbers
horizontal.thre.out[which(horizontal.thre.out$thre >10),] # we know from glmm there's one outlier
horizontal.thre.out <-horizontal.thre.out[-c(188),] 

rownames(vertical.thre.out) <-c(1:nrow(vertical.thre.out))
vertical.thre.out[which(vertical.thre.out$thre >12),] # we know from glmm there's one outlier
vertical.thre.out <-vertical.thre.out[-c(144),] 

# in the horizontal back condition left arm is right area // right arm is left area of the subject
hor.thre.front <- subset(horizontal.thre.out, head ==1)
hor.thre.back <- subset(horizontal.thre.out, head ==2)

hor.thre.back$arm <-as.character(hor.thre.back$arm)
hor.thre.back <- within(hor.thre.back, arm[arm=="left"] <-("Alien"))#just to assign into a temp
hor.thre.back <- within(hor.thre.back, arm[arm=='right'] <- 'left')
hor.thre.back <- within(hor.thre.back, arm[arm=='Alien'] <- 'right')

horizontal.thre.out <-rbind(hor.thre.front,hor.thre.back)

# combine them back
plane.thre.out <- rbind(horizontal.thre.out,vertical.thre.out)
plane.thre.out$arm <- factor(plane.thre.out$arm)
plane.thre.out$subj <- factor(plane.thre.out$subj)
plane.thre.out$head <- factor(plane.thre.out$head)
plane.thre.out$planes <- factor(plane.thre.out$planes)
plane.thre.out$group <-factor(plane.thre.out$group)

#divide back
horizontal.thre.out <- subset(plane.thre.out,planes=='hor')
vertical.thre.out <- subset(plane.thre.out,planes=='ver')

# LMM on planes: Auditory localization performance by plane, group and position (head)
# this is the model itself - subject is the random fx
plane.out.nlme <- lme(thre ~ 1 + planes * group * head, random = ~ 1|subj, weights = varIdent(form = ~ 1 | planes), data = plane.thre.out)

# Results
my.plane.anova <- anova(plane.out.nlme)
my.plane.anova
```

## 06 - AAC
These data were reported in "Supporting emergent bilinguals who use augmentative and alternative communication and their families: Lessons in telepractice from the COVID-19 pandemic" by King et al. (2022) (https://doi.org/10.1044/2022_AJSLP-22-00003) and were retrieved from https://osf.io/sx3nf/.

### Loading Data
```{r}
# Load data
qualtrics_csv = function(file){
  data = read_csv(file, col_names = FALSE, skip = 3)
  names = read_lines(file, n_max = 1) |> 
    str_split(pattern = ",") |> 
    unlist()
  names(data) = names
  labels = read_csv(file, col_names = FALSE, skip = 1, n_max = 1)
  labels = t(labels)
  labels = labels[,1]
  labels = setNames(as.list(labels), names)
  data = labelled::set_variable_labels(data, .labels = labels, .strict = FALSE)
  data = janitor::clean_names(data)
  data
}

data_aac <- qualtrics_csv(file = here::here("Data", "06_AAC", "data_Combined CSV data.csv")) |> 
  janitor::remove_empty("cols") |> 
  janitor::remove_empty("rows") |> 
  filter(progress > 98) |> 
  dplyr::select(start_date, participant_id, q2:x2f)

# Examine the chi-square difference in lack of/limited Internet and technology barriers by timepoint
chi_squared <- data_aac |> 
  dplyr::select(q87_1_1, q87_1_2, q87_1_3, q87_1_5, q87_1_6, q87_1_7,
         q87_2_1, q87_2_2, q87_2_3, q87_2_5, q87_2_6, q87_2_7,
         q87_3_1, q87_3_2, q87_3_3, q87_3_5, q87_3_6, q87_3_7,
         q89_1_1, q89_1_2, q89_1_3, q89_1_5, q89_1_6, q89_1_7,
         q89_2_1, q89_2_2, q89_2_3, q89_2_5, q89_2_6, q89_2_7,
         q89_3_1, q89_3_2, q89_3_3, q89_3_5, q89_3_6, q89_3_7) |> 
  tidyr::pivot_longer(everything()) |>
  tidyr::separate(name,
           into = c("var", "time", "question"),
           sep = "_") |>
  dplyr::mutate(time = factor(
    time,
    labels = c(
      "Prior to\nMarch 2020",
      "March to\nJuly 2020",
      "August 2020 to\nSpring 2021"
    )
  ),
  var = factor(var, labels = c("Assessment", "Intervention"))) |>
  tidyr::drop_na(value) |>
  dplyr::group_by(var, value) |>
  dplyr::summarize(prop = list(chisq.test(table(time)))) |>
  dplyr::mutate(
    chi = map_dbl(prop, ~ .x$statistic),
    df = map_dbl(prop, ~ .x$parameter),
    p = map_dbl(prop, ~ .x$p.value)
  ) |>
  dplyr::filter(var == "Intervention") |>
  dplyr::filter(value == "Lack of/limited internet")
chi_squared
```


## 07 - Language
These data were reported in "Relationships between reading performance and regional spontaneous brain activity following surgical removal of primary left-hemisphere tumors: A resting-state fMRI study" by Kearney et al. (2023) (https://doi.org/10.1016/j.neuropsychologia.2023.108631) and were retrieved from https://osf.io/3abxs.

### Loading Data
```{r}
# Load data
language_data <-
  read.csv(here::here("Data/07_Language/demographics_CAT_reading.csv")) |>
  # select only relevant variables
  dplyr::select(c(Filename, edu, tpost_cat_read_total))

# Examine correlation between education (years) and reading t-scores
cor.test(language_data$edu, language_data$tpost_cat_read_total, method = "spearman")
```

## 08 - Cognition
These data were reported in "Emotion recognition of faces and emoji in individuals with moderate-severe traumatic brain injury" by Clough et al. (2023) (https://doi.org/10.1080/02699052.2023.2181401 and were retrieved from https://osf.io/enjd8/.

### Loading Data
```{r}
# Load packages
library(lme4) # glmer
library(lmerTest)
library(broom.mixed)

# Load data
cognition_data <-
  read.csv(here::here("Data/08_Cognition/emoji_affect_recognition_data.csv")) |>
  # select only relevant variables
  dplyr::select(c(SubjectID, Correct, Group, Condition, Sex)) |>
  # dummy-code Group (NC = 1, TBI = 0) and Sex (F = 1, M = 0)
  mutate(
    Group_NC = if_else(Group == "NC", 1, 0),
    Sex_F = if_else(Sex == "Female", 1, 0)) |>
  # treat categorical variables as factors and set reference levels
  mutate(
    SubjectID = as.factor(SubjectID),
    Correct = as.factor(Correct),
    Group_NC = relevel(as.factor(Group_NC), '1', '0'),
    Condition = relevel(as.factor(Condition), 'BasicFace', 'BasicEmoji', 'SocialEmoji'),
    Sex_F =  relevel(as.factor(Sex_F), '1', '0')
  )

# Set up Helmert contrast coding for Condition
# https://marissabarlaz.github.io/portfolio/contrastcoding/
# Contrast 1: Basic Emotion Faces vs average of Basic Emotion Emoji and Social Emotion Emoji
# Contrast 2: Basic Emotion Emoji vs Social Emotion Emoji
helmert = matrix(c(2/3, -1/3, -1/3, 0, 1/2, -1/2), ncol = 2)
contrasts(cognition_data$Condition) = helmert

# Models: Effect of stimulus condition on emotion recognition accuracy
# Note: Model first failed to converge when using default control settings, so changed optimizer to bobyqa as suggested here: https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
m1 <- lme4::glmer(Correct ~ Group_NC + Condition + Sex_F + Group_NC*Condition*Sex_F + (1 |SubjectID), data = cognition_data, family = binomial(link = "logit"), control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5)))
summary(m1)

# Set group reference level to TBI (0) and rerun model
cognition_data$Group_NC = relevel(cognition_data$Group_NC, '0', '1')
m2 <- lme4::glmer(Correct ~ Group_NC + Condition + Sex_F + Group_NC*Condition*Sex_F + (1 |SubjectID), data = cognition_data, family = binomial(link = "logit"), control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5)))
summary(m2)

# Odds ratios and confidence intervals
tidy(m1,conf.int=TRUE,exponentiate=TRUE,effects="fixed") # Condition1 conf.high = 5.86, reported in paper as 5.87
tidy(m2,conf.int=TRUE,exponentiate=TRUE,effects="fixed")
```

## 09 - Social Communication
These data were reported in "Difficulties with pronouns in autism: Experimental results from Thai children with autism" by Chanchaochai & Schwarz (2023) (https://doi.org/10.1080/10489223.2023.2262457) and were retrieved from https://osf.io/92jgd/.
### Loading Data
```{r}
# Load data
Comp <-
  read.csv(here::here("Data/09_Social_Communication/Study1Comp.csv"),
           fileEncoding = 'UTF-8-BOM')

subjdata <-
  plyr::ddply(
    Comp,
    c("ParGroup", "Subj", "Gender"),
    summarise,
    N    = length(Accuracy),
    acc  = sum(Accuracy),
    pct = mean(Accuracy),
    grade = mean(Grade),
    age = mean(Months),
    NVIQ = mean(SSIQ)
  )

# Model: Non-verbal IQ by group (ASD, TD)
summary(aov(NVIQ ~ ParGroup, subjdata))
```

## {- .leave-tabset}