---
title: "Open & Synthetic Data"
output:
  html_notebook:
    toc: yes
    toc_float: true
    toc_collapsed: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

# Packages
Here, we load the packages that we will use for data wrangling, as well as the `synthpop` package, which we will use to generate synthetic data.
```{r}
library(tidyverse) # data wrangling
library(synthpop) # install.packages('synthpop')
library(ggeasy) # install.packages('ggeasy')
```

# Data Synthesis {.tabset}
## 01 - Swallowing
These data were reported in "Normative Reference Values for FEES and VASES: Preliminary Data From 39 Nondysphagic, Community-Dwelling Adults" by Curtis et al. (2023) (https://doi.org/10.1044/2023_JSLHR-23-00132) and were retrieved from https://osf.io/4anzm/.

### Loading Data
```{r}
#Load data
swallowing_data <-
  read.csv(here::here("Data/01_Swallowing/norms_ratings.csv")) |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables
  dplyr::select(c(study_id, bolus_consistency, laryngeal_vestibule_severity_rating)) |> 
  mutate(
    # create binary variable for 'absent' when 0 and present when > 0
    lv_zero = if_else(laryngeal_vestibule_severity_rating == 0, "Absent", "Present"),
    # create variables for 'amount of residue when present'
    lv_present = ifelse(
      laryngeal_vestibule_severity_rating > 0,
      laryngeal_vestibule_severity_rating,
      NA
    )
  )

# Visualize distribution
break_trans = scales::trans_new('clip_range',
                                \(x) ifelse(x <= 50, x, x - 350),
                                \(x) ifelse(x <= 50, x, x + 350))

ggplot(swallowing_data,
       aes(x = laryngeal_vestibule_severity_rating, fill = lv_zero, color = lv_zero)) +
  geom_histogram(bins = 105) +
  coord_trans(y = break_trans) +
  cowplot::theme_cowplot() +
  scale_x_continuous(limits = c(-1, 100),
                     breaks = seq(from = 0, to = 100, by = 5.0)) +
  scale_y_continuous(
    limits = c(-1, 450),
    breaks = c(
      0,
      10,
      20,
      30,
      40,
      50,
      405,
      425,
      450
    )
  ) +
  geom_hline(yintercept = 50,
             linetype = "solid",
             color = "dark grey") +
  scale_fill_manual(values = c("orange", "blue")) +
  scale_color_manual(values = c("white", "white")) +
  labs(x = "Laryngeal Vestibule Residue Rating (%)",
       y = "Count") +
  ggeasy::easy_remove_legend()

# Summarise distribution
## Percentage of 'present' laryngeal vestibule residue
### Thin (IDDSI 1)
swallowing_data |> 
  filter(bolus_consistency == "Thin (IDDSI 0)") |> 
  count(lv_zero) |> 
  mutate(total = sum(n)) |> 
  mutate(perc = (n/total)*100)

### Pudding & Cracker
swallowing_data |> 
  filter(bolus_consistency != "Thin (IDDSI 0)") |> 
  count(lv_zero) |> 
  mutate(total = sum(n)) |> 
  mutate(perc = (n/total)*100)

## Median and IQR for 'present' laryngeal vestibule residue
swallowing_data |> 
  filter(bolus_consistency == "Thin (IDDSI 0)" & lv_zero == "Present") |> 
  summarise(median = median(lv_present),
            IQR_high = quantile(lv_present, 0.75),
            IQR_low = quantile(lv_present, 0.25))
```

### Generating Synthetic Data

### Comparisons

## 02 - Articulation
These data were reported in "Vowel Acoustics as Predictors of Speech Intelligibility in Dysarthria" by Thompson et al. (2023)
(https://doi.org/10.1044/2022_JSLHR-22-00287) and were retrieved from: https://osf.io/hr7aj/.

### Loading Data
```{r}
data_articulation <- rio::import(
  file = "Data/02_Articulation/data_Acoustic Measures.csv"
) |>
  
  # Remove the reliability trials that have "_rel" in the SpeakerID variable
  dplyr::filter(
    !grepl(
      pattern = "_rel",
      x = SpeakerID
  ))
```

### Generating Synthetic Data

### Comparisons

## 03 - Fluency

## 04 - Voice/Resonance
These data were reported in "Hypernasality associated with basal ganglia dysfunction: Evidence from Parkinson's disease and Huntington's disease" by Novotny et al. (2016) (https://doi.org/10.7717/peerj.2530) and were retrieved from https://peerj.com/articles/2530/#supp-1.

### Loading Data
```{r}
# Load data
voice_data <-
  readxl::read_excel(here::here("Data/04_Voice_Resonance/RawData.xlsx"), range = "B3:Z114") |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables
  dplyr::select(c(names, efn_sd_d_b, median_of_raters))

# Examine correlation between overall perceptual rating and acoustic EFn SD parameter
cor.test(voice_data$median_of_raters, voice_data$efn_sd_d_b, method = "pearson")
```

## 05 - Hearing
These data were reported in "Ubiquitous enhancement of spatial hearing in congenitally blind people" by Battal et al. (2020) (https://doi.org/10.31234/osf.io/veh7t) and were retrieved from https://osf.io/tav35/.

### Loading Data
```{r}
# Load packages
library(nlme) # LMM

# Load data
plane.thre.out <- read.table(here::here("Data/05_Hearing/All_Thresholds_outliersmarked-500B.txt"), sep=",", header=T)

# Convert variables to factors
names(plane.thre.out)[4]<-c('head')
plane.thre.out$arm <- factor(plane.thre.out$arm)
plane.thre.out$subj <- factor(plane.thre.out$subj)
plane.thre.out$head <- factor(plane.thre.out$head)
plane.thre.out$planes <- factor(plane.thre.out$planes)
plane.thre.out$group <- ifelse(plane.thre.out$group=='EB', c('CB'), c('SC'))
plane.thre.out$group <-factor(plane.thre.out$group)

#omit all +-3SD outliers 
plane.thre.out<- subset(plane.thre.out, outlier==0)
plane.thre.out[which(plane.thre.out$thre >12),]

#divide back
horizontal.thre.out <- subset(plane.thre.out,planes=='hor')
vertical.thre.out <- subset(plane.thre.out,planes=='ver')

rownames(horizontal.thre.out) <-c(1:nrow(horizontal.thre.out)) #correct the row numbers
horizontal.thre.out[which(horizontal.thre.out$thre >10),] # we know from glmm there's one outlier
horizontal.thre.out <-horizontal.thre.out[-c(188),] 

rownames(vertical.thre.out) <-c(1:nrow(vertical.thre.out))
vertical.thre.out[which(vertical.thre.out$thre >12),] # we know from glmm there's one outlier
vertical.thre.out <-vertical.thre.out[-c(144),] 

# in the horizontal back condition left arm is right area // right arm is left area of the subject
hor.thre.front <- subset(horizontal.thre.out, head ==1)
hor.thre.back <- subset(horizontal.thre.out, head ==2)

hor.thre.back$arm <-as.character(hor.thre.back$arm)
hor.thre.back <- within(hor.thre.back, arm[arm=="left"] <-("Alien"))#just to assign into a temp
hor.thre.back <- within(hor.thre.back, arm[arm=='right'] <- 'left')
hor.thre.back <- within(hor.thre.back, arm[arm=='Alien'] <- 'right')

horizontal.thre.out <-rbind(hor.thre.front,hor.thre.back)

# combine them back
plane.thre.out <- rbind(horizontal.thre.out,vertical.thre.out)
plane.thre.out$arm <- factor(plane.thre.out$arm)
plane.thre.out$subj <- factor(plane.thre.out$subj)
plane.thre.out$head <- factor(plane.thre.out$head)
plane.thre.out$planes <- factor(plane.thre.out$planes)
plane.thre.out$group <-factor(plane.thre.out$group)

#divide back
horizontal.thre.out <- subset(plane.thre.out,planes=='hor')
vertical.thre.out <- subset(plane.thre.out,planes=='ver')

# LMM on planes
# this is the model itself - subject is the random fx
plane.out.nlme <- lme(thre ~ 1 + planes * group * head, random = ~ 1|subj, weights = varIdent(form = ~ 1 | planes), data = plane.thre.out)

# Results
my.plane.anova <- anova(plane.out.nlme)
my.plane.anova
```

## 06 - AAC

## 07 - Language

## 08 - Cognition

## 09 - Social Communication
These data were reported in "Difficulties with pronouns in autism: Experimental results from Thai children with autism" by Chanchaochai & Schwarz (2023) (https://doi.org/10.1080/10489223.2023.2262457) and were retrieved from https://osf.io/92jgd/.

```{r}
# Load data
Comp <-
  read.csv(here::here("Data/09_Social_Communication/Study1Comp.csv"),
           fileEncoding = 'UTF-8-BOM')

subjdata <-
  plyr::ddply(
    Comp,
    c("ParGroup", "Subj", "Gender"),
    summarise,
    N    = length(Accuracy),
    acc  = sum(Accuracy),
    pct = mean(Accuracy),
    grade = mean(Grade),
    age = mean(Months),
    NVIQ = mean(SSIQ)
  )

# Model
summary(aov(NVIQ ~ ParGroup, subjdata))
```

## {- .leave-tabset}