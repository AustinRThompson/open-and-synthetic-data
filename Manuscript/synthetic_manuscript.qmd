---
title: "Synthetic Data in Communication Sciences and Disorders:  \nPromoting an Open, Reproducible, and Cumulative Science [preprint]"
# Line break in title requires 2 spaces followed by \n
author: |
  James C. Borders^1^, Austin Thompson^2^, & Elaine Kearney^3, 4^
abstract: |
  1.	Department of Biobehavioral Sciences, Teachers College Columbia University
  2.	Department of Communication Sciences and Disorders, University of Houston
  3.	School of Health and Rehabilitation Sciences, University of Queensland, Brisbane, Australia
  4.	Department of Speech Pathology, Princess Alexandra Hospital, Brisbane, Australia
# specifies that the output will be a word document
format:
  docx:
    # this holds the style template for the word document
    reference-doc: "templates-data/custom-reference.docx"
    # this holds the style template for the word document
    pandoc_args: ["-Fpandoc-crossref"]
# file with bibtex citations for the document. generated with the zotero plugin
# or using the {rbbt} R package
bibliography: "asynthetic.bib"
# this is the apa style for the document
csl: apa.csl
execute:
  cache: false
---

<!-- <br> will create a blank line between text if needed.  -->

<!-- A \ at the end of a line will cause a linebreak.  -->

<!-- \newpage will create a page break to put the abstract on a new page -->

::: {custom-style="noIndentParagraph"}
<br>

**Disclosures**:\
The authors have no financial or non-financial disclosures.

<br>

**Corresponding Author**:\
James C. Borders, PhD, CCC-SLP\
jcb2271\@tc.columbia.edu

<br>

**Authorship Contributions** (CRediT taxonomy - https://casrai.org/credit/)\
*Author Roles*: ^1^conceptualization, ^2^data curation, ^3^formal analysis, ^4^funding acquisition, ^5^investigation, ^6^methodology, ^7^project administration, ^8^resources, ^9^software, ^10^supervision, ^11^validation, ^12^visualization, ^13^writing -- original draft, ^14^writing -- reviewing & editing

JCB: 1, 2, 3, 6, 9, 12, 13\
AT: 1, 2, 3, 6, 9, 11, 12, 14\
EK: 1, 2, 3, 6, 9, 11, 12, 14

<br>

**Funding**: None.

<br>

**Ethical Approval**: This study was deemed exempt by the Institutional Review Board at the University of Queensland (#2024/HE001484).

<br>

**Keywords**: Open data; Reproducibility; Meta-science; Communication sciences and disorders

\newpage

<!-- This code chunk is not shown. It allows you to set knitr options globally. -->

```{r global options, include = FALSE, warning = FALSE, message = FALSE}
# set global options
knitr::opts_chunk$set(
  include = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE
)

# set seed for reproducibility
set.seed(2024)

# load packages
library(tidyverse) # data wrangling
library(here) # importing data
library(synthpop) # synthetic data generation
library(gamlss) # zero-inflated multilevel models
library(flextable) # create tables
library(officer) # create tables
library(insight) # create tables
```

# Abstract {style="text-align: center;"}

**Purpose**: Reproducibility is a core principle of science; however, data sharing is uncommon in the field of Communication Sciences and Disorders and exacerbated by concerns related to privacy and disclosure risks. Synthetic data offers a potential solution to this barrier by generating artificial datasets that do not represent real individuals yet retain statistical properties and relationships from the original data. The present study evaluates the performance of synthetic data generation using open data from previously published studies across the 'Big Nine' domains defined by the American Speech-Language Hearing Association.

**Method**: Open datasets were obtained from previously published research across the domains of Articulation, Cognition, Communication, Fluency, Hearing, Language, Social Communication, Voice and Resonance, and Swallowing. Synthetic datasets were generated with the *synthpop* R package. Results from synthetic datasets were compared to those from the original published datasets.

**Results**: Synthetic datasets maintained the direction of the *p*-value in six studies and effect size categorization in five out of nine studies. **Add more here**.

**Conclusion**: Findings indicate that... We provide a general framework to promote sharing open data to facilitate computational reproducibility and a cumulative science.
:::

\newpage

<!-- This is the title again using a first-level header -->

# Introduction {style="text-align: center;"}

Transparency and openness are fundamental tenets of science. One aspect of transparency and openness in science relates to computational reproducibility, or the ability to recreate a study's results using the original data. Nowadays, the vast majority of scientific studies use some degree of computation, including processing data, conducting descriptive or inferential statistics, or visualizing results. When these computations are reproducible, the transparency and confidence in findings is enhanced. Achieving computational reproducibility, however, requires authors to share their data. Both the National Institutes of Health and National Science Foundation mandate data sharing and management plans to ensure that scientific data supporting a study is shared upon publication and aligns with FAIR (Findability, Accessibility, Interoperability, and Reuse) principles of digital assets [@wilkinson_etal16; @watson_etal23]. Providing open, publicly available data benefits scientists, funding bodies, and society at large by enabling researchers to verify results, generate new knowledge (e.g., meta-analyses, secondary analyses), develop hypotheses, and minimize redundant data collection. In this sense, sharing data promotes a cumulative and self-correcting science.

Despite the clear benefits of open data and its growing adoption in other fields like psychology and the biobehavioral sciences [@quintana20], only 26% of researchers in the field of Communication Sciences and Disorders (CSD) reported sharing their data publicly at least once [@elamin_etal23]. Both individual and system-level barriers hinder data sharing, including a lack of time, knowledge, support from colleagues, and perceived incentives. Privacy and confidentiality concerns, particularly in low-incidence populations, also pose significant challenges [@pfeiffer_etal24]. Researchers have traditionally attempted to minimize disclosure risks by anonymizing datasets, aggregating results, or releasing a subset of the dataset; however, these practices do not fully eliminate the risk of identification in low-incidence populations. For example, re-identifying an individual in an incomplete dataset requires only a few demographic attributes [@rocher_etal19]. A further challenge in sharing data can occur when researchers did not prospectively obtain consent to share data and may not be able to contact participants after data collection [@pfeiffer_etal24].

Synthetic data generation offers a potential solution to maintaining participants' privacy and confidentiality in publicly available datasets [@rubin93; @drechsler_haensch24]. Synthetic data involves creating artificial datasets that do not represent real individuals, ensuring no risk of disclosure since participants in the synthetic dataset do not correspond to real individuals. Importantly, synthetic data retains the statistical properties and relationships of the original data, allowing researchers to reproduce study findings, explore the dataset, and develop new questions and hypotheses. Synthetic data generation is widely used across medical research, industry, and government agencies, most notably by the United States Census Bureau [@jarmin_etal14a]. Though synthetic data methods were proposed more than 30 years ago [@rubin93], recent analytic and practical developments have made it easier and more efficient to generate high-quality synthetic data [@nowok_etal16].

Despite the potential utility of synthetic data to promote open data in the field of CSD, this approach is not widely known or adopted in the field. Data commonly collected in CSD research poses unique challenges, including smaller sample sizes than are typically recommended for synthetic data generation [@borders_etal22a; @gaeta_brydges20]. Therefore, the present study aimed to examine the utility of synthetic data generation with open datasets from the 'Big Nine' American Speech-Language Hearing Association (ASHA) domains. We hypothesize that synthetic datasets will maintain the statistical properties and relationships of the original datasets, and that synthetic data will remain stable when generating multiple datasets. A secondary goal is to provide a framework for researchers in CSD to use data synthesis as a means to share fully de-identified data, thereby addressing concerns regarding researcher knowledge and participant confidentiality in sharing data.

# Method

## Description of Original Datasets from ASHA 'Big Nine' Domains

Authors performed a manual search to obtain publicly available datasets from previously published research articles related to 'Big Nine' ASHA domains: swallowing [@curtis_etal23a], articulation [@thompson_etal23], fluency [@elsherif_etal21], voice and resonance [@novotny_etal16], hearing [@battal_etal19], communication modalities [@king_etal22], receptive and expressive language [@kearney_etal23], cognitive aspects of communication [@clough_etal23], and social aspects of communication [@chanchaochai_schwarz23]. Authors then reproduced an analysis from each study. Table 1 provides a description of the population, analysis, and open materials for each study.

##### Table 1 here.

```{r table 1, echo = FALSE, include = TRUE}
# This code reads in a template, generates table 2, 
# and then saves it as word doc in the appropriate folder

cols2 <- tibble(
    `Domain` = c(
    "Swallowing",
    "Articulation",
    "Fluency",
    "Voice and resonance",
    "Hearing",
    "Communication modalities",
    "Receptive and expressive language",
    "Cognitive aspects of communication",
    "Social aspects of communication"
  ),
  `Study` = c(
    "Curtis et al. (2023)",
    "Thompson et al. (2023)",
    "Elsherif et al. (2021)",
    "Novotný et al. (2016)",
    "Battal et al. (2019)",
    "King et al. (2022)",
    "Kearney et al. (2023)",
    "Clough et al. (2023)",
    "Chanchaochai & Schwarz (2023)"
  ),
  `Open Materials` = c(
    "Data, code",
    "Data, code",
    "Data, code",
    "Data",
    "Data, code",
    "Data, code",
    "Data",
    "Data",
    "Data, code"
  ),
  `Sample Size` = c(
    "39",
    "40",
    "164",
    "111",
    "34",
    "160",
    "34",
    "102",
    "96"
  ),
  `Population(s)` = c(
    "Neurotypical",
    "Parkinson’s disease, amyotrophic lateral sclerosis, Huntington’s disease, cerebellar ataxia",
    "Dyslexia, stuttering, neurotypical",
    "Parkinson’s disease, Huntington’s disease, neurotypical",
    "Congenitally blind, sighted",
    "Speech-language pathologists",
    "Brain tumor",
    "Traumatic brain injury, neurotypical",
    "Autism spectrum disorder, neurotypical"
  ),
  `Analysis of Interest` = c(
    "Distribution of laryngeal vestibule residue ratings",
    "Relationship between vowel space area and intelligibility",
    "Group difference in nonword repetition",
    "Relationship between overall perceptual rating and variability of nasality",
    "Group difference in auditory localization",
    "Timepoint difference in lack of/limited internet and technology barriers",
    "Relationship between years of education and reading score",
    "Group x Condition interaction in emotion recognition accuracy",
    "Group difference in non-verbal IQ"
  ),
  `Outcome Type(s)` = c(
    "Continuous",
    "Continuous",
    "Continuous",
    "Continuous",
    "Continuous",
    "Ordinal",
    "Continuous",
    "Binary",
    "Continuous"
  ),
  `Statistics` = c(
    "Descriptive",
    "Hierarchical linear regression",
    "Independent t-test",
    "Pearson correlation",
    "Linear mixed-effects model with 3-way interaction",
    "Chi-square",
    "Spearman’s rank correlation coefficient",
    "Generalized linear mixed-effects model with 3-way interaction",
    "Analysis of Variance"
  )
)

set_flextable_defaults(font.family = "Times New Roman")

t2 <- flextable(cols2) |>
  set_caption("Table 1: Characteristics of included studies by ASHA domain.") |> 
  set_table_properties(layout = "autofit", width = 1)

doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t2)
fileout <- here("manuscript", "tables-figures", "table1.docx") # uncomment to write in your working directory
print(doc, target = fileout)
```

## Generation of Synthetic Datasets and Comparison with Original Dataset

All data generation and analyses were conducted in R version 4.2.1 [@rcoreteam22]. Synthetic data was generated with the *synthpop* R package version 1.8.0 [@nowok_etal16]. Specifically, *synthpop* uses a non-parametric classification and regression tree (CART) approach that can handle any data type and generates data by sampling from a probability distribution. Our aims were twofold: (1) to determine whether a synthetic dataset maintained statistical properties and relationships of the original dataset and (2) to examine whether this remained stable when generating multiple synthetic datasets. In light of these aims, our approach involved generating 100 different synthetic datasets for each original dataset from an ASHA 'Big Nine' domain. A statistical model with the original dataset was fit and the *p*-value and effect size were recorded. If 95% of p-values and effect sizes from the synthetic datasets demonstrated a similar result as the original study, then this indicated that synthetic data maintained the statistical relationship. Specifically, we further defined this as a similar inferential result for *p*-values (i.e., a 'significant' or 'non-significant' *p*-value based on the original study's alpha level) and effect sizes that maintained their categorization (e.g., a 'medium' effect size). Measures of effect size and their interpretation for each study are provided in Table 2. If variability between the 100 synthetic datasets was appreciated, we described the dispersion of this distribution. The analysis plan for this study was preregistered on the Open Science Framework (https://osf.io/vhgq2).

##### Table 2 here.

```{r table 2, echo = FALSE, include = TRUE}
cols3 <- tibble(
    `Statistical Test` = c(
    "Hierarchical linear regression",
    "Independent t-test",
    "Correlation (Pearson’s, Spearman’s)",
    "Chi-square",
    "Generalized linear model"
  ),
  `Effect Size Measure` = c(
    "Cohen’s f\n(Cohen, 1988)",
    "Cohen’s d\n(Cohen, 1988)",
    "Correlation coefficient\n(Cohen, 1988)",
    "Cohen's 𝜔\n(Cohen, 1988)",
    "√(3/𝜋) x odds ratio\n(Haddock et al., 1998; Hasselblad & Hedges, 1995)"
  ),
  `Small` = c(
    "0.1",
    "0.2",
    "0.1",
    "0.1",
    "0.2"
  ),
  `Medium` = c(
    "0.25",
    "0.5",
    "0.3",
    "0.3",
    "0.5"
  ),
  `Large` = c(
    "0.4",
    "0.8",
    "0.5",
    "0.5",
    "0.8"
  )
)

set_flextable_defaults(font.family = "Times New Roman")

t3 <- flextable(cols3) |>
  set_caption("Table 2: Effect size measures and interpretation by statistical test.") |> 
  set_table_properties(layout = "autofit", width = 1)

doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t3)
fileout <- here("manuscript", "tables-figures", "table2.docx")
print(doc, target = fileout)
```

In addition to these inferential comparisons, we provide a tutorial to walk through the required steps to generate synthetic data for the reader. This is accomplished in the context of two datasets [@curtis_etal23a; @thompson_etal23] with additional data visualization and detailed R code. Since curtis_etal23a did not perform inferential tests, we directly compared each synthetic dataset to the original data with a zero-inflated beta multilevel model with the *gamlss* package version 5.4.3 [@stasinopoulosGeneralizedAdditiveModels2007]. This model included fixed effects of dataset type (synthetic/original) and bolus consistency (thin liquid/extremely thick/regular) and a random intercept of participant. Due to issues with model convergence, the fixed effect structure was simplified to only include dataset type. The *p*-value from both zero-inflated and beta portions of the model were evaluated and *p* \< .05 was interpreted as evidence of no statistically significant difference between the synthetic and original dataset.

The tutorial data and accompanying code can be accessed on the Open Science Framework (OSF: **add link**). To get started, download… Open the *open-and-synthetic-data.Rproj* file in RStudio and then X file.

# Results

### Study 1: Normative Reference Values for Swallowing Outcomes
```{r echo = FALSE}
# here we run the main analysis
# then show the code to generate one synthetic dataset for pedagogical purposes below

# Load original data ----
swallowing_data_original <-
  read.csv(here::here("Data/01_Swallowing/norms_ratings.csv")) |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables
  dplyr::select(c(
    study_id,
    bolus_consistency,
    laryngeal_vestibule_severity_rating
  )) |>
  dplyr::mutate(
    bolus_consistency = as.factor(bolus_consistency),
    study_id = as.factor(study_id),
    # express laryngeal_vestibule_severity_rating as a percentage
    laryngeal_vestibule_severity_rating = laryngeal_vestibule_severity_rating /
      100
  )

# Establish p_value for study
swallowing_alpha <- .05

# Reproduce original results ----
# Obtain effect size measure for original dataset
swallowing_analyze_data <- function(data) {
  model <- gamlss::gamlss(
    formula = laryngeal_vestibule_severity_rating ~ 1 + re(random = ~ 1 | study_id),
    nu.formula = laryngeal_vestibule_severity_rating ~ 1 + re(random = ~ 1 | study_id),
    family = BEZI,
    data = na.omit(data)
  )
  model_fit <- summary(model, save=TRUE) # Saving the model
  
  list(
    p_value_mu = model_fit$pvalue[1],
    p_value_nu = model_fit$pvalue[3],
    # Extracting effect sizes
    effect_size_mu = sqrt((3 / pi)) * exp(model_fit$coef[1]),
    effect_size_nu = sqrt((3 / pi)) * exp(model_fit$coef[3]),
    # Extracting CI intervals
    conf_int_lower_mu = confint(model)[1, ][1], # lower CI bound for mu
    conf_int_upper_mu = confint(model)[1, ][2], # upper CI bound for mu
    conf_int_lower_nu = confint(model)[3, ][1], # lower CI bound for nu
    conf_int_upper_nu = confint(model)[3, ][2] # upper CI bound for nu
  )
}

# Reproduce original results ----
swallowing_results_original <-
  swallowing_analyze_data(swallowing_data_original) |>
  as.data.frame(row.names = NULL) |>
  dplyr::mutate(
    dataset = "original",
    # Variables for mu
    sig_mu = case_when(
      p_value_mu < swallowing_alpha ~ "sig",
      p_value_mu >= swallowing_alpha ~ "non-sig"
    ),
    categorization_mu = case_when(
      abs(effect_size_mu) > 0.8 ~ "Large",
      abs(effect_size_mu) > 0.5 ~ "Medium",
      abs(effect_size_mu) > 0.2 ~ "Small",
      TRUE ~ "Negligible"
    ),
    # Variables for nu
    sig_nu = case_when(
      p_value_nu < swallowing_alpha ~ "sig",
      p_value_nu >= swallowing_alpha ~ "non-sig"
    ),
    categorization_nu = case_when(
      abs(effect_size_nu) > 0.8 ~ "Large",
      abs(effect_size_nu) > 0.5 ~ "Medium",
      abs(effect_size_nu) > 0.2 ~ "Small",
      TRUE ~ "Negligible"
    )
  )


# Create synthetic data ----
swallowing_data_synthetic <- syn(swallowing_data_original, 
                                   method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
swallowing_results_synthetic <- swallowing_data_synthetic$syn |>
  purrr::map_df(~ swallowing_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    # Variables for mu
    sig_mu = case_when(
      p_value_mu < swallowing_alpha ~ "sig",
      p_value_mu >= swallowing_alpha ~ "non-sig"
    ),
    categorization_mu = case_when(
      abs(effect_size_mu) > 0.8 ~ "Large",
      abs(effect_size_mu) > 0.5 ~ "Medium",
      abs(effect_size_mu) > 0.2 ~ "Small",
      TRUE ~ "Negligible"
    ),
    # Variables for nu
    sig_nu = case_when(
      p_value_nu < swallowing_alpha ~ "sig",
      p_value_nu >= swallowing_alpha ~ "non-sig"
    ),
    categorization_nu = case_when(
      abs(effect_size_nu) > 0.8 ~ "Large",
      abs(effect_size_nu) > 0.5 ~ "Medium",
      abs(effect_size_nu) > 0.2 ~ "Small",
      TRUE ~ "Negligible"
    ),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue_mu = abs(p_value_mu - swallowing_results_original$p_value_mu),
    diff_effectsize_mu = abs(effect_size_mu - swallowing_results_original$effect_size_mu),
    diff_pvalue_nu = abs(p_value_nu - swallowing_results_original$p_value_nu),
    diff_effectsize_nu = abs(effect_size_nu - swallowing_results_original$effect_size_nu)
  )

# Compile results ----
swallowing_results_summary <- data_frame(
  Domain = "Swallowing",
  Study = "Curtis et al. (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(swallowing_data_original),
  
  # Mu vlues
  p_value_mu = ifelse(
    swallowing_results_original$p_value_mu < .001, "<.001",
    sprintf("%0.3f", swallowing_results_original$p_value_mu)),
  effect_size_measure_mu = "√(3/𝜋) x odds ratio (mu)",
  effect_size_mu = round(swallowing_results_original$effect_size_mu, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue_mu = swallowing_results_synthetic |>
    count(sig_mu) |>
    dplyr::mutate(percent = n / NROW(swallowing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig_mu == swallowing_results_original$sig_mu) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean_mu = mean(swallowing_results_synthetic$diff_pvalue_mu),
  absDiff_pvalue_sd_mu = sd(swallowing_results_synthetic$diff_pvalue_mu),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize_mu = swallowing_results_synthetic |>
    count(categorization_mu) |>
    dplyr::mutate(percent = n / nrow(swallowing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization_mu == swallowing_results_original$categorization_mu) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean_mu = mean(swallowing_results_synthetic$diff_effectsize_mu),
  absDiff_effectSize_sd_mu = sd(swallowing_results_synthetic$diff_effectsize_mu),
  
  # Nu vlues
  p_value_nu = ifelse(
    swallowing_results_original$p_value_nu < .001, "<.001",
    sprintf("%0.3f", swallowing_results_original$p_value_nu)),
  effect_size_measure_nu = "√(3/𝜋) x odds ratio (nu)",
  effect_size_nu = round(swallowing_results_original$effect_size_nu, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue_nu = swallowing_results_synthetic |>
    count(sig_nu) |>
    dplyr::mutate(percent = n / NROW(swallowing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig_nu == swallowing_results_original$sig_nu) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean_nu = mean(swallowing_results_synthetic$diff_pvalue_nu),
  absDiff_pvalue_sd_nu = sd(swallowing_results_synthetic$diff_pvalue_nu),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize_nu = swallowing_results_synthetic |>
    count(categorization_nu) |>
    dplyr::mutate(percent = n / nrow(swallowing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization_nu == swallowing_results_original$categorization_nu) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean_nu = mean(swallowing_results_synthetic$diff_effectsize_nu),
  absDiff_effectSize_sd_nu = sd(swallowing_results_synthetic$diff_effectsize_nu),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
swallowing <- list(
  data_original = swallowing_data_original,
  data_synthetic = swallowing_data_synthetic,
  results_original = swallowing_results_original,
  results_synthetic = swallowing_results_synthetic,
  results_summary = swallowing_results_summary,
  alpha = swallowing_alpha
)

## Cleaning up the environment
rm(
  swallowing_data_original,
  swallowing_data_synthetic,
  swallowing_results_original,
  swallowing_results_synthetic,
  swallowing_results_summary,
  swallowing_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = swallowing,
              file = here::here("Data","01_Swallowing","analysisData.RDS"))
```

Curtis et al. (2023) examined normative reference values for swallowing outcomes during flexible endoscopic evaluations of swallowing among 39 non-dysphagic, community-dwelling adults. In this observational cohort study, participants were administered 15 swallowing trials that varied by bolus size, consistency, contrast agent, and swallowing instructions. A variety of swallowing outcomes were measured, including the amount of laryngeal vestibule residue rated with the Visual Analysis of Swallowing Efficiency and Safety. Median and interquartile ranges (IQR) were used to describe the distribution of laryngeal vestibule residue ratings.

To generate synthetic data, we first load in the original dataset, wrangle the dataset using the *tidyverse* packages @wickham_etal19 and then create a synthetic dataset with the syn() function in the *synthpop* package. The data wrangling steps include loading required R packages, importing the original dataset csv file, 'cleaning' variable names to ensure they are all lowercase, selecting only relevant variables in the dataset, converting relevant categorical variables to factors, and expressing the outcome variable as a percentage out of 100.

```{r echo = TRUE, include = TRUE}
# load required packages
library(tidyverse) # data wrangling
library(synthpop) # R package to generate synthetic data

# load original data
swallowing_original_data <-
  # read csv file from appropriate path
  read.csv(here::here("Data/01_Swallowing/norms_ratings.csv")) |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables from dataset
  dplyr::select(c(study_id, bolus_consistency, 
                  laryngeal_vestibule_severity_rating)) |> 
  mutate(
    # convert study_id and bolus_consistency to factors
    study_id = as.factor(study_id),
    bolus_consistency = as.factor(bolus_consistency),
    # express laryngeal_vestibule_severity_rating as a %
    laryngeal_vestibule_severity_rating = laryngeal_vestibule_severity_rating/100
         )
```

Next, we create a synthetic dataset with the syn() function from the *synthpop* package. Within the function, 'method' specifies the synthesising method for the data. The default in synthpop is "cart" (Classification and Regression Tree). If a synthetic dataset fails to generate with this method, Nowok et al. (2018) recommend an alternative implementation of the CART technique from package *party* [@hothorn_etal06]. This dataset, for example, required the 'ctree' CART specification. Next, specify the number of synthetic datasets to generate within 'm'. Once the synthetic dataset is generated, extract and convert it to a dataframe for additional wrangling and visualization.

```{r echo = TRUE, include = TRUE}
# Create a synthetic dataset
synthetic_data <-
  syn(swallowing_original_data, # name of the original data
      method = "ctree", # CART model to generate synthetic data
      m = 1 # number of synthetic datasets to generate
      )

# Extract the synthetic dataset and convert into a data frame
synthetic_dataset <- as.data.frame(synthetic_data$syn)
```

An important step in the process is to assess the general utility of the synthetic dataset by visualizing any obvious differences compared to the original dataset. This can be easily accomplished with the compare() function in the *synthpop* package or manually with data wrangling and the ggplot package. Figure 1 suggests that the synthetic dataset demonstrated similar distributions for the variables of bolus consistency and laryngeal vestibule residue rating.

```{r echo = TRUE, include = TRUE}
# Comparison of original and synthetic datasets with synthpop package
swallowing_comparison <- compare(
  synthetic_dataset, # synthetic dataset
  swallowing_original_data, # original dataset
  vars = c("bolus_consistency",
           "laryngeal_vestibule_severity_rating"), # variables for comparison
  stat = "counts", # Present the raw counts for each variable
  cols = c("#62B6CB", "#1B4965") # Setting the colours in the plot
)
```

```{r echo = FALSE}
# manual visualization for aesthetics
swallowing_original_data_viz <- swallowing_original_data |> 
  add_column(Dataset = "Original")
  
fig1a <- synthetic_dataset |> 
  add_column(Dataset = "Synthetic") |> 
  full_join(swallowing_original_data_viz) |> 
  ggplot(aes(x = laryngeal_vestibule_severity_rating, fill = Dataset)) +
  geom_histogram(position = position_dodge()) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(labels = scales::label_percent()) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 9)),
        axis.title.y = element_text(margin = margin(r = 9))) +
  labs(x = "Laryngeal Vestibule Residue Rating",
       y = "Count") +
  scale_fill_manual(values = c("#62B6CB", "#1B4965")) +
  ggtitle("A")

fig1b <- synthetic_dataset |> 
  add_column(Dataset = "Synthetic") |> 
  full_join(swallowing_original_data_viz) |> 
  group_by(Dataset, bolus_consistency) |> 
  count() |> 
  mutate(bolus_consistency = case_when(bolus_consistency == "Extremely thick (IDDSI 4)" ~ "Extremely Thick",
                                       bolus_consistency == "Regular (IDDSI 7)" ~ "Regular",
                                       bolus_consistency == "Thin (IDDSI 0)" ~ "Thin")) |> 
  ggplot(aes(x = factor(bolus_consistency,
                        levels = c("Thin", "Extremely Thick", "Regular")), y = n, fill = Dataset)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(t = 9)),
        axis.title.y = element_text(margin = margin(r = 9))) +
  labs(x = "Bolus Consistency",
       y = "Count") +
  scale_fill_manual(values = c("#62B6CB", "#1B4965")) +
  ggtitle("B")

fig1_legend <- cowplot::get_plot_component(fig1a, 'guide-box-top', return_all = TRUE)

fig1_combined <- cowplot::plot_grid(fig1a + theme(legend.position = "none"), fig1b,
                   nrow = 1, align = "hv")

fig1a_combined <- cowplot::plot_grid(fig1_legend, fig1_combined,
                   nrow = 2, rel_heights = c(0.05, 1))

# export figure
ggsave(plot = fig1a_combined,
       filename = here("manuscript", "tables-figures", "figure_1.png"),
       dpi = 300,
       bg = "white",
       width = 9,
       height = 7)
```

```{r echo = FALSE}
# Create a descriptive summary of the synthetic and original dataset 
# for Curtis et al. (2023)

## Synthetic Dataset
synthetic_dataset_modified <- synthetic_dataset |> 
  mutate(
    # create binary variable for 'absent' when 0 and present when > 0
    lv_zero = if_else(laryngeal_vestibule_severity_rating == 0, "Absent", "Present"),
    # create variables for 'amount of residue when present'
    lv_present = ifelse(
      laryngeal_vestibule_severity_rating > 0,
      laryngeal_vestibule_severity_rating,
      NA
    )
  )

### Summarize zero's for laryngeal vestibule residue
synthetic_summarized_absent <- synthetic_dataset_modified |>
  group_by(bolus_consistency) |>
  count(lv_zero) |>
  mutate(total = sum(n)) |>
  mutate(perc_absent = (n / total) * 100,
         perc_absent = round(perc_absent, digits = 4)) |> 
  filter(lv_zero == "Absent") |> 
  dplyr::select(-c(n, lv_zero, total))

## Median and IQR for 'present' laryngeal vestibule residue
synthetic_summarized_present <- synthetic_dataset_modified |> 
  filter(lv_zero == "Present") |> 
  group_by(bolus_consistency) |> 
  summarise(IQR_low = quantile(lv_present, 0.25),
            median = median(lv_present),
            IQR_high = quantile(lv_present, 0.75))

## Combined synthetic dataset summary
synthetic_summary <- full_join(synthetic_summarized_present, 
                               synthetic_summarized_absent) |> 
  add_column(dataset_type = "synthetic")

## Original Dataset
original_dataset_modified <- swallowing_original_data |> 
  mutate(
    # create binary variable for 'absent' when 0 and present when > 0
    lv_zero = if_else(laryngeal_vestibule_severity_rating == 0, "Absent", "Present"),
    # create variables for 'amount of residue when present'
    lv_present = ifelse(
      laryngeal_vestibule_severity_rating > 0,
      laryngeal_vestibule_severity_rating,
      NA
    )
  )

### Summarize zero's for laryngeal vestibule residue
original_summarized_absent <- original_dataset_modified |>
  group_by(bolus_consistency) |>
  count(lv_zero) |>
  mutate(total = sum(n)) |>
  mutate(perc_absent = (n / total) * 100,
         perc_absent = round(perc_absent, digits = 4)) |> 
  filter(lv_zero == "Absent") |> 
  dplyr::select(-c(n, lv_zero, total))

## Median and IQR for 'present' laryngeal vestibule residue
original_summarized_present <- original_dataset_modified |> 
  filter(lv_zero == "Present") |> 
  group_by(bolus_consistency) |> 
  summarise(IQR_low = quantile(lv_present, 0.25),
            median = median(lv_present),
            IQR_high = quantile(lv_present, 0.75))

## Combined original dataset summary
original_summary <- full_join(original_summarized_present, 
                               original_summarized_absent) |> 
  add_column(dataset_type = "original")

### Combine original and synthetic data summaries
combined_summaries <- full_join(original_summary, synthetic_summary)

### Store number of trials by bolus consistency
number_trials <- table(swallowing_original_data$bolus_consistency) |> 
  as.data.frame()
```

Descriptively, the synthetic dataset classified `r round(combined_summaries$perc_absent[6], digits = 0)`% of laryngeal vestibule ratings on thin liquid boluses as 'absent' (i.e., 0% residue) compared to `r round(combined_summaries$perc_absent[1], digits = 0)`% in the original dataset. In the synthetic dataset, the median value on thin liquids was `r combined_summaries$median[6]` (IQR: `r combined_summaries$IQR_low[6]` - `r combined_summaries$IQR_high[6]`) compared to `r combined_summaries$median[1]` (IQR: `r combined_summaries$IQR_low[1]` - `r round(combined_summaries$IQR_high[1], digits = 2)`) in the original dataset. `r round(combined_summaries$perc_absent[4], digits = 2)`% of extremely thick liquids were classified as having no laryngeal vestibule residue compared to `r round(combined_summaries$perc_absent[2], digits = 2)`% in the original dataset. A similar pattern was appreciated for regular solids (`r round(combined_summaries$perc_absent[5], digits = 2)`% in synthetic vs. `r round(combined_summaries$perc_absent[3], digits = 2)`% in original dataset). Number of trials was lower for extremely thick (`r number_trials$Freq[1]` trials) and regular solid (`r number_trials$Freq[2]` trials) boluses compared to thin liquid (`r number_trials$Freq[3]` trials). When examined across 100 synthetic datasets, findings from the zero-inflated beta multilevel models indicate that `r curtis_pvalue_mu$n[1]`% of synthetic datasets were not statistically significantly different than the original dataset for both the zero-inflated and beta portions of the model (Table 3). Additionally, effect size categorizations were maintained for `r swallowing_effect_size_mu_summary$n`% of both zero-inflated and beta portions of the model.

### Study 2: Vowel Acoustics as Predictors of Speech Intelligibility in Dysarthria

```{r echo = FALSE}
# Import original data ----
articulation_data_original <- rio::import(
  file = here::here("Data", "02_Articulation", "data_Acoustic Measures.csv")
) |>
  
  # Remove the reliability trials that have "_rel" in the SpeakerID variable
  dplyr::filter(!str_detect(SpeakerID, "_rel")) |>
  
  # Selecting just the variables we need
  dplyr::select(
    SpeakerID, # ID
    VSA_b, # VSA in Bark
    Int = Int_OT # intelligibility (orthographic transcriptions)
  ) |> 
  
  # convert SpeakerID to factor
  mutate(SpeakerID = as.factor(SpeakerID))

## Establish p_value for study
articulation_alpha <- .05

# Analysis Function ----
## Perform regression between Intelligibility and VSA (bark)
articulation_analyze_data <- function(data) {
  model <- lm(Int ~ VSA_b, data = data) # Creating the model
  model_fit <- summary(model, save=TRUE) # Saving the model
  
  list(
    p_value = model_fit$coefficients[8], # Extracting p-values
    effect_size = effectsize::cohens_f(model, partial = F)$Cohens_f, # Extracting effect size
    conf_int_lower = confint(model)[2, ][1], # Extracting lower CI bound
    conf_int_upper = confint(model)[2, ][2] # Extracting upper CI bound
    )
}

# Reproduce original results ----
articulation_results_original <- (articulation_analyze_data(articulation_data_original)) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < articulation_alpha ~ "sig",
      p_value >= articulation_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.40 ~ "Large",
                               abs(effect_size) > 0.25 ~ "Medium",
                               abs(effect_size) > 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )


# Create synthetic data ----
articulation_data_synthetic <- syn(articulation_data_original, 
                          # method = "ctree", 
                          m = 100,
                          seed = 2024)

## Perform analysis on each synthetic dataset
articulation_results_synthetic <- articulation_data_synthetic$syn |>
  purrr::map_df(~ articulation_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < articulation_alpha ~ "sig",
                    p_value >= articulation_alpha ~ "non-sig"),
    categorization = case_when(
      abs(effect_size) > 0.40 ~ "Large",
      abs(effect_size) > 0.25 ~ "Medium",
      abs(effect_size) > 0.1 ~ "Small",
      TRUE ~ "Negligible"
    ),
     # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - articulation_results_original$p_value),
    diff_effectSize = abs(effect_size - articulation_results_original$effect_size)
  )

# Compile results ----
articulation_results_summary <- data_frame(
  Domain = "Articulation",
  Study = "Thompson et al. (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(articulation_data_original),
  p_value = ifelse(
    articulation_results_original$p_value < .001,"<.001",
    sprintf("%0.3f", articulation_results_original$p_value)),
  effect_size_measure = "Cohen’s f",
  effect_size = round(articulation_results_original$effect_size, digits = 2), 
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = articulation_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(articulation_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == articulation_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(articulation_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(articulation_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = articulation_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(articulation_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == articulation_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(articulation_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(articulation_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
articulation <- list(
  data_original = articulation_data_original,
  data_synthetic = articulation_data_synthetic,
  results_original = articulation_results_original,
  results_synthetic = articulation_results_synthetic,
  results_summary = articulation_results_summary,
  alpha = articulation_alpha
)

## Cleaning up the environment
rm(
  articulation_data_original,
  articulation_data_synthetic,
  articulation_results_original,
  articulation_results_synthetic,
  articulation_results_summary,
  articulation_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = articulation,
              file = here::here("Data","02_Articulation","analysisData.RDS"))
```

Thompson et al. (2023) examined the relationship between vowel space area and speech intelligibility among 40 speakers with dysarthria of varying etiologies, including Parkinson's disease, amyotrophic lateral sclerosis, Huntington's disease, and cerebellar ataxia. A linear regression model revealed a statistically significant relationship between vowel space area and intelligibility (*p* \< .001) with a Cohen's *f* of 0.59, corresponding to a conventionally "large" effect size (Table 2).

Below we import the original dataset, wrangle the data, and generate a synthetic data set. Data wrangling stpes include importing the original dataset, removing a string from the SpeakerID variable, and selecting only relevant variables.

```{r echo = TRUE, include = TRUE}
# import original data
articulation_original_data <- rio::import(
  file = here::here("Data", "02_Articulation", "data_Acoustic Measures.csv")
) |>
  # Remove the reliability trials that have "_rel" in the SpeakerID variable
  dplyr::filter(
    !grepl(
      pattern = "_rel",
      x = SpeakerID
  )) |>
  # Selecting just the variables we need
  dplyr::select(
    SpeakerID, # ID
    VSA_b, # VSA in Bark
    Int = Int_OT # intelligibility (orthographic transcriptions)
  )
```

Next, we generate a synthetic dataset with the syn() functionm extract the dataset, and convert it to a dataframe. *Synthpop* provides a warning message since this dataset has fewer observations than recommended (> 130).

```{r echo = TRUE, include = TRUE}
# generate synthetic dataset
articulation_synthetic_dataset <- syn(articulation_original_data,
                                      m = 1,
                                      seed = 2024)

# Extract the synthetic dataset and convert into a data frame
articulation_synthetic_dataset <- as.data.frame(articulation_synthetic_dataset$syn)
```

Next, we compare the distributions for vowel space area and speech intelligibility between the synthetic and original dataset. Figure 2 suggests that while the synthetic data largely approximates the original dataset, there are several values that are oversampled in the synthetic dataset. This might affect the quality of inferences with this dataset.

```{r echo = TRUE, include = TRUE}
# Comparison of original and synthetic datasets with synthpop package
articulation_comparison <- compare(
  articulation_synthetic_dataset, # synthetic dataset
  articulation_original_data, # original dataset
  vars = c("VSA_b",
           "Int"), # variables for comparison
  stat = "counts", # Present the raw counts for each variable
  cols = c("#62B6CB", "#1B4965") # Setting the colours in the plot
)
```

```{r echo = FALSE}
# manual visualization for aesthetics
articulation_original_data_viz <- articulation_original_data |> 
  add_column(Dataset = "Original")
  
fig2a <- articulation_synthetic_dataset |> 
  add_column(Dataset = "Synthetic") |> 
  full_join(articulation_original_data_viz) |> 
  ggplot(aes(x = VSA_b, fill = Dataset)) +
  geom_histogram(position = position_dodge()) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 9)),
        axis.title.y = element_text(margin = margin(r = 9))) +
  labs(x = "Vowel Space Area",
       y = "Count") +
  scale_fill_manual(values = c("#62B6CB", "#1B4965")) +
  ggtitle("A")

fig2b <- articulation_synthetic_dataset |> 
  add_column(Dataset = "Synthetic") |> 
  full_join(articulation_original_data_viz) |> 
  ggplot(aes(x = Int, fill = Dataset)) +
  geom_histogram(position = position_dodge()) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(t = 9)),
        axis.title.y = element_text(margin = margin(r = 9))) +
  labs(x = "Speech Intelligibility",
       y = "Count") +
  scale_fill_manual(values = c("#62B6CB", "#1B4965")) +
  ggtitle("B")

fig2_combined <- cowplot::plot_grid(fig2a + theme(legend.position = "none"), fig2b,
                   nrow = 1, align = "hv")

fig2a_combined <- cowplot::plot_grid(fig1_legend, fig2_combined,
                   nrow = 2, rel_heights = c(0.05, 1))

# export figure
ggsave(plot = fig2a_combined,
       filename = here("manuscript", "tables-figures", "figure_2.png"),
       dpi = 300,
       bg = "white",
       width = 9,
       height = 7)
```

Findings from the 100 generated synthetic datasets indicate that `r articulation_pvalues_summary$n[2]`% of datasets demonstrated the same inferential result (i.e., a statistically significant *p*-value). For the effect size, `r articulation_effect_size_summary$n[1]`% of synthetic datasets maintained a 'large' effect size categorization.

### Results for Studies 3 - 9
```{r fluency, echo = FALSE}
# Import original data ----
fluency_data_original <- rio::import(file = here::here("Data", "03_Fluency", "data_Dyslexia Stutter Master.csv")) |>
  dplyr::filter(Rejection == 0) |> 
  dplyr::filter(Group != "AWD") |>
  
  # Selecting just the variables that we need
  dplyr::select(
    Subject,
    Group,
    Nonwordrepetition
  )

# Establish p_value for study
fluency_alpha <- .05

# Analysis Function ----
# Examine the difference between neurotypical and stuttering in non-word repetition
fluency_analyze_data <- function(data) {
  t_test <- stats::t.test(
    Nonwordrepetition ~ Group,
    data = data,
    paired = FALSE,
    var.equal = FALSE
  )

  list(
    p_value = t_test$p.value,# Extracting p-values
    effect_size = effectsize::cohens_d(Nonwordrepetition ~ Group,
                     data = data,
                     ci = 0.95)$Cohens_d, # Extracting effect size
    conf_int_lower = t_test$conf.int[1],# Extracting lower CI bound
    conf_int_upper = t_test$conf.int[2] # Extracting upper CI bound
  )
}

# Reproduce original results ----
fluency_results_original <- (fluency_analyze_data(fluency_data_original)) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(p_value < fluency_alpha ~ "sig",
                    p_value >= fluency_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) > 0.8 ~ "Large",
                               abs(effect_size) > 0.5 ~ "Medium",
                               abs(effect_size) > 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )


# Create synthetic data ----
fluency_data_synthetic <- syn(fluency_data_original,
                              # method = "ctree",
                              m = 100,
                              seed = 2024)

# Perform analysis on each synthetic dataset
fluency_results_synthetic <- fluency_data_synthetic$syn |>
  purrr::map_df(~ fluency_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < fluency_alpha ~ "sig",
                    p_value >= fluency_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) > 0.8 ~ "Large",
                               abs(effect_size) > 0.5 ~ "Medium",
                               abs(effect_size) > 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - fluency_results_original$p_value),
    diff_effectSize = abs(effect_size - fluency_results_original$effect_size)
  )


# Compile results ----
fluency_results_summary <- data_frame(
  Domain = "Fluency",
  Study = "Elsherif et al. (2021)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(fluency_data_original),
  p_value = ifelse(
    fluency_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", fluency_results_original$p_value)),
  effect_size_measure = "Cohen’s d",
  effect_size = round(fluency_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = fluency_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(fluency_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == fluency_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(fluency_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(fluency_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = fluency_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(fluency_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == fluency_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(fluency_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(fluency_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
fluency <- list(
  data_original = fluency_data_original,
  data_synthetic = fluency_data_synthetic,
  results_original = fluency_results_original,
  results_synthetic = fluency_results_synthetic,
  results_summary = fluency_results_summary,
  alpha = fluency_alpha
)

## Cleaning up the environment
rm(
  fluency_data_original,
  fluency_data_synthetic,
  fluency_results_original,
  fluency_results_synthetic,
  fluency_results_summary,
  fluency_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = fluency,
              file = here::here("Data","03_Fluency","analysisData.RDS"))
```

```{r voice, echo = FALSE}
# Load data ----
voice_data_original <-
  readxl::read_excel(here::here("Data/04_Voice_Resonance/RawData.xlsx"), range = "B3:Z114") |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables
  dplyr::select(c(names, efn_sd_d_b, median_of_raters))

# Establish p_value for study
voice_alpha <- .05

# Analysis Function ----
# Examine correlation between overall perceptual rating and acoustic EFn SD parameter
voice_analyze_data <- function(data) {
  # Creating the model
  model <- cor.test(data$median_of_raters,
                    data$efn_sd_d_b,
                    method = "pearson")
  
  list(
    p_value = model$p.value, # Extracting p-values
    effect_size = model$estimate, # Extracting effect size
    conf_int_lower = model[["conf.int"]][1], # Extracting lower CI bound
    conf_int_upper = model[["conf.int"]][2] # Extracting upper CI bound
  )
}

# Reproduce original results ----
voice_results_original <- voice_analyze_data(voice_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < voice_alpha ~ "sig",
      p_value >= voice_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.5 ~ "Large",
                               abs(effect_size) > 0.3 ~ "Medium",
                               abs(effect_size) > 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Create synthetic data ----
voice_data_synthetic <- syn(voice_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)


# Perform analysis on each synthetic dataset
voice_results_synthetic <- voice_data_synthetic$syn |>
  purrr::map_df(~ voice_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < voice_alpha ~ "sig",
      p_value >= voice_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.5 ~ "Large",
                               abs(effect_size) > 0.3 ~ "Medium",
                               abs(effect_size) > 0.1 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - voice_results_original$p_value),
    diff_effectSize = abs(effect_size - voice_results_original$effect_size)
  )

# Compile results ----
voice_results_summary <- data_frame(
  Domain = "Voice",
  Study = "Novotný et al. (2016)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(voice_data_original),
  p_value = ifelse(
    voice_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", voice_results_original$p_value)),
  effect_size_measure = "Correlation coefficient",
  effect_size = round(voice_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = voice_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(voice_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == voice_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(voice_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(voice_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = voice_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(voice_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == voice_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(voice_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(voice_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
voice <- list(
  data_original = voice_data_original,
  data_synthetic = voice_data_synthetic,
  results_original = voice_results_original,
  results_synthetic = voice_results_synthetic,
  results_summary = voice_results_summary,
  alpha = voice_alpha
)

## Cleaning up the environment
rm(
  voice_data_original,
  voice_data_synthetic,
  voice_results_original,
  voice_results_synthetic,
  voice_results_summary,
  voice_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = voice,
              file = here::here("Data","04_Voice_Resonance","analysisData.RDS"))
```

```{r hearing, echo = FALSE}
# Load packages
library(nlme) # linear mixed models

# Import original data ----
plane.thre.out <- read.table(here::here("Data/05_Hearing/All_Thresholds_outliersmarked-500B.txt"), 
                             sep=",", header=T)

## data wrangling from author's code
# Convert variables to factors
names(plane.thre.out)[4]<-c('head')
plane.thre.out$arm <- factor(plane.thre.out$arm)
plane.thre.out$subj <- factor(plane.thre.out$subj)
plane.thre.out$head <- factor(plane.thre.out$head)
plane.thre.out$planes <- factor(plane.thre.out$planes)
plane.thre.out$group <- ifelse(plane.thre.out$group=='EB', c('CB'), c('SC'))
plane.thre.out$group <-factor(plane.thre.out$group)

#omit all +-3SD outliers 
plane.thre.out<- subset(plane.thre.out, outlier==0)
plane.thre.out[which(plane.thre.out$thre >12),]

#divide back
horizontal.thre.out <- subset(plane.thre.out,planes=='hor')
vertical.thre.out <- subset(plane.thre.out,planes=='ver')

rownames(horizontal.thre.out) <-c(1:nrow(horizontal.thre.out)) #correct the row numbers
horizontal.thre.out[which(horizontal.thre.out$thre >10),] # we know from glmm there's one outlier
horizontal.thre.out <-horizontal.thre.out[-c(188),] 

rownames(vertical.thre.out) <-c(1:nrow(vertical.thre.out))
vertical.thre.out[which(vertical.thre.out$thre >12),] # we know from glmm there's one outlier
vertical.thre.out <-vertical.thre.out[-c(144),] 

# in the horizontal back condition left arm is right area // right arm is left area of the subject
hor.thre.front <- subset(horizontal.thre.out, head ==1)
hor.thre.back <- subset(horizontal.thre.out, head ==2)

hor.thre.back$arm <-as.character(hor.thre.back$arm)
hor.thre.back <- within(hor.thre.back, arm[arm=="left"] <-("Alien"))#just to assign into a temp
hor.thre.back <- within(hor.thre.back, arm[arm=='right'] <- 'left')
hor.thre.back <- within(hor.thre.back, arm[arm=='Alien'] <- 'right')

horizontal.thre.out <-rbind(hor.thre.front,hor.thre.back)

# combine them back
plane.thre.out <- rbind(horizontal.thre.out,vertical.thre.out)
plane.thre.out$arm <- factor(plane.thre.out$arm)
plane.thre.out$subj <- factor(plane.thre.out$subj)
plane.thre.out$head <- factor(plane.thre.out$head)
plane.thre.out$planes <- factor(plane.thre.out$planes)
plane.thre.out$group <-factor(plane.thre.out$group)

#divide back
horizontal.thre.out <- subset(plane.thre.out,planes=='hor')
vertical.thre.out <- subset(plane.thre.out,planes=='ver')

# re-label dataset as 'original_data'
hearing_data_original <- plane.thre.out |> 
  dplyr::select(c(thre, planes, group, head, subj))

# Removing unneeded items
rm(plane.thre.out, hor.thre.back, hor.thre.front, horizontal.thre.out, vertical.thre.out)

# Establish p_value for study
hearing_alpha <- .05

# Analysis Function ----
# LMM on planes: Auditory localization performance by plane, group and position (head)
hearing_analyze_data <- function(data) {
  # Creating the model
  model <-
    nlme::lme(
      thre ~ 1 + planes * group * head,
      random = ~ 1 | subj,
      weights = varIdent(form = ~ 1 | planes),
      data = data
    )
  # Saving the model
  model_fit <- summary(model, save=TRUE)
  
  model_results <- anova(model_fit) |> 
    as.data.frame()
  
  list(
    p_value = model_results$`p-value`[3], # Extracting p-values
    # Extracting effect size - standardized effect size (Haddock et al., 1998; Hasselblad & Hedges, 1995)
    effect_size = (sqrt(3/pi))*exp(model_fit$coefficients$fixed[3]),
    conf_int_lower = intervals(model)[["fixed"]][3,][1], # Extracting lower CI bound
    conf_int_upper = intervals(model)[["fixed"]][3,][3] # Extracting upper CI bound
  )
}


# Reproduce original results ----
hearing_results_original <- hearing_analyze_data(hearing_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < hearing_alpha ~ "sig",
      p_value >= hearing_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.8 ~ "Large",
                               abs(effect_size) > 0.5 ~ "Medium",
                               abs(effect_size) > 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )



# Create synthetic data ----
hearing_data_synthetic <- syn(hearing_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
hearing_results_synthetic <- hearing_data_synthetic$syn |>
  purrr::map_df(~ hearing_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < hearing_alpha ~ "sig",
                    p_value >= hearing_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) > 0.8 ~ "Large",
                               abs(effect_size) > 0.5 ~ "Medium",
                               abs(effect_size) > 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - hearing_results_original$p_value),
    diff_effectSize = abs(effect_size - hearing_results_original$effect_size)
  )


# Compile results ----
hearing_results_summary <- data_frame(
  Domain = "Hearing",
  Study = "Battal et al. (2019)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(hearing_data_original),
  p_value = ifelse(
    hearing_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", hearing_results_original$p_value)),
  effect_size_measure = "√(3/𝜋) x odds ratio",
  effect_size = round(hearing_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = hearing_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(hearing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == hearing_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(hearing_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(hearing_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = hearing_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(hearing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == hearing_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(hearing_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(hearing_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
hearing <- list(
  data_original = hearing_data_original,
  data_synthetic = hearing_data_synthetic,
  results_original = hearing_results_original,
  results_synthetic = hearing_results_synthetic,
  results_summary = hearing_results_summary,
  alpha = hearing_alpha
)

## Cleaning up the environment
rm(
  hearing_data_original,
  hearing_data_synthetic,
  hearing_results_original,
  hearing_results_synthetic,
  hearing_results_summary,
  hearing_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = hearing,
              file = here::here("Data","05_Hearing","analysisData.RDS"))
```

```{r comm_modalities, echo = FALSE}

# Import original data ----
## Function to load in AAC data from the author's code
qualtrics_csv = function(file){
  data = read_csv(file, col_names = FALSE, skip = 3)
  names = read_lines(file, n_max = 1) |> 
    str_split(pattern = ",") |> 
    unlist()
  names(data) = names
  labels = read_csv(file, col_names = FALSE, skip = 1, n_max = 1)
  labels = t(labels)
  labels = labels[,1]
  labels = setNames(as.list(labels), names)
  data = labelled::set_variable_labels(data, .labels = labels, .strict = FALSE)
  data = janitor::clean_names(data)
  data
}

# Load original data
aac_data_original <- qualtrics_csv(file = here::here("Data", "06_AAC", "data_Combined CSV data.csv")) |> 
  janitor::remove_empty("cols") |> 
  janitor::remove_empty("rows") |> 
  filter(progress > 98) |> 
  dplyr::select(start_date, participant_id, q2:x2f) |> 
  dplyr::select(q87_1_1, q87_1_2, q87_1_3, q87_1_5, q87_1_6, q87_1_7,
         q87_2_1, q87_2_2, q87_2_3, q87_2_5, q87_2_6, q87_2_7,
         q87_3_1, q87_3_2, q87_3_3, q87_3_5, q87_3_6, q87_3_7,
         q89_1_1, q89_1_2, q89_1_3, q89_1_5, q89_1_6, q89_1_7,
         q89_2_1, q89_2_2, q89_2_3, q89_2_5, q89_2_6, q89_2_7,
         q89_3_1, q89_3_2, q89_3_3, q89_3_5, q89_3_6, q89_3_7) |> 
  tidyr::pivot_longer(everything()) |>
  tidyr::separate(name,
           into = c("var", "time", "question"),
           sep = "_") |>
  dplyr::mutate(time = factor(
    time,
    labels = c(
      "Prior to\nMarch 2020",
      "March to\nJuly 2020",
      "August 2020 to\nSpring 2021"
    )
  ),
  var = factor(var, labels = c("Assessment", "Intervention"))) |>
  tidyr::drop_na(value) |> 
    dplyr::select(-question)

# Establish p_value for study
aac_alpha <- .05

# Analysis Function ----
# Examine the chi-square difference in lack of/limited Internet and technology barriers by timepoint
aac_analyze_data <- function(data) {
  chi_squared <- data |>
    dplyr::group_by(var, value) |>
    dplyr::summarize(prop = list(chisq.test(table(time))),
                     .groups = "drop_last") |>
    dplyr::mutate(
      chi = map_dbl(prop, ~ .x$statistic),
      df = map_dbl(prop, ~ .x$parameter),
      p = map_dbl(prop, ~ .x$p.value)
    ) |>
    dplyr::filter(var == "Intervention") |>
    dplyr::filter(value == "Lack of/limited internet")
  
  list(
    p_value = chi_squared$p,
    # Extracting p-values
    effect_size = chi_squared$chi,
    # There are no confidence intervals for chi-squared tests
    conf_int_lower = NA,
    conf_int_upper = NA 
  )
}

# Reproduce original results ----
aac_results_original <- aac_analyze_data(aac_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < aac_alpha ~ "sig",
      p_value >= aac_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.5 ~ "Large",
                               abs(effect_size) > 0.3 ~ "Medium",
                               abs(effect_size) > 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Create synthetic data ----
aac_data_synthetic <- syn(aac_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
aac_results_synthetic <- aac_data_synthetic$syn |>
  purrr::map_df(~ aac_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < aac_alpha ~ "sig",
                    p_value >= aac_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) > 0.5 ~ "Large",
                               abs(effect_size) > 0.3 ~ "Medium",
                               abs(effect_size) > 0.1 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - aac_results_original$p_value),
    diff_effectSize = abs(effect_size - aac_results_original$effect_size)
  )

# Compile results ----
aac_results_summary <- data_frame(
  Domain = "Communication modalities",
  Study = "King et al. (2022)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(aac_data_original),
  p_value = ifelse(
    aac_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", aac_results_original$p_value)),
  effect_size_measure = "Cohen's 𝜔",
  effect_size = round(aac_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = aac_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(aac_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == aac_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(aac_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(aac_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = aac_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(aac_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == aac_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(aac_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(aac_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
aac <- list(
  data_original = aac_data_original,
  data_synthetic = aac_data_synthetic,
  results_original = aac_results_original,
  results_synthetic = aac_results_synthetic,
  results_summary = aac_results_summary,
  alpha = aac_alpha
)

## Cleaning up the environment
rm(
  aac_data_original,
  aac_data_synthetic,
  aac_results_original,
  aac_results_synthetic,
  aac_results_summary,
  aac_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = aac,
              file = here::here("Data","06_AAC","analysisData.RDS"))
```


```{r recep_language, echo = FALSE}
library(DescTools) # install.packages("DescTools")

# Import original data ----
language_data_original <-
  read.csv(here::here("Data/07_Language/demographics_CAT_reading.csv")) |>
  # select only relevant variables
  dplyr::select(c(Filename, edu, tpost_cat_read_total))

# Establish p_value for study
language_alpha <- .05

# Analysis Function ----
# Examine correlation between education (years) and reading t-scores
language_analyze_data <- function(data) {
  model <- cor.test(data$edu,
                    data$tpost_cat_read_total,
                    exact = F,
                    method = "spearman") # Creating the model
  
  # Obtaining CI's for SpearmansRho using a bootstrapping method
  model_CI <- DescTools::SpearmanRho(data$edu,
                                     data$tpost_cat_read_total,
                                     conf.level = .95)
  
  list(
    p_value = model$p.value, # Extracting p-values
    effect_size = model$estimate, # Extracting effect size
    # CI for S
    conf_int_lower = model_CI[2],
    conf_int_upper = model_CI[3]
  )
}

# Reproduce original results ----
language_results_original <- language_analyze_data(language_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < language_alpha ~ "sig",
      p_value >= language_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.5 ~ "Large",
                               abs(effect_size) > 0.3 ~ "Medium",
                               abs(effect_size) > 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Create synthetic data ----
language_data_synthetic <- syn(language_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
language_results_synthetic <- language_data_synthetic$syn |>
  purrr::map_df(~ language_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < language_alpha ~ "sig",
      p_value >= language_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.5 ~ "Large",
                               abs(effect_size) > 0.3 ~ "Medium",
                               abs(effect_size) > 0.1 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - language_results_original$p_value),
    diff_effectSize = abs(effect_size - language_results_original$effect_size)
  )

# Compile results ----
language_results_summary <- data_frame(
  Domain = "Language",
  Study = "Kearney et al. (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(language_data_original),
  p_value = ifelse(
    language_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", language_results_original$p_value)),
  effect_size_measure = "Correlation coefficient",
  effect_size = round(language_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = language_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(language_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == language_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(language_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(language_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = language_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(language_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == language_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(language_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(language_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
language <- list(
  data_original = language_data_original,
  data_synthetic = language_data_synthetic,
  results_original = language_results_original,
  results_synthetic = language_results_synthetic,
  results_summary = language_results_summary,
  alpha = language_alpha
)

## Cleaning up the environment
rm(
  language_data_original,
  language_data_synthetic,
  language_results_original,
  language_results_synthetic,
  language_results_summary,
  language_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = language,
              file = here::here("Data","07_Language","analysisData.RDS"))
```

```{r}
# Load data
cognition_original_data <-
  read.csv(here::here("Data/08_Cognition/emoji_affect_recognition_data.csv")) |>
  # select only relevant variables
  dplyr::select(c(SubjectID, Correct, Group, Condition, Sex)) |>
  # dummy-code Group (NC = 1, TBI = 0) and Sex (F = 1, M = 0)
  mutate(
    Group_NC = if_else(Group == "NC", 1, 0),
    Sex_F = if_else(Sex == "Female", 1, 0)) |>
  # treat categorical variables as factors and set reference levels
  mutate(
    SubjectID = as.factor(SubjectID),
    Correct = as.factor(Correct),
    Group_NC = relevel(as.factor(Group_NC), '1', '0'),
    Condition = relevel(as.factor(Condition), 'BasicFace', 'BasicEmoji', 'SocialEmoji'),
    Sex_F =  relevel(as.factor(Sex_F), '1', '0')
  ) |> 
  dplyr::select(-c(Group, Sex))

# Set up Helmert contrast coding for Condition
# https://marissabarlaz.github.io/portfolio/contrastcoding/
# Contrast 1: Basic Emotion Faces vs average of Basic Emotion Emoji and Social Emotion Emoji
# Contrast 2: Basic Emotion Emoji vs Social Emotion Emoji
helmert = matrix(c(2/3, -1/3, -1/3, 0, 1/2, -1/2), ncol = 2)
contrasts(cognition_original_data$Condition) = helmert

# Models: Effect of stimulus condition on emotion recognition accuracy
# Note: Model first failed to converge when using default control settings, so changed optimizer to bobyqa as suggested here: https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
m1 <- lme4::glmer(Correct ~ Group_NC + Condition + Sex_F + Group_NC*Condition*Sex_F + (1 |SubjectID), data = cognition_original_data, family = binomial(link = "logit"), control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5)))
summary(m1)

# Set group reference level to TBI (0) and rerun model
cognition_original_data$Group_NC = relevel(cognition_original_data$Group_NC, '0', '1')
m2 <- lme4::glmer(Correct ~ Group_NC + Condition + Sex_F + Group_NC*Condition*Sex_F + (1 |SubjectID), data = cognition_original_data, family = binomial(link = "logit"), control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5)))
summary(m2)

# Odds ratios and confidence intervals
tidy(m1,conf.int=TRUE,exponentiate=TRUE,effects="fixed") # Condition1 conf.high = 5.86, reported in paper as 5.87
tidy(m2,conf.int=TRUE,exponentiate=TRUE,effects="fixed")

# Examine p-value for Group X Condition Interaction
tidy(m1,conf.int=TRUE,exponentiate=TRUE,effects="fixed")[7,]$`p.value`

# Calculate standardized effect size from odds ratio for Group X Condition Interaction
## this is a "medium" effect (>= 0.50 & < 0.80)
sqrt((3/pi))*tidy(m1,conf.int=TRUE,exponentiate=TRUE,effects="fixed")[7,]$estimate

```

```{r cognition, echo = FALSE}
# Load packages
library(lme4) # glmer
library(lmerTest) # p-values for glmer
library(broom.mixed) # model summaries

# Import original data ----
cognition_data_original <-
  read.csv(here::here("Data/08_Cognition/emoji_affect_recognition_data.csv")) |>
  # select only relevant variables
  dplyr::select(c(SubjectID, Correct, Group, Condition, Sex)) |>
  # dummy-code Group (NC = 1, TBI = 0) and Sex (F = 1, M = 0)
  mutate(
    Group_NC = if_else(Group == "NC", 1, 0),
    Sex_F = if_else(Sex == "Female", 1, 0)) |>
  # treat categorical variables as factors and set reference levels
  mutate(
    SubjectID = as.factor(SubjectID),
    Correct = as.factor(Correct),
    Group_NC = relevel(as.factor(Group_NC), '1', '0'),
    Condition = relevel(as.factor(Condition), 'BasicFace', 'BasicEmoji', 'SocialEmoji'),
    Sex_F =  relevel(as.factor(Sex_F), '1', '0')
  ) |> 
  dplyr::select(-c(Group, Sex))

# Set up Helmert contrast coding for Condition
# https://marissabarlaz.github.io/portfolio/contrastcoding/
# Contrast 1: Basic Emotion Faces vs average of Basic Emotion Emoji and Social Emotion Emoji
# Contrast 2: Basic Emotion Emoji vs Social Emotion Emoji
helmert = matrix(c(2/3, -1/3, -1/3, 0, 1/2, -1/2), ncol = 2)
contrasts(cognition_data_original$Condition) = helmert

cognition_alpha <- .05

# Analysis Function ----
## Models: Effect of stimulus condition on emotion recognition accuracy
## Note: Model first failed to converge when using default control settings, so changed optimizer to bobyqa as suggested here: https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
cognition_analyze_data <- function(data) {
  # Creating the model
  m1 <-
    lme4::glmer(Correct ~ Group_NC + Condition + Sex_F + Group_NC * Condition * Sex_F + (1 | SubjectID),
      data = data,
      family = binomial(link = "logit"),
      control = glmerControl(optimizer = "bobyqa",
                             optCtrl = list(maxfun = 2e5))
    )
  
  # Set group reference level to TBI (0) and rerun model
  data$Group_NC = relevel(data$Group_NC, '0', '1')
  m2 <-
    lme4::glmer(
      Correct ~ Group_NC + Condition + Sex_F + Group_NC * Condition * Sex_F + (1 | SubjectID),
      data = data,
      family = binomial(link = "logit"),
      control = lme4::glmerControl(optimizer = "bobyqa",
                             optCtrl = list(maxfun = 2e5))
    )
  
  list(
    # Extracting p-values
    p_value = tidy(
      m1,
      conf.int = TRUE,
      exponentiate = TRUE,
      effects = "fixed")[7,]$`p.value`,
    # Extract effect size from odds ratio for Group (NC0) X Condition (2) Interaction
    effect_size = sqrt((3 / pi)) * tidy(m1,
                        conf.int = TRUE,
                        exponentiate = TRUE,
                        effects = "fixed")[7, ]$estimate,
    # Extracting lower CI bound
    conf_int_lower = tidy(
      m1,
      conf.int = TRUE,
      exponentiate = TRUE,
      effects = "fixed")[7,]$conf.low,
    # Extracting upper CI bound
    conf_int_upper = tidy(
      m1,
      conf.int = TRUE,
      exponentiate = TRUE,
      effects = "fixed")[7,]$conf.high
  )
}

# Reproduce original results ----
cognition_results_original <- cognition_analyze_data(cognition_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < cognition_alpha ~ "sig",
      p_value >= cognition_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.8 ~ "Large",
                               abs(effect_size) > 0.5 ~ "Medium",
                               abs(effect_size) > 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )


# Create synthetic data ----
cognition_data_synthetic <- syn(cognition_data_original, 
                                   method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
cognition_results_synthetic <- cognition_data_synthetic$syn |>
  purrr::map_df(~ cognition_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < cognition_alpha ~ "sig",
      p_value >= cognition_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.8 ~ "Large",
                               abs(effect_size) > 0.5 ~ "Medium",
                               abs(effect_size) > 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - cognition_results_original$p_value),
    diff_effectSize = abs(effect_size - cognition_results_original$effect_size)
  )


# Compile results ----
cognition_results_summary <- data_frame(
  Domain = "Cognition",
  Study = "Clough et al. (2023))",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(cognition_data_original),
  p_value = ifelse(
    cognition_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", cognition_results_original$p_value)),
  effect_size_measure = "√(3/𝜋) x odds ratio",
  effect_size = round(cognition_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = cognition_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(cognition_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == cognition_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(cognition_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(cognition_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = cognition_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(cognition_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == cognition_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(cognition_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(cognition_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
cognition <- list(
  data_original = cognition_data_original,
  data_synthetic = cognition_data_synthetic,
  results_original = cognition_results_original,
  results_synthetic = cognition_results_synthetic,
  results_summary = cognition_results_summary,
  alpha = cognition_alpha
)

## Cleaning up the environment
rm(
  cognition_data_original,
  cognition_data_synthetic,
  cognition_results_original,
  cognition_results_synthetic,
  cognition_results_summary,
  cognition_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = cognition,
              file = here::here("Data","08_Cognition","analysisData.RDS"))
```


```{r social_comm, echo = FALSE}
# Import original data ----
social_data_original <-
  read.csv(here::here("Data/09_Social_Communication/Study1Comp.csv"),
           fileEncoding = 'UTF-8-BOM') |> 
  plyr::ddply(
    c("ParGroup", "Subj", "Gender"),
    summarise,
    N    = length(Accuracy),
    acc  = sum(Accuracy),
    pct = mean(Accuracy),
    grade = mean(Grade),
    age = mean(Months),
    NVIQ = mean(SSIQ)
  ) |> 
  dplyr::select(c(Subj, NVIQ, ParGroup))

# Establish p_value for study
social_alpha <- .05

# Analysis Function ----
# Model: Non-verbal IQ by group (ASD, TD) two-tailed Fisher’s Exact Test
social_analyze_data <- function(data) {
  model <- aov(NVIQ ~ ParGroup,
               data = data) # Creating the model
  model_summary <- summary(model)
  
  list(
    p_value = model_summary[[1]]$'Pr(>F)'[1],
    # Extracting p-values
    effect_size = effectsize::cohens_d(NVIQ ~ ParGroup,
                     data = data,
                     ci = 0.95)$Cohens_d,
    # Extracting effect size
    conf_int_lower = confint(model)[2, ][1],
    # Extracting lower CI bound
    conf_int_upper = confint(model)[2,][2] # Extracting upper CI bound
  )
}

# Reproduce original results ----
social_results_original <- social_analyze_data(social_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < social_alpha ~ "sig",
      p_value >= social_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.8 ~ "Large",
                               abs(effect_size) > 0.5 ~ "Medium",
                               abs(effect_size) > 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )



# Create synthetic data ----
social_data_synthetic <- syn(social_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
social_results_synthetic <- social_data_synthetic$syn |>
  purrr::map_df(~ social_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < social_alpha ~ "sig",
      p_value >= social_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) > 0.8 ~ "Large",
                               abs(effect_size) > 0.5 ~ "Medium",
                               abs(effect_size) > 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - social_results_original$p_value),
    diff_effectSize = abs(effect_size - social_results_original$effect_size)
  )

# Compile results ----
social_results_summary <- data_frame(
  Domain = "Social aspects of communication",
  Study = "Chanchaochai & Schwarz (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(social_data_original),
  p_value = ifelse(
    social_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", social_results_original$p_value)),
  effect_size_measure = "Cohen’s d",
  effect_size = round(social_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = social_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(social_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == social_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(social_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(social_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = social_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(social_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == social_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(social_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(social_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
social <- list(
  data_original = social_data_original,
  data_synthetic = social_data_synthetic,
  results_original = social_results_original,
  results_synthetic = social_results_synthetic,
  results_summary = social_results_summary,
  alpha = social_alpha
)

## Cleaning up the environment
rm(
  social_data_original,
  social_data_synthetic,
  social_results_original,
  social_results_synthetic,
  social_results_summary,
  social_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = social,
              file = here::here("Data","09_Social_Communication","analysisData.RDS"))
```

##### Table 3 here.

```{r table 3, echo = FALSE}

# This code reads in a template, generates table 3, 
# and then saves it as word doc in the appropriate folder

# Load in the data
swallowing <- base::readRDS(file = here::here("Data","01_Swallowing","analysisData.RDS"))
articulation <- base::readRDS(file = here::here("Data","02_Articulation","analysisData.RDS"))
fluency <- base::readRDS(file = here::here("Data","03_Fluency","analysisData.RDS"))
voice <- base::readRDS(file = here::here("Data","04_Voice_Resonance","analysisData.RDS"))
hearing <- base::readRDS(file = here::here("Data","05_Hearing","analysisData.RDS"))
aac <- base::readRDS(file = here::here("Data","06_AAC","analysisData.RDS"))
language <- base::readRDS(file = here::here("Data","07_Language","analysisData.RDS"))
cognition <- base::readRDS(file = here::here("Data","08_Cognition","analysisData.RDS"))
social <-base::readRDS(file = here::here("Data","09_Social_Communication","analysisData.RDS"))

cols2 <- tibble(
  `Domain` = c(
    "Swallowing",
    "Articulation",
    "Fluency",
    "Voice and resonance",
    "Hearing",
    "Communication modalities",
    "Receptive and expressive language",
    "Cognitive aspects of communication",
    "Social aspects of communication"
  ),
  `Study` = c(
    "Curtis et al. (2023)",
    "Thompson et al. (2023)",
    "Elsherif et al. (2021)",
    "Novotný et al. (2016)",
    "Battal et al. (2019)",
    "King et al. (2022)",
    "Kearney et al. (2023)",
    "Clough et al. (2023)",
    "Chanchaochai & Schwarz (2023)"
  ),
  `N` = c(
    swallowing$results_summary$N,
    articulation$results_summary$N,
    fluency$results_summary$N,
    voice$results_summary$N,
    hearing$results_summary$N,
    aac$results_summary$N,
    language$results_summary$N,
    cognition$results_summary$N,
    social$results_summary$N
  ),
  `P-value` = c(
    swallowing$results_summary$p_value_mu,
    articulation$results_summary$p_value,
    fluency$results_summary$p_value,
    voice$results_summary$p_value,
    sub("^0", "", hearing$results_summary$p_value),
    aac$results_summary$p_value,
    language$results_summary$p_value,
    sub("^0", "", cognition$results_summary$p_value),
    social$results_summary$p_value
  ),
  `Effect Size Measure` = c(
    swallowing$results_summary$effect_size_measure_mu,
    articulation$results_summary$effect_size_measure,
    fluency$results_summary$effect_size_measure,
    voice$results_summary$effect_size_measure,
    hearing$results_summary$effect_size_measure,
    aac$results_summary$effect_size_measure,
    language$results_summary$effect_size_measure,
    cognition$results_summary$effect_size_measure,
    social$results_summary$effect_size_measure
  ),
  `Effect Size` = c(
    sub("^0", "", swallowing$results_summary$effect_size_mu),
    sub("^0", "", articulation$results_summary$effect_size),
    sub("^0", "", fluency$results_summary$effect_size),
    sub("^0", "", voice$results_summary$effect_size),
    sub("^0", "", hearing$results_summary$effect_size),
    sub("^0", "", aac$results_summary$effect_size),
    sub("^0", "", language$results_summary$effect_size),
    sub("^0", "", cognition$results_summary$effect_size),
    sub("0.", ".", social$results_summary$effect_size)
  ),
  `P-value\nStability` = c(
    swallowing$results_summary$synAgreement_pvalue_mu,
    articulation$results_summary$synAgreement_pvalue,
    fluency$results_summary$synAgreement_pvalue,
    voice$results_summary$synAgreement_pvalue,
    hearing$results_summary$synAgreement_pvalue,
    aac$results_summary$synAgreement_pvalue,
    language$results_summary$synAgreement_pvalue,
    cognition$results_summary$synAgreement_pvalue,
    social$results_summary$synAgreement_pvalue
  ),
  `Effect Size Categorization\nStability` = c(
    swallowing$results_summary$synAgreement_effectSize_mu,
    articulation$results_summary$synAgreement_effectSize,
    fluency$results_summary$synAgreement_effectSize,
    voice$results_summary$synAgreement_effectSize,
    hearing$results_summary$synAgreement_effectSize,
    aac$results_summary$synAgreement_effectSize,
    language$results_summary$synAgreement_effectSize,
    cognition$results_summary$synAgreement_effectSize,
    social$results_summary$synAgreement_effectSize
  )
)

set_flextable_defaults(font.family = "Times New Roman")

t2 <- flextable(cols2) |>
  set_caption("Table 3: Stability of synthetic datasets across ASHA domains.") |> 
  set_table_properties(layout = "autofit", width = 1)

doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t2)
fileout <- here("manuscript", "tables-figures", "table3.docx") # uncomment to write in your working directory
print(doc, target = fileout)
```

# Discussion

XXX

# Conclusions

XXX

\newpage

# Preregistration and Data Availability

Preregistration, data, and analysis scripts are publicly available on the Open Science Framework (https://osf.io/vhgq2 - **note: update this link when preregistration goes public**).

# Acknowledgements

We would like to thank the authors of the studies included in this manuscript for making their data publicly available.

\newpage

# References

::: {#refs}
:::

\newpage

::: {custom-style="noIndentParagraph"}
# Table and Figure Captions

Table 1. Table 1: Characteristics of included studies by ASHA domain.

Figure 1. Visualization of data distributions from synthetic and original data for Study #1 (Curtis et al., 2023). *Caption*: Panel A displays the overall distribution of laryngeal vestibule residue. Panel B displays the frequency of values by bolus consistency.
:::
