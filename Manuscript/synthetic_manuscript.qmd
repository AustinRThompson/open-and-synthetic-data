---
title: "Synthetic Data in Communication Sciences and Disorders:  \nPromoting an Open, Reproducible, and Cumulative Science [preprint]"
# Line break in title requires 2 spaces followed by \n
author: |
  James C. Borders^1^, Austin Thompson^2^, & Elaine Kearney^3,4^
abstract: |
  1.	Department of Biobehavioral Sciences, Teachers College Columbia University
  2.	Department of Communication Sciences and Disorders, University of Houston
  3.	School of Health and Rehabilitation Sciences, University of Queensland, Brisbane, Australia
  4.	Department of Speech Pathology, Princess Alexandra Hospital, Brisbane, Australia
# specifies that the output will be a word document
format:
  docx:
    # this holds the style template for the word document
    reference-doc: "templates-data/custom-reference.docx"
    # this holds the style template for the word document
    pandoc_args: ["-Fpandoc-crossref"]
# file with bibtex citations for the document. generated with the zotero plugin
# or using the {rbbt} R package
bibliography: "asynthetic.bib"
# this is the apa style for the document
csl: apa.csl
execute:
  cache: false
---

<!-- <br> will create a blank line between text if needed.  -->

<!-- A \ at the end of a line will cause a linebreak.  -->

<!-- \newpage will create a page break to put the abstract on a new page -->

::: {custom-style="noIndentParagraph"}
<br>

**Disclosures**:\
The authors have no financial or non-financial disclosures.

<br>

**Corresponding Author**:\
James C. Borders, PhD, CCC-SLP\
jcb2271\@tc.columbia.edu

<br>

**Authorship Contributions** (CRediT taxonomy - https://casrai.org/credit/)\
*Author Roles*: ^1^conceptualization, ^2^data curation, ^3^formal analysis, ^4^funding acquisition, ^5^investigation, ^6^methodology, ^7^project administration, ^8^resources, ^9^software, ^10^supervision, ^11^validation, ^12^visualization, ^13^writing -- original draft, ^14^writing -- reviewing & editing

JCB: 1, 2, 3, 6, 9, 12, 13\
AT: 1, 2, 3, 6, 9, 11, 12, 14\
EK: 1, 2, 3, 6, 9, 11, 12, 14

<br>

**Funding**: None.

<br>

**Ethical Approval**: This study was deemed exempt by the Institutional Review Board at the University of Queensland (#2024/HE001484).

<br>

**Keywords**: Open data; Reproducibility; Meta-science; Communication sciences and disorders

<br>

**Study Preregistration and Data Availability**: The study preregistration (https://osf.io/vhgq2) and associated data and analysis scripts (https://osf.io/yhkqf/?view_only=) are publicly available on the Open Science Framework (**Note**: *update both private links when these are made public after peer review*).

\newpage

<!-- This code chunk is not shown. It allows you to set knitr options globally. -->

```{r global options, include = FALSE, warning = FALSE, message = FALSE}
# set global options
knitr::opts_chunk$set(
  include = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE
)

# set seed for reproducibility
set.seed(2024)

# load packages
library(tidyverse) # data wrangling
library(here) # importing data
library(synthpop) # synthetic data generation
library(gamlss) # zero-inflated multilevel models
library(flextable) # create tables
library(officer) # create tables
library(insight) # create tables
```

# Abstract {style="text-align: center;"}

**Purpose**: Reproducibility is a core principle of science and access to a study's data is essential to reproduce its findings. However, data sharing is uncommon in the field of Communication Sciences and Disorders (CSD), often due to concerns related to privacy and disclosure risks. Synthetic data offers a potential solution to this barrier by generating artificial datasets that do not represent real individuals yet retain statistical properties and relationships from the original data. This study evaluates the performance of synthetic data generation using open data from previously published studies across the American Speech-Language-Hearing Association (ASHA) 'Big Nine' domains.

**Method**: Open datasets were obtained from previously published research within the ASHA domains of articulation, cognition, communication, fluency, hearing, language, social communication, voice and resonance, and swallowing. Synthetic datasets were generated with the *synthpop* R package. Inferential statistics (*p*-values) and effect sizes from synthetic datasets were compared to those from the original datasets.

**Results**: Synthetic datasets maintained the direction of *p*-values in six studies and effect size categorizations in five out of nine studies. In cases where synthetic datasets did not maintain 95% of the inferential or effect size results, the absolute mean difference between synthetic and original datasets was relatively low, suggesting that the distribution of results from synthetic datasets closely approximated the alpha or effect size categorization threshold.

**Conclusion**: Findings suggest that synthetic data can effectively maintain statistical properties and relationships across a wide range of data commonly seen in the field of CSD. While some studies with fewer observations than recommended (i.e., n < 130) showed lower agreement and greater variability in *p*-values and effect size estimates, this was not consistently appreciated. Therefore, researchers who use synthetic data should assess its stability in preserving their results. This study concludes with a general framework on sharing open data to facilitate computational reproducibility and foster a cumulative science in the field of CSD.
:::

\newpage

<!-- This is the title again using a first-level header -->

# Introduction {style="text-align: center;"}

Transparency and openness are fundamental tenets of science, with computational reproducibility playing a key role in maintaining these values. Computational reproducibility refers to the ability to recreate a study's results using the original data. Nowadays, the vast majority of scientific studies use some degree of computation, including processing data, conducting descriptive or inferential statistics, and visualizing results. When these computations are reproducible, the transparency and confidence in findings are enhanced. Achieving computational reproducibility, however, requires authors to share their data. Both the National Institutes of Health and the National Science Foundation mandate data sharing and management plans to ensure that scientific data supporting a study is shared upon publication and aligns with FAIR (Findability, Accessibility, Interoperability, and Reuse) principles of digital assets [@wilkinson_etal16; @watson_etal23]. 

Providing open, publicly available data benefits scientists, funding bodies, and society at large by enabling researchers to verify results, generate new knowledge (e.g., meta-analyses, secondary analyses), develop hypotheses, and minimize redundant data collection [@chow_etal23]. In this sense, sharing data promotes a cumulative and self-correcting science. Despite the clear benefits of open data and its growing adoption in other fields like psychology and the biobehavioral sciences [@quintana20], only 26% of a sample of researchers in the field of Communication Sciences and Disorders (CSD) reported sharing their data publicly at least once [@elamin_etal23]. 

Understanding the nuances of data sharing requires a closer look at the different types of data generated throughout a research project’s life cycle. These include raw collected data, processed intermediate data, and final analysis data (Table 1). However, a common misconception is that open data refers solely to sharing raw data (e.g., audio recordings, videos, MRI data) [@pfeiffer_etal24]. In reality, sharing intermediate or analysis data can also support reproducibility while reducing privacy and confidentiality concerns associated with sharing raw data. However, these different types of data offer varying levels of utility: sharing raw data enables maximum reproducibility and secondary research opportunities, while analysis data (although easier to share) primarily supports computational reproducibility.

##### Table 1 here.

Both individual and system-level barriers hinder data sharing, including a lack of time, knowledge, support from colleagues, and perceived incentives [@pfeiffer_etal24]. Furthermore, each type of data comes with unique challenges regarding data sharing. For raw data, it is common that researchers often do not obtain consent to share data or cannot contact participants after data collection. Additionally, sharing de-identified raw or intermediate data may require additional approval from the institutional review board. Even when de-identification is possible, anonymized intermediate or analysis datasets can still carry re-identification risks, especially in small samples or vulnerable populations where indirect identifiers (e.g., gender, age, or race) may compromise participant confidentiality [@rocher_etal19]. Therefore, although sharing de-identified analysis data is the minimum requirement for ensuring computational reproducibility and promoting cumulative science, concerns about privacy and confidentiality may persist.

Synthetic data generation offers a potential solution to maintaining participants' privacy and confidentiality in publicly available datasets [@rubin93; @drechsler_haensch24]. Synthetic data involves creating an artificial dataset that does not represent real individuals, ensuring no risk of disclosure since participants in the synthetic dataset do not correspond to real individuals. Importantly, synthetic data retains the statistical properties and relationships of the original data, allowing researchers to reproduce study findings, explore the dataset, and develop new questions and hypotheses. Synthetic data generation is widely used across medical research, industry, and government agencies, most notably by the United States Census Bureau [@jarmin_etal14a]. Though synthetic data methods were first proposed more than 30 years ago [@rubin93], recent analytic and software developments have made it easier and more efficient to generate high-quality synthetic data [@nowok_etal16].

Despite the potential utility of synthetic data to promote data sharing in the field of CSD, this approach is not widely known or adopted in the field. Data commonly collected in CSD research poses unique challenges, including smaller sample sizes than are typically recommended for synthetic data generation and a wide range of outcomes and analyses [@borders_etal22a; @gaeta_brydges20]. Therefore, the present study aimed to examine the utility of synthetic data generation with open datasets from the 'Big Nine' American Speech-Language-Hearing Association (ASHA) domains. We hypothesized that synthetic datasets would maintain the statistical properties and relationships (i.e., *p*-value and effect size) of the original datasets, and that synthetic data would remain stable when generating multiple datasets. A secondary goal was to provide a framework to describe considerations when sharing data, thereby addressing concerns regarding researcher knowledge and participant confidentiality in open data.

# Method

## Description of Original Datasets from ASHA 'Big Nine' Domains

Authors performed a manual search to obtain publicly available datasets from previously published research articles related to the 'Big Nine' ASHA domains: swallowing [@curtis_etal23a], articulation [@thompson_etal23], fluency [@elsherif_etal21], voice and resonance [@novotny_etal16], hearing [@battal_etal19], communication modalities [@king_etal22], receptive and expressive language [@kearney_etal23], cognitive aspects of communication [@clough_etal23], and social aspects of communication [@chanchaochai_schwarz23]. Authors then reproduced an analysis from each study. Table 2 provides a description of the population, analysis, and open materials for each study.

##### Table 2 here.

```{r table 2, echo = FALSE, include = TRUE}
# This code reads in a template, generates table 2, 
# and then saves it as word doc in the appropriate folder

cols2 <- tibble(
    `Domain` = c(
    "Swallowing",
    "Articulation",
    "Fluency",
    "Voice and resonance",
    "Hearing",
    "Communication modalities",
    "Receptive and expressive language",
    "Cognitive aspects of communication",
    "Social aspects of communication"
  ),
  `Study` = c(
    "Curtis et al. (2023)",
    "Thompson et al. (2023)",
    "Elsherif et al. (2021)",
    "Novotný et al. (2016)",
    "Battal et al. (2019)",
    "King et al. (2022)",
    "Kearney et al. (2023)",
    "Clough et al. (2023)",
    "Chanchaochai & Schwarz (2023)"
  ),
  `Open Materials` = c(
    "Data, code",
    "Data, code",
    "Data, code",
    "Data",
    "Data, code",
    "Data, code",
    "Data",
    "Data",
    "Data, code"
  ),
  `Sample Size` = c(
    "39",
    "40",
    "164",
    "111",
    "34",
    "160",
    "34",
    "102",
    "96"
  ),
  `Population(s)` = c(
    "Neurotypical",
    "Parkinson’s disease, amyotrophic lateral sclerosis, Huntington’s disease, cerebellar ataxia",
    "Dyslexia, stuttering, neurotypical",
    "Parkinson’s disease, Huntington’s disease, neurotypical",
    "Congenitally blind, sighted",
    "Speech-language pathologists",
    "Brain tumor",
    "Traumatic brain injury, neurotypical",
    "Autism spectrum disorder, neurotypical"
  ),
  `Analysis of Interest` = c(
    "Distribution of laryngeal vestibule residue ratings",
    "Relationship between vowel space area and intelligibility",
    "Group difference in nonword repetition",
    "Relationship between overall perceptual rating and variability of nasality",
    "Group difference in auditory localization",
    "Timepoint difference in lack of/limited internet and technology barriers",
    "Relationship between years of education and reading score",
    "Group x Condition interaction in emotion recognition accuracy",
    "Group difference in non-verbal IQ"
  ),
  `Outcome Type(s)` = c(
    "Continuous",
    "Continuous",
    "Continuous",
    "Continuous",
    "Continuous",
    "Ordinal",
    "Continuous",
    "Binary",
    "Continuous"
  ),
  `Statistics` = c(
    "Descriptive",
    "Hierarchical linear regression",
    "Independent t-test",
    "Pearson correlation",
    "Linear mixed-effects model with 3-way interaction",
    "Chi-square",
    "Spearman’s rank correlation coefficient",
    "Generalized linear mixed-effects model with 3-way interaction",
    "Analysis of Variance"
  )
)

set_flextable_defaults(font.family = "Times New Roman")

t2 <- flextable(cols2) |>
  set_caption("Table 2: Characteristics of included studies by ASHA domain.") |> 
  set_table_properties(layout = "autofit", width = 1)

doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t2)
fileout <- here("manuscript", "tables-figures", "table2.docx") # uncomment to write in your working directory
print(doc, target = fileout)
```

## Generation of Synthetic Datasets and Comparison with Original Dataset

Synthetic data generation and statistical analyses were conducted in R version 4.2.1 [@rcoreteam22]. Synthetic data was generated with the *synthpop* R package (version 1.8.0) [@nowok_etal16]. Specifically, *synthpop* uses a non-parametric classification and regression tree approach that generates data by sampling from a probability distribution and can handle any type of data. Our aims were twofold: (1) to determine whether a synthetic dataset maintained the statistical properties and relationships of the original dataset and (2) to examine whether this remained stable when generating multiple synthetic datasets. In light of these aims, our approach involved generating 100 different synthetic datasets for each original dataset from an ASHA 'Big Nine' domain. A statistical model with the original dataset was fit, and the *p*-value and effect size were recorded. If 95% of *p*-values and effect sizes from the synthetic datasets demonstrated a similar result as the original study, then this indicated that synthetic data maintained the statistical relationship. Specifically, we further defined this as a similar inferential result for *p*-values (i.e., a 'significant' or 'non-significant' *p*-value based on the original study's alpha level) and effect sizes that maintained their categorization based on conventional thresholds (e.g., a 'medium' effect size). Measures of effect size and their interpretation for each study are provided in Table 3. If variability between the 100 synthetic datasets was appreciated, we visualized and described the dispersion of this distribution. The analysis plan for this study was preregistered on the Open Science Framework (https://osf.io/vhgq2).

##### Table 3 here.

```{r table 3, echo = FALSE, include = TRUE}
cols3 <- tibble(
    `Statistical Test` = c(
    "Hierarchical linear regression",
    "Independent t-test",
    "Correlation (Pearson’s, Spearman’s)",
    "Chi-square",
    "Generalized linear model"
  ),
  `Effect Size Measure` = c(
    "Cohen’s f\n(Cohen, 1988)",
    "Cohen’s d (Cohen, 1988) &\nGlass' Δ (Hedges & Olkin, 1985)",
    "Correlation coefficient\n(Cohen, 1988)",
    "Cohen's 𝜔\n(Cohen, 1988)",
    "√(3/𝜋) x odds ratio\n(Haddock et al., 1998; Hasselblad & Hedges, 1995)"
  ),
  `Small` = c(
    "0.1",
    "0.2",
    "0.1",
    "0.1",
    "0.2"
  ),
  `Medium` = c(
    "0.25",
    "0.5",
    "0.3",
    "0.3",
    "0.5"
  ),
  `Large` = c(
    "0.4",
    "0.8",
    "0.5",
    "0.5",
    "0.8"
  )
)

set_flextable_defaults(font.family = "Times New Roman")

t3 <- flextable(cols3) |>
  set_caption("Table 3: Effect size measures and interpretation by statistical test.") |> 
  set_table_properties(layout = "autofit", width = 1)

doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t3)
fileout <- here("manuscript", "tables-figures", "table3.docx")
print(doc, target = fileout)
```

In addition to these inferential comparisons, we provide a brief tutorial to guide the reader through the required steps to generate synthetic data. This is accomplished in the context of two datasets [@curtis_etal23a; @thompson_etal23] with additional data visualization and detailed R code. Since @curtis_etal23a did not perform inferential tests, we directly compared each synthetic dataset to the original data with a zero-inflated beta multilevel model with the *gamlss* package (version 5.4.3) [@stasinopoulos_rigby07]. This model included fixed effects of dataset type (synthetic/original) and bolus consistency (thin liquid/extremely thick/regular), and a random intercept of participant. Due to issues with model convergence, the fixed effect structure was simplified to only include dataset type. The *p*-value from both zero-inflated and beta portions of the model were evaluated and *p* \< .05 was interpreted as evidence of no statistically significant difference between the synthetic and original dataset.

# Results

The tutorial data and accompanying code can be accessed on the Open Science Framework (https://osf.io/yhkqf/). To get started, download R (https://cran.r-project.org/) and an interface like RStudio (https://posit.co/download/rstudio-desktop/). Open the *open-and-synthetic-data.Rproj* file in RStudio and then X file. **Add more here about the tutorial scripts**.

### Study 1: Normative Reference Values for Swallowing Outcomes
```{r}
# Import original data ----
swallowing_data_original <-
  read.csv(here::here("Data/01_Swallowing/norms_ratings.csv")) |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables
  dplyr::select(c(study_id, bolus_consistency, laryngeal_vestibule_severity_rating)) |> 
  mutate(bolus_consistency = as.factor(bolus_consistency),
         study_id = as.factor(study_id),
         # express laryngeal_vestibule_severity_rating as a percentage
         laryngeal_vestibule_severity_rating = laryngeal_vestibule_severity_rating/100,
         dataset_type = "original",
         dataset_number = 0)

swallowing_alpha <- .05

# Reproduce original results ----
# Obtain effect size measure for original dataset
swallowing_model_original <- gamlss(
  formula = laryngeal_vestibule_severity_rating ~ 1 + re(random = ~ 1 | study_id),
  nu.formula = laryngeal_vestibule_severity_rating ~ 1 + re(random = ~ 1 | study_id),
  family = BEZI,
  data = na.omit(swallowing_data_original)
)

# save model fit
model_fit <- summary(swallowing_model_original, save = TRUE)

swallowing_results_original <- list(
    p_value_mu = model_fit$pvalue[1],
    p_value_nu = model_fit$pvalue[3],
    # Extracting effect sizes
    effect_size_mu = sqrt((3 / pi)) * exp(model_fit$coef[1]),
    effect_size_nu = sqrt((3 / pi)) * exp(model_fit$coef[3])
) |>
  as.data.frame(row.names = NULL) |>
  dplyr::mutate(
    dataset = "original",
    # Variables for mu
    sig_mu = case_when(
      p_value_mu < swallowing_alpha ~ "sig",
      p_value_mu >= swallowing_alpha ~ "non-sig"
    ),
    categorization_mu = case_when(
      abs(effect_size_mu) >= 0.8 ~ "Large",
      abs(effect_size_mu) >= 0.5 ~ "Medium",
      abs(effect_size_mu) >= 0.2 ~ "Small",
      TRUE ~ "Negligible"
    ),
    # Variables for nu
    sig_nu = case_when(
      p_value_nu < swallowing_alpha ~ "sig",
      p_value_nu >= swallowing_alpha ~ "non-sig"
    ),
    categorization_nu = case_when(
      abs(effect_size_nu) >= 0.8 ~ "Large",
      abs(effect_size_nu) >= 0.5 ~ "Medium",
      abs(effect_size_nu) >= 0.2 ~ "Small",
      TRUE ~ "Negligible"
    )
  )

# Create synthetic data ----
swallowing_data_synthetic <- syn(swallowing_data_original, 
             method = "ctree", 
             m = 100,
             seed = 2024)
swallowing_data_synthetic <- swallowing_data_synthetic$syn

# Initialize a list for synthetic results
swallowing_results_synthetic <- vector("list", 100)

# Loop through each synthetic dataset
for (i in 1:100) {
  # Get the i-th synthetic dataset
  data_synthetic <- swallowing_data_synthetic[i] |>
    as.data.frame() |>
    dplyr::mutate(dataset_type = "synthetic",
           dataset_number = i)
  
  # Combine original and synthetic datasets
  data_combined <- rbind(swallowing_data_original,
                    data_synthetic) |>
    mutate(
      study_id = as.character(study_id),
      dataset_type = as.factor(dataset_type),
      
      # Remove "HOA" from study_id and convert to numeric
      study_id = as.numeric(gsub("HOA", "", study_id))
    )
  
  # Fit the model
  ## 11% of models demonstrated non-convergence so the fixed effects structure was simplified to only include dataset_type.
  model <- gamlss(
    formula = laryngeal_vestibule_severity_rating ~ dataset_type + re(random = ~ 1 | study_id),
    nu.formula = laryngeal_vestibule_severity_rating ~ dataset_type + re(random = ~ 1 | study_id),
    family = BEZI,
    data = na.omit(data_combined)
  )
  
  # Save the model
  model_fit <- summary(model, save=TRUE) 

  # Compiling the results in the swallowing_results_synthetic list.
  swallowing_results_synthetic[[i]] <- list(
    dataset = i,
    p_value_mu = model_fit$pvalue[2],
    p_value_nu = model_fit$pvalue[5],
    effect_size_mu = sqrt((3 / pi)) * exp(model_fit$coef[1]),
    effect_size_nu = sqrt((3 / pi)) * exp(model_fit$coef[3])
  )
}

# Collapsing the list into a single dataframe & categorizing effect sizes and p-values
swallowing_results_synthetic <- do.call(rbind, lapply(swallowing_results_synthetic, as.data.frame, row.names = NULL)) |>
  dplyr::mutate(
    # Variables for mu
    sig_mu = case_when(
      p_value_mu < swallowing_alpha ~ "sig",
      p_value_mu >= swallowing_alpha ~ "non-sig"
    ),
    categorization_mu = case_when(
      abs(effect_size_mu) >= 0.8 ~ "Large",
      abs(effect_size_mu) >= 0.5 ~ "Medium",
      abs(effect_size_mu) >= 0.2 ~ "Small",
      TRUE ~ "Negligible"
    ),
    # Variables for nu
    sig_nu = case_when(
      p_value_nu < swallowing_alpha ~ "sig",
      p_value_nu >= swallowing_alpha ~ "non-sig"
    ),
    categorization_nu = case_when(
      abs(effect_size_nu) >= 0.8 ~ "Large",
      abs(effect_size_nu) >= 0.5 ~ "Medium",
      abs(effect_size_nu) >= 0.2 ~ "Small",
      TRUE ~ "Negligible"
    ),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue_mu = abs(p_value_mu - swallowing_results_original$p_value_mu),
    diff_effectsize_mu = abs(effect_size_mu - swallowing_results_original$effect_size_mu),
    diff_pvalue_nu = abs(p_value_nu - swallowing_results_original$p_value_nu),
    diff_effectsize_nu = abs(effect_size_nu - swallowing_results_original$effect_size_nu)
  )

# Compile results ----
swallowing_results_summary <- dplyr::data_frame(
  Domain = "Swallowing",
  Study = "Curtis et al. (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(swallowing_data_original),
  
  # Mu values
  p_value_mu = ifelse(
    swallowing_results_original$p_value_mu < .001, "<.001",
    sprintf("%0.3f", swallowing_results_original$p_value_mu)),
  effect_size_measure_mu = "√(3/𝜋) x odds ratio (mu)",
  effect_size_mu = round(swallowing_results_original$effect_size_mu, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue_mu = swallowing_results_synthetic |>
    count(sig_mu) |>
    dplyr::mutate(percent = n / NROW(swallowing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig_mu == "non-sig") |> # if the data_type fixed effect was non-sig, it indicates the data were similar
    dplyr::pull(percent),
  absDiff_pvalue_mean_mu = mean(swallowing_results_synthetic$diff_pvalue_mu),
  absDiff_pvalue_sd_mu = sd(swallowing_results_synthetic$diff_pvalue_mu),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize_mu = swallowing_results_synthetic |>
    count(categorization_mu) |>
    dplyr::mutate(percent = n / nrow(swallowing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization_mu == swallowing_results_original$categorization_mu) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean_mu = mean(swallowing_results_synthetic$diff_effectsize_mu),
  absDiff_effectSize_sd_mu = sd(swallowing_results_synthetic$diff_effectsize_mu),
  
  # Nu values
  p_value_nu = ifelse(
    swallowing_results_original$p_value_nu < .001, "<.001",
    sprintf("%0.3f", swallowing_results_original$p_value_nu)),
  effect_size_measure_nu = "√(3/𝜋) x odds ratio (nu)",
  effect_size_nu = round(swallowing_results_original$effect_size_nu, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue_nu = swallowing_results_synthetic |>
    count(sig_nu) |>
    dplyr::mutate(percent = n / NROW(swallowing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig_nu == "non-sig") |> # if the data_type fixed effect was non-sig, it indicates the data were similar
    dplyr::pull(percent),
  absDiff_pvalue_mean_nu = mean(swallowing_results_synthetic$diff_pvalue_nu),
  absDiff_pvalue_sd_nu = sd(swallowing_results_synthetic$diff_pvalue_nu),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize_nu = swallowing_results_synthetic |>
    count(categorization_nu) |>
    dplyr::mutate(percent = n / nrow(swallowing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization_nu == swallowing_results_original$categorization_nu) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean_nu = mean(swallowing_results_synthetic$diff_effectsize_nu),
  absDiff_effectSize_sd_nu = sd(swallowing_results_synthetic$diff_effectsize_nu)
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
swallowing <- list(
  data_original = swallowing_data_original,
  data_synthetic = swallowing_data_synthetic,
  results_original = swallowing_results_original,
  results_synthetic = swallowing_results_synthetic,
  results_summary = swallowing_results_summary,
  alpha = swallowing_alpha
)

## Cleaning up the environment
rm(
  swallowing_data_original,
  swallowing_data_synthetic,
  swallowing_model_original,
  swallowing_results_original,
  swallowing_results_synthetic,
  swallowing_results_summary,
  swallowing_alpha,
  model,
  model_fit,
  data_combined,
  data_synthetic,
  i
)

# Saving the data and results for easy importing later.
base::saveRDS(object = swallowing,
              file = here::here("Data","01_Swallowing","analysisData.RDS"))


```

Curtis et al. (2023) examined normative reference values for swallowing outcomes during flexible endoscopic evaluations of swallowing among 39 non-dysphagic, community-dwelling adults. In this observational cohort study, participants were administered 15 swallowing trials that varied by bolus size, consistency, contrast agent, and swallowing instructions. A variety of swallowing outcomes were measured, including the amount of laryngeal vestibule residue rated with the Visual Analysis of Swallowing Efficiency and Safety. Median and interquartile ranges (IQR) were used to describe the distribution of laryngeal vestibule residue ratings.

To generate synthetic data, we first load in the original dataset, wrangle the dataset using the *tidyverse* collection of packages [@wickham_etal19] (v. 1.3.2) and then create a synthetic dataset with the syn() function from the *synthpop* package. The data wrangling steps include (1) loading required R packages, (2) importing the original dataset csv file, (3) reformatting variable names for consistency and readability, (4) selecting the variables needed for the analysis, (4) converting appropriate categorical variables to factors, and (5) calculating the laryngeal vestibule severity rating as a percentage. These steps are shown below with the following commands:

```{r echo = TRUE, include = TRUE}
# load required packages
library(tidyverse) # data wrangling
library(synthpop) # R package to generate synthetic data

# load original data
swallowing_original_data <-
  # read csv file from appropriate path
  read.csv(here::here("Data/01_Swallowing/norms_ratings.csv")) |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables from dataset
  dplyr::select(c(study_id, bolus_consistency, 
                  laryngeal_vestibule_severity_rating)) |> 
  mutate(
    # convert study_id and bolus_consistency to factors
    study_id = as.factor(study_id),
    bolus_consistency = as.factor(bolus_consistency),
    # express laryngeal_vestibule_severity_rating as a %
    laryngeal_vestibule_severity_rating = laryngeal_vestibule_severity_rating/100
         )
```

Next, we create a synthetic dataset with the syn() function from the *synthpop* package. Within the function, 'method' specifies the synthesising method for the data. The default in synthpop is "cart" (Classification and Regression Tree). If a synthetic dataset fails to generate with this method, Nowok et al. (2018) recommend an alternative implementation of the CART technique from package *party* [@hothorn_etal06]. This dataset, for example, required the 'ctree' CART specification. Next, specify the number of synthetic datasets to generate within 'm'. Once the synthetic dataset is generated, extract and convert it to a dataframe for additional wrangling and visualization.

```{r echo = TRUE, include = TRUE}
# Create a synthetic dataset
synthetic_data <-
  syn(swallowing_original_data, # name of the original data
      method = "ctree", # CART model to generate synthetic data
      m = 1 # number of synthetic datasets to generate
      )

# Extract the synthetic dataset and convert into a data frame
synthetic_dataset <- as.data.frame(synthetic_data$syn)
```

An important step in the process is to assess the general utility of the synthetic dataset by visualizing any obvious differences compared to the original dataset. This can be easily accomplished with the compare() function in the *synthpop* package or manually with data wrangling and the ggplot package. Figure 1 suggests that the synthetic dataset demonstrated similar distributions for the variables of bolus consistency and laryngeal vestibule residue rating.

```{r echo = TRUE, include = TRUE}
# Comparison of original and synthetic datasets with synthpop package
swallowing_comparison <- compare(
  synthetic_dataset, # synthetic dataset
  swallowing_original_data, # original dataset
  vars = c("bolus_consistency",
           "laryngeal_vestibule_severity_rating"), # variables for comparison
  stat = "counts", # Present the raw counts for each variable
  cols = c("#62B6CB", "#1B4965") # Setting the colours in the plot
)
```

```{r echo = FALSE}
# manual visualization for aesthetics
swallowing_original_data_viz <- swallowing_original_data |> 
  add_column(Dataset = "Original")
  
fig1a <- synthetic_dataset |> 
  add_column(Dataset = "Synthetic") |> 
  full_join(swallowing_original_data_viz) |> 
  ggplot(aes(x = laryngeal_vestibule_severity_rating, fill = Dataset)) +
  geom_histogram(position = position_dodge()) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(labels = scales::label_percent()) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 9)),
        axis.title.y = element_text(margin = margin(r = 9))) +
  labs(x = "Laryngeal Vestibule Residue Rating",
       y = "Count") +
  scale_fill_manual(values = c("#62B6CB", "#1B4965")) +
  ggtitle("A")

fig1b <- synthetic_dataset |> 
  add_column(Dataset = "Synthetic") |> 
  full_join(swallowing_original_data_viz) |> 
  group_by(Dataset, bolus_consistency) |> 
  count() |> 
  mutate(bolus_consistency = case_when(bolus_consistency == "Extremely thick (IDDSI 4)" ~ "Extremely Thick",
                                       bolus_consistency == "Regular (IDDSI 7)" ~ "Regular",
                                       bolus_consistency == "Thin (IDDSI 0)" ~ "Thin")) |> 
  ggplot(aes(x = factor(bolus_consistency,
                        levels = c("Thin", "Extremely Thick", "Regular")), y = n, fill = Dataset)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(t = 9)),
        axis.title.y = element_text(margin = margin(r = 9))) +
  labs(x = "Bolus Consistency",
       y = "Count") +
  scale_fill_manual(values = c("#62B6CB", "#1B4965")) +
  ggtitle("B")

fig1_legend <- cowplot::get_plot_component(fig1a, 'guide-box-top', return_all = TRUE)

fig1_combined <- cowplot::plot_grid(fig1a + theme(legend.position = "none"), fig1b,
                   nrow = 1, align = "hv")

fig1a_combined <- cowplot::plot_grid(fig1_legend, fig1_combined,
                   nrow = 2, rel_heights = c(0.05, 1))

# export figure
ggsave(plot = fig1a_combined,
       filename = here("manuscript", "tables-figures", "figure_1.png"),
       dpi = 300,
       bg = "white",
       width = 9,
       height = 5)
```

```{r echo = FALSE}
# Create a descriptive summary of the synthetic and original dataset 
# for Curtis et al. (2023)

## Synthetic Dataset
synthetic_dataset_modified <- synthetic_dataset |> 
  mutate(
    # create binary variable for 'absent' when 0 and present when > 0
    lv_zero = if_else(laryngeal_vestibule_severity_rating == 0, "Absent", "Present"),
    # create variables for 'amount of residue when present'
    lv_present = ifelse(
      laryngeal_vestibule_severity_rating > 0,
      laryngeal_vestibule_severity_rating,
      NA
    )
  )

### Summarize zero's for laryngeal vestibule residue
synthetic_summarized_absent <- synthetic_dataset_modified |>
  group_by(bolus_consistency) |>
  count(lv_zero) |>
  mutate(total = sum(n)) |>
  mutate(perc_absent = (n / total) * 100,
         perc_absent = round(perc_absent, digits = 4)) |> 
  filter(lv_zero == "Absent") |> 
  dplyr::select(-c(n, lv_zero, total))

## Median and IQR for 'present' laryngeal vestibule residue
synthetic_summarized_present <- synthetic_dataset_modified |> 
  filter(lv_zero == "Present") |> 
  group_by(bolus_consistency) |> 
  summarise(IQR_low = quantile(lv_present, 0.25),
            median = median(lv_present),
            IQR_high = quantile(lv_present, 0.75))

## Combined synthetic dataset summary
synthetic_summary <- full_join(synthetic_summarized_present, 
                               synthetic_summarized_absent) |> 
  add_column(dataset_type = "synthetic")

## Original Dataset
original_dataset_modified <- swallowing_original_data |> 
  mutate(
    # create binary variable for 'absent' when 0 and present when > 0
    lv_zero = if_else(laryngeal_vestibule_severity_rating == 0, "Absent", "Present"),
    # create variables for 'amount of residue when present'
    lv_present = ifelse(
      laryngeal_vestibule_severity_rating > 0,
      laryngeal_vestibule_severity_rating,
      NA
    )
  )

### Summarize zero's for laryngeal vestibule residue
original_summarized_absent <- original_dataset_modified |>
  group_by(bolus_consistency) |>
  count(lv_zero) |>
  mutate(total = sum(n)) |>
  mutate(perc_absent = (n / total) * 100,
         perc_absent = round(perc_absent, digits = 4)) |> 
  filter(lv_zero == "Absent") |> 
  dplyr::select(-c(n, lv_zero, total))

## Median and IQR for 'present' laryngeal vestibule residue
original_summarized_present <- original_dataset_modified |> 
  filter(lv_zero == "Present") |> 
  group_by(bolus_consistency) |> 
  summarise(IQR_low = quantile(lv_present, 0.25),
            median = median(lv_present),
            IQR_high = quantile(lv_present, 0.75))

## Combined original dataset summary
original_summary <- full_join(original_summarized_present, 
                               original_summarized_absent) |> 
  add_column(dataset_type = "original")

### Combine original and synthetic data summaries
combined_summaries <- full_join(original_summary, synthetic_summary)

### Store number of trials by bolus consistency
number_trials <- table(swallowing_original_data$bolus_consistency) |> 
  as.data.frame()
```

Descriptively, the synthetic dataset classified `r round(combined_summaries$perc_absent[6], digits = 0)`% of laryngeal vestibule ratings on thin liquid boluses as 'absent' (i.e., 0% residue) compared to `r round(combined_summaries$perc_absent[1], digits = 0)`% in the original dataset. In the synthetic dataset, the median value on thin liquids was `r combined_summaries$median[6]` (IQR: `r combined_summaries$IQR_low[6]` - `r combined_summaries$IQR_high[6]`) compared to `r combined_summaries$median[1]` (IQR: `r combined_summaries$IQR_low[1]` - `r round(combined_summaries$IQR_high[1], digits = 2)`) in the original dataset. `r round(combined_summaries$perc_absent[4], digits = 2)`% of extremely thick liquids were classified as having no laryngeal vestibule residue compared to `r round(combined_summaries$perc_absent[2], digits = 2)`% in the original dataset. A similar pattern was appreciated for regular solids (`r round(combined_summaries$perc_absent[5], digits = 2)`% in synthetic vs. `r round(combined_summaries$perc_absent[3], digits = 2)`% in original dataset). When examined across 100 synthetic datasets, findings from the zero-inflated beta multilevel models indicate that `r swallowing$results_summary$synAgreement_pvalue_nu` and `r swallowing$results_summary$synAgreement_pvalue_mu` of synthetic datasets were not statistically significantly different than the original dataset for the zero-inflated and beta portions of the model, respectively (Table 4). Additionally, effect size categorizations were maintained for `r swallowing$results_summary$synAgreement_effectSize_nu` of both zero-inflated and beta portions of the model.

### Study 2: Vowel Acoustics as Predictors of Speech Intelligibility in Dysarthria

```{r echo = FALSE}
# Import original data ----
articulation_data_original <- rio::import(
  file = here::here("Data", "02_Articulation", "data_Acoustic Measures.csv")
) |>
  
  # Remove the reliability trials that have "_rel" in the SpeakerID variable
  dplyr::filter(!str_detect(SpeakerID, "_rel")) |>
  
  # Selecting just the variables we need
  dplyr::select(
    SpeakerID, # ID
    VSA_b, # VSA in Bark
    Int = Int_OT # intelligibility (orthographic transcriptions)
  ) |> 
  
  # convert SpeakerID to factor
  mutate(SpeakerID = as.factor(SpeakerID))

## Establish p_value for study
articulation_alpha <- .05

# Analysis Function ----
## Perform regression between Intelligibility and VSA (bark)
articulation_analyze_data <- function(data) {
  model <- lm(Int ~ VSA_b, data = data) # Creating the model
  model_fit <- summary(model, save=TRUE) # Saving the model
  
  list(
    p_value = model_fit$coefficients[8], # Extracting p-values
    effect_size = effectsize::cohens_f(model, partial = F)$Cohens_f, # Extracting effect size
    conf_int_lower = confint(model)[2, ][1], # Extracting lower CI bound
    conf_int_upper = confint(model)[2, ][2] # Extracting upper CI bound
    )
}

# Reproduce original results ----
articulation_results_original <- (articulation_analyze_data(articulation_data_original)) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < articulation_alpha ~ "sig",
      p_value >= articulation_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.40 ~ "Large",
                               abs(effect_size) >= 0.25 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )


# Create synthetic data ----
articulation_data_synthetic <- syn(articulation_data_original, 
                          # method = "ctree", 
                          m = 100,
                          seed = 2024)

## Perform analysis on each synthetic dataset
articulation_results_synthetic <- articulation_data_synthetic$syn |>
  purrr::map_df(~ articulation_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < articulation_alpha ~ "sig",
                    p_value >= articulation_alpha ~ "non-sig"),
    categorization = case_when(
      abs(effect_size) >= 0.40 ~ "Large",
      abs(effect_size) >= 0.25 ~ "Medium",
      abs(effect_size) >= 0.1 ~ "Small",
      TRUE ~ "Negligible"
    ),
     # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - articulation_results_original$p_value),
    diff_effectSize = abs(effect_size - articulation_results_original$effect_size)
  ) |> 
  add_column(domain = "Articulation")

# Compile results ----
articulation_results_summary <- data_frame(
  Domain = "Articulation",
  Study = "Thompson et al. (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(articulation_data_original),
  p_value = ifelse(
    articulation_results_original$p_value < .001,"<.001",
    sprintf("%0.3f", articulation_results_original$p_value)),
  effect_size_measure = "Cohen’s f",
  effect_size = round(articulation_results_original$effect_size, digits = 2), 
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = articulation_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(articulation_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == articulation_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(articulation_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(articulation_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = articulation_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(articulation_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == articulation_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(articulation_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(articulation_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
articulation <- list(
  data_original = articulation_data_original,
  data_synthetic = articulation_data_synthetic,
  results_original = articulation_results_original,
  results_synthetic = articulation_results_synthetic,
  results_summary = articulation_results_summary,
  alpha = articulation_alpha
)

## Cleaning up the environment
rm(
  articulation_data_original,
  articulation_data_synthetic,
  articulation_results_original,
  articulation_results_synthetic,
  articulation_results_summary,
  articulation_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = articulation,
              file = here::here("Data","02_Articulation","analysisData.RDS"))
```

Thompson et al. (2023) examined the relationship between vowel space area and speech intelligibility among 40 speakers with dysarthria of varying etiologies, including Parkinson's disease, amyotrophic lateral sclerosis, Huntington's disease, and cerebellar ataxia. A linear regression model revealed a statistically significant relationship between vowel space area and intelligibility (*p* \< .001) with a Cohen's *f* of 0.59, corresponding to a conventionally "large" effect size (Table 3).

Below we import the original dataset, wrangle the data, and generate a synthetic data set. The data wrangling steps include (1) importing the original dataset, (2) removing the reliability trials from the dataset, (3) removing a string character from the SpeakerID variable, and (4) selecting only the variables needed for the analysis.

```{r echo = TRUE, include = TRUE}
# import original data
articulation_original_data <- rio::import(
  file = here::here("Data", "02_Articulation", "data_Acoustic Measures.csv")
) |>
  # Remove the reliability trials that have "_rel" in the SpeakerID variable
  dplyr::filter(
    !grepl(
      pattern = "_rel",
      x = SpeakerID
  )) |>
  # Selecting just the variables we need
  dplyr::select(
    SpeakerID, # ID
    VSA_b, # VSA in Bark
    Int = Int_OT # intelligibility (orthographic transcriptions)
  )
```

Next, we generate a synthetic dataset with the syn() function, extract the dataset, and convert it to a dataframe. *Synthpop* provides a warning message that this dataset has fewer observations than recommended (> 130).

```{r echo = TRUE, include = TRUE}
# generate synthetic dataset
articulation_synthetic_dataset <- syn(articulation_original_data,
                                      m = 1,
                                      seed = 2024)

# Extract the synthetic dataset and convert into a data frame
articulation_synthetic_dataset <- as.data.frame(articulation_synthetic_dataset$syn)
```

Next, we compare the distributions for vowel space area and speech intelligibility between the synthetic and original dataset. Figure 2 suggests that while the synthetic data largely approximates the original dataset, there are several values that are oversampled in the synthetic dataset.

```{r echo = TRUE, include = TRUE}
# Comparison of original and synthetic datasets with synthpop package
articulation_comparison <- compare(
  articulation_synthetic_dataset, # synthetic dataset
  articulation_original_data, # original dataset
  vars = c("VSA_b",
           "Int"), # variables for comparison
  stat = "counts", # Present the raw counts for each variable
  cols = c("#62B6CB", "#1B4965") # Setting the colours in the plot
)
```

```{r echo = FALSE}
# manual visualization for aesthetics
articulation_original_data_viz <- articulation_original_data |> 
  add_column(Dataset = "Original")
  
fig2a <- articulation_synthetic_dataset |> 
  add_column(Dataset = "Synthetic") |> 
  full_join(articulation_original_data_viz) |> 
  ggplot(aes(x = VSA_b, fill = Dataset)) +
  geom_histogram(position = position_dodge()) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 9)),
        axis.title.y = element_text(margin = margin(r = 9))) +
  labs(x = "Vowel Space Area",
       y = "Count") +
  scale_fill_manual(values = c("#62B6CB", "#1B4965")) +
  ggtitle("A")

fig2b <- articulation_synthetic_dataset |> 
  add_column(Dataset = "Synthetic") |> 
  full_join(articulation_original_data_viz) |> 
  ggplot(aes(x = Int, fill = Dataset)) +
  geom_histogram(position = position_dodge()) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(t = 9)),
        axis.title.y = element_text(margin = margin(r = 9))) +
  labs(x = "Speech Intelligibility",
       y = "Count") +
  scale_fill_manual(values = c("#62B6CB", "#1B4965")) +
  ggtitle("B")

fig2_combined <- cowplot::plot_grid(fig2a + theme(legend.position = "none"), fig2b,
                   nrow = 1, align = "hv")

fig2a_combined <- cowplot::plot_grid(fig1_legend, fig2_combined,
                   nrow = 2, rel_heights = c(0.05, 1))

# export figure
ggsave(plot = fig2a_combined,
       filename = here("manuscript", "tables-figures", "figure_2.png"),
       dpi = 300,
       bg = "white",
       width = 9,
       height = 5)
```

Findings from the 100 generated synthetic datasets indicate that `r articulation$results_summary$synAgreement_pvalue` of datasets demonstrated the same inferential result (i.e., a statistically significant *p*-value). For the effect size, `r articulation$results_summary$synAgreement_effectSize` of synthetic datasets maintained a 'large' effect size categorization.

### Results for Studies 3 - 9
```{r fluency, echo = FALSE}
# Import original data ----
fluency_data_original <- rio::import(file = here::here("Data", "03_Fluency", "data_Dyslexia Stutter Master.csv")) |>
  dplyr::filter(Rejection == 0) |> 
  dplyr::filter(Group != "AWD") |>
  
  # Selecting just the variables that we need
  dplyr::select(
    Subject,
    Group,
    Nonwordrepetition
  )

# Establish p_value for study
fluency_alpha <- .05

# Analysis Function ----
# Examine the difference between neurotypical and stuttering in non-word repetition
fluency_analyze_data <- function(data) {
  t_test <- stats::t.test(
    Nonwordrepetition ~ Group,
    data = data,
    paired = FALSE,
    var.equal = FALSE
  )

  list(
    p_value = t_test$p.value,# Extracting p-values
    effect_size = effectsize::glass_delta(Nonwordrepetition ~ Group,
                                 data = data,
                                 correction = FALSE,
                                 ci = 0.95)$Glass_delta, # Extracting effect size
    conf_int_lower = t_test$conf.int[1],# Extracting lower CI bound
    conf_int_upper = t_test$conf.int[2] # Extracting upper CI bound
  )
}

# Reproduce original results ----
fluency_results_original <- (fluency_analyze_data(fluency_data_original)) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(p_value < fluency_alpha ~ "sig",
                    p_value >= fluency_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )


# Create synthetic data ----
fluency_data_synthetic <- syn(fluency_data_original,
                              # method = "ctree",
                              m = 100,
                              seed = 2024)

# Perform analysis on each synthetic dataset
fluency_results_synthetic <- fluency_data_synthetic$syn |>
  purrr::map_df(~ fluency_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < fluency_alpha ~ "sig",
                    p_value >= fluency_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - fluency_results_original$p_value),
    diff_effectSize = abs(effect_size - fluency_results_original$effect_size)
  ) |> 
  add_column(domain = "Fluency")


# Compile results ----
fluency_results_summary <- data_frame(
  Domain = "Fluency",
  Study = "Elsherif et al. (2021)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(fluency_data_original),
  p_value = ifelse(
    fluency_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", fluency_results_original$p_value)),
  effect_size_measure = "Glass' Δ",
  effect_size = round(fluency_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = fluency_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(fluency_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == fluency_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(fluency_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(fluency_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = fluency_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(fluency_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == fluency_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(fluency_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(fluency_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
fluency <- list(
  data_original = fluency_data_original,
  data_synthetic = fluency_data_synthetic,
  results_original = fluency_results_original,
  results_synthetic = fluency_results_synthetic,
  results_summary = fluency_results_summary,
  alpha = fluency_alpha
)

## Cleaning up the environment
rm(
  fluency_data_original,
  fluency_data_synthetic,
  fluency_results_original,
  fluency_results_synthetic,
  fluency_results_summary,
  fluency_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = fluency,
              file = here::here("Data","03_Fluency","analysisData.RDS"))
```

```{r voice, echo = FALSE}
# Load data ----
voice_data_original <-
  readxl::read_excel(here::here("Data/04_Voice_Resonance/RawData.xlsx"), range = "B3:Z114") |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables
  dplyr::select(c(names, efn_sd_d_b, median_of_raters))

# Establish p_value for study
voice_alpha <- .05

# Analysis Function ----
# Examine correlation between overall perceptual rating and acoustic EFn SD parameter
voice_analyze_data <- function(data) {
  # Creating the model
  model <- cor.test(data$median_of_raters,
                    data$efn_sd_d_b,
                    method = "pearson")
  
  list(
    p_value = model$p.value, # Extracting p-values
    effect_size = model$estimate, # Extracting effect size
    conf_int_lower = model[["conf.int"]][1], # Extracting lower CI bound
    conf_int_upper = model[["conf.int"]][2] # Extracting upper CI bound
  )
}

# Reproduce original results ----
voice_results_original <- voice_analyze_data(voice_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < voice_alpha ~ "sig",
      p_value >= voice_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Create synthetic data ----
voice_data_synthetic <- syn(voice_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)


# Perform analysis on each synthetic dataset
voice_results_synthetic <- voice_data_synthetic$syn |>
  purrr::map_df(~ voice_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < voice_alpha ~ "sig",
      p_value >= voice_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - voice_results_original$p_value),
    diff_effectSize = abs(effect_size - voice_results_original$effect_size)
  ) |> 
  add_column(domain = "Voice and resonance")

# Compile results ----
voice_results_summary <- data_frame(
  Domain = "Voice",
  Study = "Novotný et al. (2016)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(voice_data_original),
  p_value = ifelse(
    voice_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", voice_results_original$p_value)),
  effect_size_measure = "Correlation coefficient",
  effect_size = round(voice_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = voice_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(voice_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == voice_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(voice_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(voice_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = voice_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(voice_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == voice_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(voice_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(voice_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
voice <- list(
  data_original = voice_data_original,
  data_synthetic = voice_data_synthetic,
  results_original = voice_results_original,
  results_synthetic = voice_results_synthetic,
  results_summary = voice_results_summary,
  alpha = voice_alpha
)

## Cleaning up the environment
rm(
  voice_data_original,
  voice_data_synthetic,
  voice_results_original,
  voice_results_synthetic,
  voice_results_summary,
  voice_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = voice,
              file = here::here("Data","04_Voice_Resonance","analysisData.RDS"))
```

```{r hearing, echo = FALSE}
# Load packages
library(nlme) # linear mixed models

# Import original data ----
plane.thre.out <- read.table(here::here("Data/05_Hearing/All_Thresholds_outliersmarked-500B.txt"), 
                             sep=",", header=T)

## data wrangling from author's code
# Convert variables to factors
names(plane.thre.out)[4]<-c('head')
plane.thre.out$arm <- factor(plane.thre.out$arm)
plane.thre.out$subj <- factor(plane.thre.out$subj)
plane.thre.out$head <- factor(plane.thre.out$head)
plane.thre.out$planes <- factor(plane.thre.out$planes)
plane.thre.out$group <- ifelse(plane.thre.out$group=='EB', c('CB'), c('SC'))
plane.thre.out$group <-factor(plane.thre.out$group)

#omit all +-3SD outliers 
plane.thre.out<- subset(plane.thre.out, outlier==0)
plane.thre.out[which(plane.thre.out$thre >12),]

#divide back
horizontal.thre.out <- subset(plane.thre.out,planes=='hor')
vertical.thre.out <- subset(plane.thre.out,planes=='ver')

rownames(horizontal.thre.out) <-c(1:nrow(horizontal.thre.out)) #correct the row numbers
horizontal.thre.out[which(horizontal.thre.out$thre >10),] # we know from glmm there's one outlier
horizontal.thre.out <-horizontal.thre.out[-c(188),] 

rownames(vertical.thre.out) <-c(1:nrow(vertical.thre.out))
vertical.thre.out[which(vertical.thre.out$thre >12),] # we know from glmm there's one outlier
vertical.thre.out <-vertical.thre.out[-c(144),] 

# in the horizontal back condition left arm is right area // right arm is left area of the subject
hor.thre.front <- subset(horizontal.thre.out, head ==1)
hor.thre.back <- subset(horizontal.thre.out, head ==2)

hor.thre.back$arm <-as.character(hor.thre.back$arm)
hor.thre.back <- within(hor.thre.back, arm[arm=="left"] <-("Alien"))#just to assign into a temp
hor.thre.back <- within(hor.thre.back, arm[arm=='right'] <- 'left')
hor.thre.back <- within(hor.thre.back, arm[arm=='Alien'] <- 'right')

horizontal.thre.out <-rbind(hor.thre.front,hor.thre.back)

# combine them back
plane.thre.out <- rbind(horizontal.thre.out,vertical.thre.out)
plane.thre.out$arm <- factor(plane.thre.out$arm)
plane.thre.out$subj <- factor(plane.thre.out$subj)
plane.thre.out$head <- factor(plane.thre.out$head)
plane.thre.out$planes <- factor(plane.thre.out$planes)
plane.thre.out$group <-factor(plane.thre.out$group)

#divide back
horizontal.thre.out <- subset(plane.thre.out,planes=='hor')
vertical.thre.out <- subset(plane.thre.out,planes=='ver')

# re-label dataset as 'original_data'
hearing_data_original <- plane.thre.out |> 
  dplyr::select(c(thre, planes, group, head, subj))

# Removing unneeded items
rm(plane.thre.out, hor.thre.back, hor.thre.front, horizontal.thre.out, vertical.thre.out)

# Establish p_value for study
hearing_alpha <- .05

# Analysis Function ----
# LMM on planes: Auditory localization performance by plane, group and position (head)
hearing_analyze_data <- function(data) {
  # Creating the model
  model <-
    nlme::lme(
      thre ~ 1 + planes * group * head,
      random = ~ 1 | subj,
      weights = varIdent(form = ~ 1 | planes),
      data = data
    )
  # Saving the model
  model_fit <- summary(model, save=TRUE)
  
  model_results <- anova(model_fit) |> 
    as.data.frame()
  
  list(
    p_value = model_results$`p-value`[3], # Extracting p-values
    # Extracting effect size - standardized effect size (Haddock et al., 1998; Hasselblad & Hedges, 1995)
    effect_size = (sqrt(3/pi))*exp(model_fit$coefficients$fixed[3]),
    conf_int_lower = intervals(model)[["fixed"]][3,][1], # Extracting lower CI bound
    conf_int_upper = intervals(model)[["fixed"]][3,][3] # Extracting upper CI bound
  )
}


# Reproduce original results ----
hearing_results_original <- hearing_analyze_data(hearing_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < hearing_alpha ~ "sig",
      p_value >= hearing_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )



# Create synthetic data ----
hearing_data_synthetic <- syn(hearing_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
hearing_results_synthetic <- hearing_data_synthetic$syn |>
  purrr::map_df(~ hearing_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < hearing_alpha ~ "sig",
                    p_value >= hearing_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - hearing_results_original$p_value),
    diff_effectSize = abs(effect_size - hearing_results_original$effect_size)
  ) |> 
  add_column(domain = "Hearing")


# Compile results ----
hearing_results_summary <- data_frame(
  Domain = "Hearing",
  Study = "Battal et al. (2019)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(hearing_data_original),
  p_value = ifelse(
    hearing_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", hearing_results_original$p_value)),
  effect_size_measure = "√(3/𝜋) x odds ratio",
  effect_size = round(hearing_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = hearing_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(hearing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == hearing_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(hearing_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(hearing_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = hearing_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(hearing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == hearing_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(hearing_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(hearing_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
hearing <- list(
  data_original = hearing_data_original,
  data_synthetic = hearing_data_synthetic,
  results_original = hearing_results_original,
  results_synthetic = hearing_results_synthetic,
  results_summary = hearing_results_summary,
  alpha = hearing_alpha
)

## Cleaning up the environment
rm(
  hearing_data_original,
  hearing_data_synthetic,
  hearing_results_original,
  hearing_results_synthetic,
  hearing_results_summary,
  hearing_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = hearing,
              file = here::here("Data","05_Hearing","analysisData.RDS"))
```

```{r comm_modalities, echo = FALSE}

# Import original data ----
## Function to load in AAC data from the author's code
qualtrics_csv = function(file){
  data = read_csv(file, col_names = FALSE, skip = 3)
  names = read_lines(file, n_max = 1) |> 
    str_split(pattern = ",") |> 
    unlist()
  names(data) = names
  labels = read_csv(file, col_names = FALSE, skip = 1, n_max = 1)
  labels = t(labels)
  labels = labels[,1]
  labels = setNames(as.list(labels), names)
  data = labelled::set_variable_labels(data, .labels = labels, .strict = FALSE)
  data = janitor::clean_names(data)
  data
}

# Load original data
aac_data_original <- qualtrics_csv(file = here::here("Data", "06_AAC", "data_Combined CSV data.csv")) |> 
  janitor::remove_empty("cols") |> 
  janitor::remove_empty("rows") |> 
  filter(progress > 98) |> 
  dplyr::select(start_date, participant_id, q2:x2f) |> 
  dplyr::select(q87_1_1, q87_1_2, q87_1_3, q87_1_5, q87_1_6, q87_1_7,
         q87_2_1, q87_2_2, q87_2_3, q87_2_5, q87_2_6, q87_2_7,
         q87_3_1, q87_3_2, q87_3_3, q87_3_5, q87_3_6, q87_3_7,
         q89_1_1, q89_1_2, q89_1_3, q89_1_5, q89_1_6, q89_1_7,
         q89_2_1, q89_2_2, q89_2_3, q89_2_5, q89_2_6, q89_2_7,
         q89_3_1, q89_3_2, q89_3_3, q89_3_5, q89_3_6, q89_3_7) |> 
  tidyr::pivot_longer(everything()) |>
  tidyr::separate(name,
           into = c("var", "time", "question"),
           sep = "_") |>
  dplyr::mutate(time = factor(
    time,
    labels = c(
      "Prior to\nMarch 2020",
      "March to\nJuly 2020",
      "August 2020 to\nSpring 2021"
    )
  ),
  var = factor(var, labels = c("Assessment", "Intervention"))) |>
  tidyr::drop_na(value) |> 
    dplyr::select(-question)

# Establish p_value for study
aac_alpha <- .05

# Analysis Function ----
# Examine the chi-square difference in lack of/limited Internet and technology barriers by timepoint
aac_analyze_data <- function(data) {
  chi_squared <- data |>
    dplyr::group_by(var, value) |>
    dplyr::summarize(prop = list(chisq.test(table(time))),
                     .groups = "drop_last") |>
    dplyr::mutate(
      chi = map_dbl(prop, ~ .x$statistic),
      df = map_dbl(prop, ~ .x$parameter),
      p = map_dbl(prop, ~ .x$p.value)
    ) |>
    dplyr::filter(var == "Intervention") |>
    dplyr::filter(value == "Lack of/limited internet")
  
  list(
    p_value = chi_squared$p,
    # Extracting p-values
    effect_size = chi_squared$chi,
    # There are no confidence intervals for chi-squared tests
    conf_int_lower = NA,
    conf_int_upper = NA 
  )
}

# Reproduce original results ----
aac_results_original <- aac_analyze_data(aac_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < aac_alpha ~ "sig",
      p_value >= aac_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Create synthetic data ----
aac_data_synthetic <- syn(aac_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
aac_results_synthetic <- aac_data_synthetic$syn |>
  purrr::map_df(~ aac_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < aac_alpha ~ "sig",
                    p_value >= aac_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - aac_results_original$p_value),
    diff_effectSize = abs(effect_size - aac_results_original$effect_size)
  ) |> 
  add_column(domain = "Communication modalities")

# Compile results ----
aac_results_summary <- data_frame(
  Domain = "Communication modalities",
  Study = "King et al. (2022)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(aac_data_original),
  p_value = ifelse(
    aac_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", aac_results_original$p_value)),
  effect_size_measure = "Cohen's 𝜔",
  effect_size = round(aac_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = aac_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(aac_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == aac_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(aac_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(aac_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = aac_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(aac_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == aac_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(aac_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(aac_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
aac <- list(
  data_original = aac_data_original,
  data_synthetic = aac_data_synthetic,
  results_original = aac_results_original,
  results_synthetic = aac_results_synthetic,
  results_summary = aac_results_summary,
  alpha = aac_alpha
)

## Cleaning up the environment
rm(
  aac_data_original,
  aac_data_synthetic,
  aac_results_original,
  aac_results_synthetic,
  aac_results_summary,
  aac_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = aac,
              file = here::here("Data","06_AAC","analysisData.RDS"))
```

```{r recep_language, echo = FALSE}
library(DescTools) # install.packages("DescTools")

# Import original data ----
language_data_original <-
  read.csv(here::here("Data/07_Language/demographics_CAT_reading.csv")) |>
  # select only relevant variables
  dplyr::select(c(Filename, edu, tpost_cat_read_total))

# Establish p_value for study
language_alpha <- .05

# Analysis Function ----
# Examine correlation between education (years) and reading t-scores
language_analyze_data <- function(data) {
  model <- cor.test(data$edu,
                    data$tpost_cat_read_total,
                    exact = F,
                    method = "spearman") # Creating the model
  
  # Obtaining CI's for SpearmansRho using a bootstrapping method
  model_CI <- DescTools::SpearmanRho(data$edu,
                                     data$tpost_cat_read_total,
                                     conf.level = .95)
  
  list(
    p_value = model$p.value, # Extracting p-values
    effect_size = model$estimate, # Extracting effect size
    # CI for S
    conf_int_lower = model_CI[2],
    conf_int_upper = model_CI[3]
  )
}

# Reproduce original results ----
language_results_original <- language_analyze_data(language_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < language_alpha ~ "sig",
      p_value >= language_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Create synthetic data ----
language_data_synthetic <- syn(language_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
language_results_synthetic <- language_data_synthetic$syn |>
  purrr::map_df(~ language_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < language_alpha ~ "sig",
      p_value >= language_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - language_results_original$p_value),
    diff_effectSize = abs(effect_size - language_results_original$effect_size)
  ) |> 
  add_column(domain = "Receptive and expressive language")

# Compile results ----
language_results_summary <- data_frame(
  Domain = "Language",
  Study = "Kearney et al. (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(language_data_original),
  p_value = ifelse(
    language_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", language_results_original$p_value)),
  effect_size_measure = "Correlation coefficient",
  effect_size = round(language_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = language_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(language_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == language_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(language_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(language_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = language_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(language_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == language_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(language_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(language_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
language <- list(
  data_original = language_data_original,
  data_synthetic = language_data_synthetic,
  results_original = language_results_original,
  results_synthetic = language_results_synthetic,
  results_summary = language_results_summary,
  alpha = language_alpha
)

## Cleaning up the environment
rm(
  language_data_original,
  language_data_synthetic,
  language_results_original,
  language_results_synthetic,
  language_results_summary,
  language_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = language,
              file = here::here("Data","07_Language","analysisData.RDS"))
```

```{r}
# # Load data
# cognition_original_data <-
#   read.csv(here::here("Data/08_Cognition/emoji_affect_recognition_data.csv")) |>
#   # select only relevant variables
#   dplyr::select(c(SubjectID, Correct, Group, Condition, Sex)) |>
#   # dummy-code Group (NC = 1, TBI = 0) and Sex (F = 1, M = 0)
#   mutate(
#     Group_NC = if_else(Group == "NC", 1, 0),
#     Sex_F = if_else(Sex == "Female", 1, 0)) |>
#   # treat categorical variables as factors and set reference levels
#   mutate(
#     SubjectID = as.factor(SubjectID),
#     Correct = as.factor(Correct),
#     Group_NC = relevel(as.factor(Group_NC), '1', '0'),
#     Condition = relevel(as.factor(Condition), 'BasicFace', 'BasicEmoji', 'SocialEmoji'),
#     Sex_F =  relevel(as.factor(Sex_F), '1', '0')
#   ) |> 
#   dplyr::select(-c(Group, Sex))
# 
# # Set up Helmert contrast coding for Condition
# # https://marissabarlaz.github.io/portfolio/contrastcoding/
# # Contrast 1: Basic Emotion Faces vs average of Basic Emotion Emoji and Social Emotion Emoji
# # Contrast 2: Basic Emotion Emoji vs Social Emotion Emoji
# helmert = matrix(c(2/3, -1/3, -1/3, 0, 1/2, -1/2), ncol = 2)
# contrasts(cognition_original_data$Condition) = helmert
# 
# # Models: Effect of stimulus condition on emotion recognition accuracy
# # Note: Model first failed to converge when using default control settings, so changed optimizer to bobyqa as suggested here: https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
# m1 <- lme4::glmer(Correct ~ Group_NC + Condition + Sex_F + Group_NC*Condition*Sex_F + (1 |SubjectID), data = cognition_original_data, family = binomial(link = "logit"), control=lme4::glmerControl(optimizer="bobyqa",
#                             optCtrl=list(maxfun=2e5)))
# summary(m1)
# 
# # Set group reference level to TBI (0) and rerun model
# cognition_original_data$Group_NC = relevel(cognition_original_data$Group_NC, '0', '1')
# m2 <- lme4::glmer(Correct ~ Group_NC + Condition + Sex_F + Group_NC*Condition*Sex_F + (1 |SubjectID), data = cognition_original_data, family = binomial(link = "logit"), control=lme4::glmerControl(optimizer="bobyqa",
#                             optCtrl=list(maxfun=2e5)))
# summary(m2)
# 
# # Odds ratios and confidence intervals
# tidy(m1,conf.int=TRUE,exponentiate=TRUE,effects="fixed") # Condition1 conf.high = 5.86, reported in paper as 5.87
# tidy(m2,conf.int=TRUE,exponentiate=TRUE,effects="fixed")
# 
# # Examine p-value for Group X Condition Interaction
# tidy(m1,conf.int=TRUE,exponentiate=TRUE,effects="fixed")[7,]$`p.value`
# 
# # Calculate standardized effect size from odds ratio for Group X Condition Interaction
# ## this is a "medium" effect (>= 0.50 & < 0.80)
# sqrt((3/pi))*tidy(m1,conf.int=TRUE,exponentiate=TRUE,effects="fixed")[7,]$estimate
```

```{r cognition, echo = FALSE}
# Load packages
library(lme4) # glmer
library(lmerTest) # p-values for glmer
library(broom.mixed) # model summaries

# Import original data ----
cognition_data_original <-
  read.csv(here::here("Data/08_Cognition/emoji_affect_recognition_data.csv")) |>
  # select only relevant variables
  dplyr::select(c(SubjectID, Correct, Group, Condition, Sex)) |>
  # dummy-code Group (NC = 1, TBI = 0) and Sex (F = 1, M = 0)
  mutate(
    Group_NC = if_else(Group == "NC", 1, 0),
    Sex_F = if_else(Sex == "Female", 1, 0)) |>
  # treat categorical variables as factors and set reference levels
  mutate(
    SubjectID = as.factor(SubjectID),
    Correct = as.factor(Correct),
    Group_NC = relevel(as.factor(Group_NC), '1', '0'),
    Condition = relevel(as.factor(Condition), 'BasicFace', 'BasicEmoji', 'SocialEmoji'),
    Sex_F =  relevel(as.factor(Sex_F), '1', '0')
  ) |> 
  dplyr::select(-c(Group, Sex))

# Set up Helmert contrast coding for Condition
# https://marissabarlaz.github.io/portfolio/contrastcoding/
# Contrast 1: Basic Emotion Faces vs average of Basic Emotion Emoji and Social Emotion Emoji
# Contrast 2: Basic Emotion Emoji vs Social Emotion Emoji
helmert = matrix(c(2/3, -1/3, -1/3, 0, 1/2, -1/2), ncol = 2)
contrasts(cognition_data_original$Condition) = helmert

cognition_alpha <- .05

# Analysis Function ----
## Models: Effect of stimulus condition on emotion recognition accuracy
## Note: Model first failed to converge when using default control settings, so changed optimizer to bobyqa as suggested here: https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
cognition_analyze_data <- function(data) {
  # Creating the model
  m1 <-
    lme4::glmer(Correct ~ Group_NC + Condition + Sex_F + Group_NC * Condition * Sex_F + (1 | SubjectID),
      data = data,
      family = binomial(link = "logit"),
      control = glmerControl(optimizer = "bobyqa",
                             optCtrl = list(maxfun = 2e5))
    )
  
  # Set group reference level to TBI (0) and rerun model
  data$Group_NC = relevel(data$Group_NC, '0', '1')
  m2 <-
    lme4::glmer(
      Correct ~ Group_NC + Condition + Sex_F + Group_NC * Condition * Sex_F + (1 | SubjectID),
      data = data,
      family = binomial(link = "logit"),
      control = lme4::glmerControl(optimizer = "bobyqa",
                             optCtrl = list(maxfun = 2e5))
    )
  
  list(
    # Extracting p-values
    p_value = tidy(
      m1,
      conf.int = TRUE,
      exponentiate = TRUE,
      effects = "fixed")[7,]$`p.value`,
    # Extract effect size from odds ratio for Group (NC0) X Condition (2) Interaction
    effect_size = sqrt((3 / pi)) * tidy(m1,
                        conf.int = TRUE,
                        exponentiate = TRUE,
                        effects = "fixed")[7, ]$estimate,
    # Extracting lower CI bound
    conf_int_lower = tidy(
      m1,
      conf.int = TRUE,
      exponentiate = TRUE,
      effects = "fixed")[7,]$conf.low,
    # Extracting upper CI bound
    conf_int_upper = tidy(
      m1,
      conf.int = TRUE,
      exponentiate = TRUE,
      effects = "fixed")[7,]$conf.high
  )
}

# Reproduce original results ----
cognition_results_original <- cognition_analyze_data(cognition_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < cognition_alpha ~ "sig",
      p_value >= cognition_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )


# Create synthetic data ----
cognition_data_synthetic <- syn(cognition_data_original, 
                                   method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
cognition_results_synthetic <- cognition_data_synthetic$syn |>
  purrr::map_df(~ cognition_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < cognition_alpha ~ "sig",
      p_value >= cognition_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - cognition_results_original$p_value),
    diff_effectSize = abs(effect_size - cognition_results_original$effect_size)
  ) |> 
  add_column(domain = "Cognition")

# Compile results ----
cognition_results_summary <- data_frame(
  Domain = "Cognition",
  Study = "Clough et al. (2023))",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(cognition_data_original),
  p_value = ifelse(
    cognition_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", cognition_results_original$p_value)),
  effect_size_measure = "√(3/𝜋) x odds ratio",
  effect_size = round(cognition_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = cognition_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(cognition_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == cognition_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(cognition_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(cognition_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = cognition_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(cognition_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == cognition_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(cognition_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(cognition_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
cognition <- list(
  data_original = cognition_data_original,
  data_synthetic = cognition_data_synthetic,
  results_original = cognition_results_original,
  results_synthetic = cognition_results_synthetic,
  results_summary = cognition_results_summary,
  alpha = cognition_alpha
)

## Cleaning up the environment
rm(
  cognition_data_original,
  cognition_data_synthetic,
  cognition_results_original,
  cognition_results_synthetic,
  cognition_results_summary,
  cognition_alpha,
  helmert
)

# Saving the data and results for easy importing later.
base::saveRDS(object = cognition,
              file = here::here("Data","08_Cognition","analysisData.RDS"))
```

```{r social_comm, echo = FALSE}
# Import original data ----
social_data_original <-
  read.csv(here::here("Data/09_Social_Communication/Study1Comp.csv"),
           fileEncoding = 'UTF-8-BOM') |> 
  plyr::ddply(
    c("ParGroup", "Subj", "Gender"),
    summarise,
    N    = length(Accuracy),
    acc  = sum(Accuracy),
    pct = mean(Accuracy),
    grade = mean(Grade),
    age = mean(Months),
    NVIQ = mean(SSIQ)
  ) |> 
  dplyr::select(c(Subj, NVIQ, ParGroup))

# Establish p_value for study
social_alpha <- .05

# Analysis Function ----
# Model: Non-verbal IQ by group (ASD, TD) two-tailed Fisher’s Exact Test
social_analyze_data <- function(data) {
  model <- aov(NVIQ ~ ParGroup,
               data = data) # Creating the model
  model_summary <- summary(model)
  
  list(
    p_value = model_summary[[1]]$'Pr(>F)'[1],
    # Extracting p-values
    effect_size = effectsize::cohens_d(NVIQ ~ ParGroup,
                     data = data,
                     ci = 0.95)$Cohens_d,
    # Extracting effect size
    conf_int_lower = confint(model)[2, ][1],
    # Extracting lower CI bound
    conf_int_upper = confint(model)[2,][2] # Extracting upper CI bound
  )
}

# Reproduce original results ----
social_results_original <- social_analyze_data(social_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < social_alpha ~ "sig",
      p_value >= social_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )



# Create synthetic data ----
social_data_synthetic <- syn(social_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
social_results_synthetic <- social_data_synthetic$syn |>
  purrr::map_df(~ social_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < social_alpha ~ "sig",
      p_value >= social_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - social_results_original$p_value),
    diff_effectSize = abs(effect_size - social_results_original$effect_size)
  ) |> 
  add_column(domain = "Social aspects of communication")

# Compile results ----
social_results_summary <- data_frame(
  Domain = "Social aspects of communication",
  Study = "Chanchaochai & Schwarz (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(social_data_original),
  p_value = ifelse(
    social_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", social_results_original$p_value)),
  effect_size_measure = "Cohen’s d",
  effect_size = round(social_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = social_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(social_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == social_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(social_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(social_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = social_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(social_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == social_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(social_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(social_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
social <- list(
  data_original = social_data_original,
  data_synthetic = social_data_synthetic,
  results_original = social_results_original,
  results_synthetic = social_results_synthetic,
  results_summary = social_results_summary,
  alpha = social_alpha
)

## Cleaning up the environment
rm(
  social_data_original,
  social_data_synthetic,
  social_results_original,
  social_results_synthetic,
  social_results_summary,
  social_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = social,
              file = here::here("Data","09_Social_Communication","analysisData.RDS"))
```

Studies in the domains of fluency, voice and resonance, communication modalities, receptive and expressive language, and social aspects of communication demonstrated more than 95% *p*-value agreement between the original and synthetic datasets (Table 3). Among studies that demonstrated lower agreement, the absolute mean difference between the synthetic *p*-values and the original *p*-value was `r round(articulation$results_summary$absDiff_pvalue_mean, digits = 2)` (*SD* = `r round(articulation$results_summary$absDiff_pvalue_sd, digits = 2)`) for articulation, `r round(hearing$results_summary$absDiff_pvalue_mean, digits = 2)` (*SD* = `r round(hearing$results_summary$absDiff_pvalue_sd, digits = 2)`) for hearing, and `r round(cognition$results_summary$absDiff_pvalue_mean, digits = 2)` (*SD* = `r round(cognition$results_summary$absDiff_pvalue_sd, digits = 2)`) for cognitive aspects of communication (Figure 3). For effect size categorization agreement, studies in the domains of fluency, hearing, communication modalities, and cognitive aspects of communication maintained the effect size categorization of the original study. Among studies that demonstrated lower effect size cateogrization agreement, the absolute mean difference between the effect size from synthetic datasets and the original study's effect size was `r round(articulation$results_summary$absDiff_effectSize_mean, digits = 2)` (SD = `r round(articulation$results_summary$absDiff_effectSize_sd, digits = 2)`) for articulation, `r round(voice$results_summary$absDiff_effectSize_mean, digits = 2)` (*SD* = `r round(voice$results_summary$absDiff_effectSize_sd, digits = 2)`) for voice and resonance, `r round(language$results_summary$absDiff_effectSize_mean, digits = 2)` (*SD* = `r round(language$results_summary$absDiff_effectSize_sd, digits = 2)`) for receptive and expressive language, and `r round(social$results_summary$absDiff_effectSize_mean, digits = 2)` (*SD* = `r round(social$results_summary$absDiff_effectSize_sd, digits = 2)`) for social aspects of communication.

```{r dispersion figures, echo = FALSE}
# Visualize p-value distributions
library(scales)

articulation_pval_viz <- 
  articulation$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(0.05))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  # scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.59, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("A: Articulation",
          subtitle = "Mean difference of 0.05 (SD = 0.10)") +
  geom_vline(xintercept = log(articulation$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)

fluency_pval_viz <- 
  fluency$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(0.05))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  # scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.59, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("B: Fluency",
          subtitle = "Mean difference of 0.0000007 (SD = 0.00003)") +
  geom_vline(xintercept = log(fluency$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
voice_pval_viz <- 
  voice$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(0.05))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  # scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.59, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("C: Voice and resonance",
          subtitle = "Mean difference of 0.005 (SD = 0.05)") +
  geom_vline(xintercept = log(voice$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
hearing_pval_viz <- 
  hearing$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(0.05))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  # scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.59, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("D: Hearing",
          subtitle = "Mean difference of 0.03 (SD = 0.04)") +
  geom_vline(xintercept = log(hearing$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
language_pval_viz <- 
  language$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(0.05))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  # scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.59, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("E: Receptive and expressive language",
          subtitle = "Mean difference of 0.0009 (SD = 0.002)") +
  geom_vline(xintercept = log(language$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
aac_pval_viz <- 
  aac$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(0.05))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  # scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.59, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("F: Communication modalities",
          subtitle = "Mean difference of 0.00004 (SD = 0.0002)") +
  geom_vline(xintercept = log(aac$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
cognition_pval_viz <- 
  cognition$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(0.05))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  # scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.59, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("G: Cognition",
          subtitle = "Mean difference of 0.25 (SD = 0.28)") +
  geom_vline(xintercept = log(cognition$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
social_pval_viz <- 
  social$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(0.05))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  # scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.59, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("H: Social aspects of communication",
          subtitle = "Mean difference of 0.01 (SD = 0.07)") +
  geom_vline(xintercept = log(social$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)

# Combine effect size figures together
pval_combined <- cowplot::plot_grid(articulation_pval_viz,
                   fluency_pval_viz,
                   voice_pval_viz,
                   hearing_pval_viz,
                   language_pval_viz,
                   aac_pval_viz,
                   cognition_pval_viz,
                   social_pval_viz, 
                   nrow = 4, 
                   align = "hv"
                   )

# save figure
ggsave(
  filename = here::here(
    "Manuscript/tables-figures/figure_3.png"),
    pval_combined,
    height = 10,
    width = 10,
    dpi = 300,
    bg = "white"
  )

 # Visualize effect size distributions
articulation_es_viz <- 
  articulation$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=0.40, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.59, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "Cohen's f",
       y = "Count") +
  ggtitle("A: Articulation",
          subtitle = "Mean difference of 0.19 (SD = 0.12)") +
  geom_vline(xintercept = 0.59, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)

fluency_es_viz <- 
  fluency$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=Inf, xmax=-Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(-4, -3, -2.18, -1), limits = c(-4, -1)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "Glass' Δ",
       y = "Count") +
  ggtitle("B: Fluency",
          subtitle = "Mean difference of 0.36 (SD = 0.28)") +
  geom_vline(xintercept = -2.18, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
voice_es_viz <- 
  voice$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=0.50, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(0, 0.20, 0.40, 0.51, 0.60, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "Correlation coefficient",
       y = "Count") +
  ggtitle("C: Voice and resonance",
          subtitle = "Mean difference of 0.09 (SD = 0.07)") +
  geom_vline(xintercept = 0.51, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
hearing_es_viz <- 
  hearing$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(1, 1.2, 1.4, 1.56, 1.8, 2, 2.2), limits = c(0.90, 2.20)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "√(3/𝜋) x odds ratio",
       y = "Count") +
  ggtitle("D: Hearing",
          subtitle = "Mean difference of 0.18 (SD = 0.14)") +
  geom_vline(xintercept = 1.56, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
language_es_viz <- 
  language$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=0.50, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(0.30, 0.4, 0.5, 0.59, 0.7, 0.8), 
                     limits = c(0.30, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "Correlation coefficient",
       y = "Count") +
  ggtitle("E: Receptive and expressive language",
          subtitle = "Mean difference of 0.06 (SD = 0.05)") +
  geom_vline(xintercept = 0.59, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
aac_es_viz <- 
  aac$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(10, 20, 30, 34.6, 40, 50), limits = c(10, 50)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "Cohen's 𝜔",
       y = "Count") +
  ggtitle("F: Communication modalities",
          subtitle = "Mean difference of 7.24 (SD = 5.08)") +
  geom_vline(xintercept = 34.6, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
cognition_es_viz <- 
  cognition$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=0.80, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(0.80, 1, 1.25, 1.53, 1.75, 2), limits = c(0.70, 2.1)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "√(3/𝜋) x odds ratio",
       y = "Count") +
  ggtitle("G: Cognition",
          subtitle = "Mean difference of 0.28 (SD = 0.19)") +
  geom_vline(xintercept = 1.53, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
social_es_viz <- 
  social$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=-0.80, xmax=-Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(-2, -1.5, -0.85, -0.5, 0), limits = c(-2, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "√(3/𝜋) x odds ratio",
       y = "Count") +
  ggtitle("H: Social aspects of communication",
          subtitle = "Mean difference of 0.21 (SD = 0.20)") +
  geom_vline(xintercept = -0.85, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)

# Combine effect size figures together
es_combined <- cowplot::plot_grid(articulation_es_viz,
                   fluency_es_viz,
                   voice_es_viz,
                   hearing_es_viz,
                   language_es_viz,
                   aac_es_viz,
                   cognition_es_viz,
                   social_es_viz, 
                   nrow = 4, 
                   align = "hv"
                   )

# save figure
ggsave(
  filename = here::here(
    "Manuscript/tables-figures/figure_4.png"),
    es_combined,
    height = 10,
    width = 10,
    dpi = 300,
    bg = "white"
  )
```

##### Table 4 here.

```{r table 4, echo = FALSE}

# This code reads in a template, generates table 3, 
# and then saves it as word doc in the appropriate folder

# Load in the data
swallowing <- base::readRDS(file = here::here("Data","01_Swallowing","analysisData.RDS"))
articulation <- base::readRDS(file = here::here("Data","02_Articulation","analysisData.RDS"))
fluency <- base::readRDS(file = here::here("Data","03_Fluency","analysisData.RDS"))
voice <- base::readRDS(file = here::here("Data","04_Voice_Resonance","analysisData.RDS"))
hearing <- base::readRDS(file = here::here("Data","05_Hearing","analysisData.RDS"))
aac <- base::readRDS(file = here::here("Data","06_AAC","analysisData.RDS"))
language <- base::readRDS(file = here::here("Data","07_Language","analysisData.RDS"))
cognition <- base::readRDS(file = here::here("Data","08_Cognition","analysisData.RDS"))
social <-base::readRDS(file = here::here("Data","09_Social_Communication","analysisData.RDS"))

cols2 <- tibble(
  `Domain` = c(
    "Swallowing",
    "Articulation",
    "Fluency",
    "Voice and resonance",
    "Hearing",
    "Communication modalities",
    "Receptive and expressive language",
    "Cognitive aspects of communication",
    "Social aspects of communication"
  ),
  `Study` = c(
    "Curtis et al. (2023)",
    "Thompson et al. (2023)",
    "Elsherif et al. (2021)",
    "Novotný et al. (2016)",
    "Battal et al. (2019)",
    "King et al. (2022)",
    "Kearney et al. (2023)",
    "Clough et al. (2023)",
    "Chanchaochai & Schwarz (2023)"
  ),
  `Sample Size` = c(
    swallowing$results_summary$N,
    articulation$results_summary$N,
    fluency$results_summary$N,
    voice$results_summary$N,
    hearing$results_summary$N,
    aac$results_summary$N,
    language$results_summary$N,
    cognition$results_summary$N,
    social$results_summary$N
  ),
  `P-value` = c(
    swallowing$results_summary$p_value_mu,
    articulation$results_summary$p_value,
    fluency$results_summary$p_value,
    voice$results_summary$p_value,
    sub("^0", "", hearing$results_summary$p_value),
    aac$results_summary$p_value,
    language$results_summary$p_value,
    sub("^0", "", cognition$results_summary$p_value),
    social$results_summary$p_value
  ),
  `Effect Size Measure` = c(
    swallowing$results_summary$effect_size_measure_mu,
    articulation$results_summary$effect_size_measure,
    fluency$results_summary$effect_size_measure,
    voice$results_summary$effect_size_measure,
    hearing$results_summary$effect_size_measure,
    aac$results_summary$effect_size_measure,
    language$results_summary$effect_size_measure,
    cognition$results_summary$effect_size_measure,
    social$results_summary$effect_size_measure
  ),
  `Effect Size` = c(
    sub("^0", "", swallowing$results_summary$effect_size_mu),
    sub("^0", "", articulation$results_summary$effect_size),
    sub("^0", "", fluency$results_summary$effect_size),
    sub("^0", "", voice$results_summary$effect_size),
    sub("^0", "", hearing$results_summary$effect_size),
    sub("^0", "", aac$results_summary$effect_size),
    sub("^0", "", language$results_summary$effect_size),
    sub("^0", "", cognition$results_summary$effect_size),
    sub("0.", ".", social$results_summary$effect_size)
  ),
  `P-value\nAgreement` = c(
    paste0(swallowing$results_summary$synAgreement_pvalue_nu, 
           " (zero-inflated) & ", 
           swallowing$results_summary$synAgreement_pvalue_mu, 
           " (beta)"),
    articulation$results_summary$synAgreement_pvalue,
    fluency$results_summary$synAgreement_pvalue,
    voice$results_summary$synAgreement_pvalue,
    hearing$results_summary$synAgreement_pvalue,
    aac$results_summary$synAgreement_pvalue,
    language$results_summary$synAgreement_pvalue,
    cognition$results_summary$synAgreement_pvalue,
    social$results_summary$synAgreement_pvalue
  ),
  `Effect Size Categorization\nAgreement` = c(
    paste0(swallowing$results_summary$synAgreement_effectSize_nu, 
           " (zero-inflated) & ", 
           swallowing$results_summary$synAgreement_effectSize_mu, 
           " (beta)"),
    articulation$results_summary$synAgreement_effectSize,
    fluency$results_summary$synAgreement_effectSize,
    voice$results_summary$synAgreement_effectSize,
    hearing$results_summary$synAgreement_effectSize,
    aac$results_summary$synAgreement_effectSize,
    language$results_summary$synAgreement_effectSize,
    cognition$results_summary$synAgreement_effectSize,
    social$results_summary$synAgreement_effectSize
  )
)

set_flextable_defaults(font.family = "Times New Roman")

t2 <- flextable(cols2) |>
  set_caption("Table 4: Stability of synthetic datasets across ASHA domains.") |> 
  set_table_properties(layout = "autofit", width = 1) |> 
  footnote(
    i = 1,
    j = 4,
    value = as_paragraph(
      "Caption: 100 synthetic datasets were generated for each domain. For the Swallowing domain, a zero-inflated beta multilevel model was performed to directly compare original and synthetic datasets. The percentage of non-significant p-values, which is indicative of no statistically significant difference between dataset types, is shown for both zero-inflated and beta portions of the model."
    ),
    ref_symbols = c("a"),
    part = "header"
  )

doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t2)
fileout <- here("manuscript", "tables-figures", "table4.docx") # uncomment to write in your working directory
print(doc, target = fileout)
```

# Discussion

Although computational reproducibility is a core principle of science, data sharing is uncommon in CSD, partly due to concerns regarding disclosure risk [@pfeiffer_etal24]. This study demonstrates the utility of synthetic datasets to protect participant confidentiality while preserving the statistical properties and relationships of the original analysis data. The utility of synthetic data is further strengthened by the range of datasets included in the current study, which varied by domain (across nine ASHA domains), sample size (from 40 to >8,000 data points), statistical models (from simple correlations to multilevel model with 3-way interactions), and effect sizes (from conventionally "small" to "large"). These results suggest that synthetic datasets can be effectively used across a wide range of studies in the field of CSD to preserve participant confidentiality when sharing data.

One key finding is that lower agreement between synthetic and original datasets was not attributed to sample size, despite the *synthpop* package's recommendation of a minimum of 130 observations for generating synthetic datasets [@nowok_etal16]. For example, in the original study from the cognition domain, which included over 8,000 observations, only 35% of synthetic datasets maintained the same inferential result as the original dataset. Instead, *p*-value and effect size agreement between the synthetic and original datasets was influenced by the original data’s proximity to the statistical significance or effect size thresholds. For example, the original cognition study reported a *p*-value of .013, resulting in a 35% agreement rate for synthetic datasets. Conversely, studies that reported an original *p*-value of <.001 showed a *p*-value agreement rate of 97-100%, with the exception of the articulation study, which had a *p*-value agreement of 71%.

These findings highlight the importance of verifying the accuracy of synthetic datasets and providing these comparisons in supplemental manuscript materials. To ensure synthetic data quality, researchers should generate multiple versions of a synthetic dataset and select the one that most closely reproduces the statistical findings of the original analysis. If the synthetic dataset fails to sufficiently maintain these relationships, it should not be shared.

This study is not without limitations. We used predetermined thresholds (e.g., 'significant' *p*-values and effect size categories) to evaluate whether synthetic data maintained the relationships observed in the original study. When the original analyses had *p*-values near the threshold for significance (e.g., .01 < *p* < .05) or effect sizes near the boundary of a category, lower agreement was more likely. This likely reflects the distribution of synthetic data across both sides of these thresholds rather than actual poor agreement (Figures 3 & 4). Additionally, it's important to recognize that synthetic data is inherently a proxy and cannot entirely preserve all statistical properties of the original dataset. Therefore, researchers should provide de-identified (or identifiable when ethical approval is obtained) data whenever possible, as well as evaluate the utility of the synthetic dataset in the context of their own study. Finally, open data alone does not ensure computational reproducibility. Instead, both open data and accompanied code or syntax is required to reproduce analyses. In fact, recent research showed that a high percentage of findings from registered reports that provided open data were unable to be reproduced [@obels_etal20a]. Reproducible workflows in languages like R have been proposed and warrant consideration [@peikert_etal21a].

## Data Sharing Framework

Sharing data is not a trivial task and requires careful planning from the beginning of a project. Unfortunately, researchers lack formal data management training, and doctoral coursework on best practices is uncommon. Fortunately, an abundance of resources exist that outline each step of data sharing in detail (e.g., [@meyer18; @shero_hart20; @lewis24]). Here we provide a framework to briefly discuss the benefits of data sharing, how to decide when and what type of data to share, and recommendations on steps to move towards data sharing as the norm in CSD. This framework is by no means meant to be exhaustive; instead, we hope this framework will foster discussion on the merits and current barriers of data sharing in CSD.

### Benefits of Data Sharing

Sharing data offers substantial benefits for both the scientific community and the researcher collecting the data. Open data enables researchers to verify results during peer review or after publication, leading to a more transparent and reliable scientific record. Publicly available data also allows researchers to generate new knowledge through secondary analyses or meta-analysis, accelerating scientific discoveries and the implementation of research in clinical practice. Access to data is critical for meta-analyses, since traditional meta-analyses that rely on studies with aggregate data or slight variations in statistical methods can be especially challenging to conduct. Specifically, open data allows for a higher-quality synthesis of data across studies via individual participant data (IPD) meta-analyses, yielding more precise and robust estimates and greater scientific impact [@yu_romero24]. Moreover, open data minimizes redundant data collection. For instance, normative data or control groups can serve as historical comparisons for studies with patient populations, saving valuable time and resources. Lastly, sharing data benefits researchers directly, as studies have shown that openly shared data are associated with higher citation rates for the original work [@piwowar_etal07; @piwowar_vision13; @drachen_etal16].

Closed data is not only inefficient and counterproductive to cumulative science, but also conflicts with research ethics. Researchers hold an ethical responsibility to maximize the use of clinical data and patients agree to participate in research studies with the understanding that their data will help answer important public health questions. However, many research questions cannot be fully addressed in a single study, often due to factors like low statistical power or unrepresentative samples. Data sharing extends the value and longevity of collected data. Alternatively, keeping data closed places an undue burden on future participants. Many research methodologies are invasive and pose additional risk to the participant (e.g., radiation from videofluroscopic swallow studies) or requires extensive travel and time to participate. If previously collected data are not made open, future participants may be unnecessarily subjected to the same procedures.

Beyond the advantages of open data for the scientific community, researcher, and participant, data sharing also fulfills an important ethical obligation to society. Researchers conducting publicly funded studies have a responsibility to return the data to the public that financed their research - a sentiment that has recently been emphasized by funding agencies like the National Institutes of Health and the National Science Foundation (Watson et al., 2023; Wilkinson et al., 2016). Even in cases where research is not publicly funded, participants arguably have the right to see their data shared and used to its fullest potential. In our experience, when participants are informed during the consent process about the potential for their data to be shared and reused, they overwhelmingly support data sharing to maximize the impact of their contribution.

Although some researchers may consider data availability statements like "available upon reasonable request" as a step toward data sharing (Table 5), recent research has shown poor compliance with less than half of studies providing requested data [@tedersoo_etal21]. Moreover, purposefully vague and unclear data availability statements may exacerbate inequities in the field. Researchers may not devote the time to properly organize their data, thereby hindering its availability when requested, or may restrict access to unnecessarily protect their data from reuse. This practice limits access, particularly for those with fewer resources or opportunities, and poses a direct barrier to a cumulative and transparent scientific literature. In cases where data is identifiable or where informed consent cannot be obtained, synthetic data offers a solution by preserving the privacy of the original data while maintaining the statistical properties necessary for computational reproducibility. Consequently, some degree of data sharing is achievable, offering the potential for data sharing to become the norm in CSD, not the exception.

### Considerations for Deciding When and What Data to Share

### Moving Forward





Then, we can walk the reader through identifying what type of data to share using the “Can I share my data” flowchart.
In the introduction, we discussed the challenges with sharing each type of data. However, here, we can discuss the benefits of sharing each type of data.
Sharing analysis data/synthetic data can help researchers conduct meta-analyses, which has implications for cumulative science.
Sharing raw and/or intermediate data can relieve the burden of each research team collecting data in their silos, therefore, speeding up our ability to investigate these populations, which can be especially helpful for these low-incidence/less well-understood populations.


Ideas: 
- planning what data you'd like to share (raw vs analyzed)
- obtaining approval from an institutional review board
- creating a data dictionary or codebook to describe the data
- providing code to reproduce analyses
- selecting repositories to store the data

*SCRAP*
Steps to ensure data is open and reproducible (Howard et al., 2024)
* Store data files in a freely downloadable or protected access repository (e.g., https://osf.io, https://nda.nih.gov, https:// www.ldbase.org, https://www.icpsr.umich.edu). 
* Include enough data to reproduce all elements of a study, including variables used for descriptive statistics and item-level data behind composite-scale scores.  
* Provide a data dictionary or codebook.  
* Provide syntax files that document all statistical analyses (and ensure the files referenced in syntax match the files included with open data).
* As possible and practical, statistical analyses should be performed in freely available software for maximal reproducibility. Otherwise, include copies of data and syntax files from proprietary software in a generic format (e.g., .csv files for data; .txt or .pdf for syntax).  
* In the repository in which data and/or materials are stored, include detailed instructions (e.g., a “Read Me” file) to help visitors navigate and use the provided files.

Decision-tree (eg diagram that Austin created)
* Guidance on when to obtain IRB approval, how to define 'de-identified' data, etc.

# Conclusions

To ensure computational reproducibility and promote a cumulative science, de-identified data should be shared whenever ethically permissible. Synthetic data should be reserved for instances where the data cannot be de-identified, and participants did not consent to sharing identifiable data.

\newpage

# Acknowledgements

We would like to thank the authors of the studies included in this manuscript for making their data publicly available.

\newpage

# References

::: {#refs}
:::

\newpage

::: {custom-style="noIndentParagraph"}
# Table and Figure Captions

Table 1: Description of types of data.

<br>

Table 2: Characteristics of included studies by ASHA domain.

<br>

Table 3: Effect size measures and interpretation by statistical test.

<br>

Table 4: Stability of synthetic datasets across ASHA domains.

<br>

Table 5: Pros and cons of different levels of data access.

<br>

Figure 1. Visualization of data distributions from synthetic and original data for Study #1 (Curtis et al., 2023). 

*Caption*: Panel A displays the overall distribution of laryngeal vestibule residue. Panel B displays the frequency of values by bolus consistency.

<br>

Figure 2. Visualization of data distributions from synthetic and original data for Study #2 (Thompson et al., 2023). 

*Caption*: Panel A displays the distribution of vowel space area and panel B displays the distribution of speech intelligibility.

<br>

Figure 3. Distribution of log-transformed *p*-values in synthetic datasets across ASHA domains.

*Caption*: Each panel displays the distribution of log-transformed *p*-values across 100 synthetic datasets for a given ASHA domain. The dashed line indicates the threshold for statistical significance from the original study. Shaded green areas indicate synthetic *p*-values that maintained the statistical inferential result of the original study. The mean difference and standard deviation of raw *p*-values compared to the *p*-value reported in the original study is shown below each panel's title.

<br>

Figure 4. Distribution of effect sizes in synthetic datasets across ASHA domains.

*Caption*: Each panel displays the distribution of effect sizes across 100 synthetic datasets for a given ASHA domain. The dashed line indicates the effect size reported in the original study and the light blue shaded area indicates the range of the effect size categorization. The mean difference and standard deviation of the effect size compared to the result reported in the original study is shown below each panel's title.
:::
