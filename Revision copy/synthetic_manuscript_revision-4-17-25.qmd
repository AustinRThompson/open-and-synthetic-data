---
title: "Using Synthetic Data in Communication Sciences and Disorders to Promote Transparency and Reproducibility"
# Line break in title requires 2 spaces followed by \n
author: |
  James C. Borders^1^, Austin Thompson^2^, & Elaine Kearney^3,4^
abstract: |
  1.	Department of Speech, Language, and Hearing Sciences, Boston University
  2.	Department of Communication Sciences and Disorders, University of Houston
  3.	School of Health and Rehabilitation Sciences, University of Queensland, Brisbane, Australia
  4.	Department of Speech Pathology, Princess Alexandra Hospital, Brisbane, Australia
# specifies that the output will be a word document
format:
  docx:
    # this holds the style template for the word document
    reference-doc: "templates-data/custom-reference.docx"
    # this holds the style template for the word document
    pandoc_args: ["-Fpandoc-crossref"]
# file with bibtex citations for the document. generated with the zotero plugin
# or using the {rbbt} R package
bibliography: "synthetic.bib"
# this is the apa style for the document
csl: apa.csl
execute:
  cache: false
---

<!-- <br> will create a blank line between text if needed.  -->

<!-- A \ at the end of a line will cause a linebreak.  -->

<!-- \newpage will create a page break to put the abstract on a new page -->

::: {custom-style="noIndentParagraph"}
<br>

**Disclosures**:\
The authors have no financial or non-financial disclosures.

<br>

**Corresponding Author**:\
James C. Borders, PhD, CCC-SLP\

<br>

**Authorship Contributions** (CRediT taxonomy - https://casrai.org/credit/)\
*Author Roles*: ^1^conceptualization, ^2^data curation, ^3^formal analysis, ^4^funding acquisition, ^5^investigation, ^6^methodology, ^7^project administration, ^8^resources, ^9^software, ^10^supervision, ^11^validation, ^12^visualization, ^13^writing -- original draft, ^14^writing -- reviewing & editing

JCB: 1, 2, 3, 6, 9, 12, 13\
AT: 1, 2, 3, 6, 9, 11, 12, 14\
EK: 1, 2, 3, 6, 9, 11, 12, 14

<br>

**Ethical Approval**: This study was deemed exempt by the Human Research Ethics Committee at the University of Queensland (#2024/HE001484).

<br>

**Keywords**: Open data; Reproducibility; Meta-science; Communication sciences and disorders

\newpage

<!-- This code chunk is not shown. It allows you to set knitr options globally. -->

```{r global options, include = FALSE, warning = FALSE, message = FALSE}
# set global options
knitr::opts_chunk$set(
  include = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE
)

# set seed for reproducibility
set.seed(2024)

# load packages
library(tidyverse) # data wrangling
library(here) # importing data
library(synthpop) # synthetic data generation
library(gamlss) # zero-inflated multilevel models
library(flextable) # create tables
library(officer) # create tables
library(insight) # create tables
library(cowplot) # figures
library(remotes)
library(printy) # printing pretty p-values [remotes::install_github("tjmahr/printy")]
library(brms)
library(rstan)

# Function to add title to plots
draw_label_theme <- function(label, theme = NULL, element = "text", ...) {
  if (is.null(theme)) {
    theme <- ggplot2::theme_get()
  }
  if (!element %in% names(theme)) {
    stop("Element must be a valid ggplot theme element name")
  }
  
  elements <- ggplot2::calc_element(element, theme)
  
  cowplot::draw_label(label, 
                      fontfamily = elements$family,
                      fontface = elements$face,
                      colour = elements$color,
                      size = elements$size,
                      ...
  )
}
```

# Abstract {style="text-align: center;"}

**Purpose**: Reproducibility is a core principle of science and access to a study’s data is essential to reproduce its findings. However, data sharing is uncommon in the field of Communication Sciences and Disorders (CSD), often due to concerns related to privacy and disclosure risks. Synthetic data offers a potential solution to this barrier by generating artificial datasets that do not represent real individuals yet retain statistical properties and relationships from the original data. This study aimed to explore the feasibility and preliminary utility of synthetic data promote transparency and reproducibility in the field of CSD.

**Method**: Open datasets were obtained from previously published research within the American Speech-Language-Hearing Association ‘Big Nine’ domains (articulation, cognition, communication, fluency, hearing, language, social communication, voice and resonance, and swallowing) across a range of study outcomes and designs. Synthetic datasets were generated with the *synthpop* R package. General utility was assessed visually and with the standardized ratio of the propensity mean squared error (*S_pMSE*). Specific utility assessed whether inferential relationships from the original data were preserved in the synthetic dataset by comparing model fit indices, coefficients, and *p*-values.

**Results**: Update.

**Conclusion**: Findings suggest that synthetic data can effectively maintain statistical properties and relationships across a wide range of non-hierarchical data commonly seen in the field of CSD. While some studies with fewer observations than recommended (i.e., n < 130) showed lower agreement and greater variability in *p*-values and effect size estimates, this was not consistently appreciated. Therefore, researchers who use synthetic data should assess its utility in preserving their results for their own data and use-case.
:::

\newpage

<!-- This is the title again using a first-level header -->

# Introduction {style="text-align: center;"}

Transparency and openness are fundamental tenets of science, with computational reproducibility playing a key role in maintaining these values. Computational reproducibility refers to the ability to recreate a study's results using the original data. Nowadays, the vast majority of scientific studies use some degree of computation in processing data, conducting descriptive or inferential statistics, and visualizing results. When these computations are reproducible, the transparency and confidence in findings are enhanced. Achieving computational reproducibility, however, requires authors to share their data. Both the National Institutes of Health and the National Science Foundation mandate data sharing and management plans to ensure that scientific data supporting a study is shared upon publication and aligns with FAIR (Findability, Accessibility, Interoperability, and Reuse) principles of digital assets [@wilkinson_etal16; @watson_etal23]. 

Providing open, publicly available data benefits scientists, funding bodies, and society at large by enabling researchers to verify results, generate new knowledge (e.g., meta-analyses, secondary analyses), develop hypotheses, and minimize redundant data collection [@chow_etal23]. In this sense, sharing data promotes a cumulative and self-correcting science. Despite the clear benefits of open data and its growing adoption in other fields like psychology and the biobehavioral sciences [@quintana20], only 26% of a sample of researchers in the field of Communication Sciences and Disorders (CSD) reported sharing their data publicly at least once [@elamin_etal23]. 

Understanding the nuances of data sharing requires a closer look at the different types of data generated throughout a research project’s life cycle. These include raw collected data, processed intermediate data, and final analysis data (Table 1). However, a common misconception is that open data refers solely to sharing raw data (e.g., audio recordings, videos, MRI data) [@pfeiffer_etal24]. In reality, sharing intermediate or analysis data can also support reproducibility while reducing privacy and confidentiality concerns associated with sharing raw data. However, these different types of data offer varying levels of utility: sharing raw data enables maximum reproducibility and secondary research opportunities, while analysis data (although easier to share) primarily supports computational reproducibility.

##### Table 1 here.

```{r table 1, echo = FALSE, include = TRUE}
# This code reads in a template, generates table 2, 
# and then saves it as word doc in the appropriate folder

cols2 <- tibble(
  ` ` = c(
    "Description",
    "Examples",
    "Acoustic Data (Thompson et al., 2023)",
    "Swallowing Data (Curtis et al., 2023)",
    "Eye Tracking Data (Baron et al., 2023)",
    "Focus Group or Interview Data (Pfeiffer et al., 2024)",
    "Survey Data (Riccardi, 2024)",
    "Assessment Tool Data (Pfeiffer & Landa, 2024)"
  ),
  `Raw Data` = c(
    "Original, unmodified data collected from studies.\n\nAlso known as primary data, microdata, individual-level data.",
    "",
    "Raw audio recordings.",
    "Video files from flexible endoscopic evaluations of swallowing.",
    "Eye movement recordings (gaze, saccades, fixations).",
    "Audio recordings and transcripts.",
    "Raw survey responses.",
    "Raw, unscored assessment protocols."
  ),
  `Intermediate Data` = c(
    "Cleaned, de-identified, and processed data, used for creating the analysis data.\n\nAlso known as cleaned data, transactional data, processed data.",
    "",
    "Extracted formant data.",
    "Not applicable.",
    "Cleaned data with merged fixations and removed artifacts.",
    "De-identified transcripts.",
    "Cleaned and coded responses.",
    "Scored protocols with calculated totals and subscales."
  ),
  `Analysis Data` = c(
    "Final dataset containing variables used in statistical analyses.\n\nAlso known as derived data, result data, aggregate data.",
    "",
    "Dataset containing average vowel space area per speaker.",
    "Dataset containing ratings of swallowing safety and efficiency.",
    "Dataset containing summary of fixation durations, reading times, and target proportions.",
    "Dataset containing coded categories and themes from qualitative analysis.",
    "Dataset containing summary scores and frequencies of survey responses.",
    "Dataset containing standard scores, confidence intervals, percentile ranks."
  )
)

set_flextable_defaults(font.family = "Times New Roman")

t2 <- flextable(cols2) |>
  set_caption("Table 1: Description of types of data.") |> 
  set_table_properties(layout = "autofit", width = 1) |>
  bg(i = 2, bg = "#D3D3D3") # Change second row background to light grey

doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t2)
fileout <- here("manuscript", "tables-figures", "table1.docx") # uncomment to write in your working directory
print(doc, target = fileout)
```

Both individual and system-level barriers hinder data sharing, including a lack of time, knowledge, support from colleagues, and perceived incentives [@pfeiffer_etal24]. Furthermore, each type of data comes with unique challenges regarding data sharing. For raw data, it is common that researchers often do not obtain consent to share data or cannot contact participants after data collection. Additionally, sharing de-identified raw or intermediate data may require additional approval from the institutional review board. Even when de-identification is possible, anonymized intermediate or analysis datasets can still carry re-identification risks, especially in small samples or vulnerable populations where indirect identifiers (e.g., gender, age, or race) may compromise participant confidentiality [@rocher_etal19]. Therefore, although sharing de-identified analysis data is the minimum requirement for ensuring computational reproducibility and promoting cumulative science, concerns about privacy must be addressed when sharing sensitive data.

## Synthetic Data as an Approach to Promote Transparency and Reproducibility

Synthetic data generation offers a promising solution to safeguarding participants’ privacy and confidentiality in publicly available datasets [@rubin93; @drechsler_haensch24]. This approach can be applied to a wide variety of data types (e.g., demographic information, outcome measures) and involves creating artificial datasets that do not represent real individuals, thereby significantly reducing the risk of disclosure. Importantly, synthetic data retains the statistical properties and relationships of the original data, enabling readers to evaluate key aspects of the study’s analysis workflow (e.g., data pre-processing, statistical modeling), reproduce study findings, explore datasets, and develop new questions or hypotheses. Synthetic data generation is widely used across medical research, industry, and government agencies, most notably by the United States Census Bureau [@jarmin_etal14a]. Although the concept of synthetic data methods was first proposed more than 30 years ago [@rubin93], recent analytic and software developments have streamlined the process, making it easier and more efficient to generate high-quality synthetic data [@nowok_etal16].

The intended use of synthetic data influences the level of rigor and scrutiny required. For example, synthetic data can serve as a pedagogical tool to teach data analysis skills or novel statistical methods [@shepherd_etal17]. In such cases, preserving general statistical properties is sufficient, even if precise relationships between variables are not fully maintained. Similarly, synthetic data accompanying publications can facilitate reproducible workflows to illustrate data pre-processing steps or statistical models without reproducing exact study results. However, higher standards are required when synthetic data is used for hypothesis testing, meta-analyses, or methodological development [@raab_etal17]. In these scenarios, synthetic datasets must accurately preserve multivariable relationships to ensure their validity and utility.

Two main approaches are used to assess the utility of synthetic datasets: general and specific [@snoke_etal18]. General utility evaluates whether the synthetic dataset maintains the overall statistical properties of the original dataset. This includes visual comparisons of univariate (e.g., bar charts, histograms) and bivariate joint distributions (e.g., scatterplots), as well as metrics to determine to what degree synthetic data is distinguishable from the original data (e.g., standardized propensity mean squared error; *S_pMSE*). Specific utility assesses whether inferential relationships from the original dataset are preserved in the synthetic dataset by comparing model fit indices and coefficients.

## Application of Synthetic Data in Communication Sciences and Disorders

Despite its potential to enhance data sharing in the field of CSD, synthetic data is not widely known or adopted in the field. Data commonly collected in CSD research poses unique challenges, including smaller sample sizes than are typically recommended for synthetic data generation and a wide range of study designs, outcomes, and analyses [@borders_etal22a; @gaeta_brydges20]. Moreover, reproducible workflows that detail important steps for data wrangle or statistical modeling are rarely provided in publications, further hindering transparency and reproducibility.

To address this gap, the present study aimed to explore the feasibility and preliminary utility of synthetic data generation in CSD. We applied synthetic data methods to open datasets from the ‘Big Nine’ American Speech-Language-Hearing Association (ASHA) domains and hypothesized that synthetic datasets would preserve both the statistical properties (general utility) and the inferential results (specific utility) of the original data. It’s important to recognize that synthetic data must be evaluated on a case-by-case basis and that the utility of the datasets included in this manuscript may not apply to one’s own dataset. To this end, the broad goal of the current investigation was to provide a proof-of-concept to the interested reader.

# Method

## Description of Original Datasets from ASHA 'Big Nine' Domains

A convenience sampling approach was used to identify publicly available datasets from previously published research articles related to the ‘Big Nine’ ASHA domains: swallowing [@curtis_etal23a], articulation [@thompson_etal23], fluency [@elsherif_etal21], voice and resonance [@novotny_etal16], hearing [@battal_etal19], communication modalities [@king_etal22], receptive and expressive language [@kearney_etal23; @robinaugh_etal24a], cognitive aspects of communication [@clough_etal23], and social aspects of communication [@chanchaochai_schwarz23]. Given the prevalence of single subject experimental designs in the field of CSD, an additional study was included to ensure adequate representation [@robinaugh_etal24a], resulting in ten studies. These studies were classified by their study design, population, and statistical analysis (Table 2).

It is important to note that not *all* research designs are represented due to the limited availability of public data in the field of CSD and the inherent challenge of including every possible study design. Instead, this approach was chosen to prioritize representation across all subfields to illustrate the application of synthetic data methods in CSD. To demonstrate the feasibility and preliminary utility of synthetic data, an analysis was chosen from each study and synthetic data was generated for those variables, as described below.

##### Table 2 here.

```{r table 2, echo = FALSE, include = TRUE}
# This code reads in a template, generates table 2, 
# and then saves it as word doc in the appropriate folder

cols2 <- tibble(
    `Domain` = c(
    "Swallowing",
    "Articulation",
    "Fluency",
    "Voice and resonance",
    "Hearing",
    "Communication modalities",
    "Receptive and expressive language",
    "Cognitive aspects of communication",
    "Social aspects of communication"
  ),
  `Study` = c(
    "Curtis et al. (2023)",
    "Thompson et al. (2023)",
    "Elsherif et al. (2021)",
    "Novotný et al. (2016)",
    "Battal et al. (2019)",
    "King et al. (2022)",
    "Kearney et al. (2023)",
    "Clough et al. (2023)",
    "Chanchaochai & Schwarz (2023)"
  ),
  `Open Materials` = c(
    "Data, code",
    "Data, code",
    "Data, code",
    "Data",
    "Data, code",
    "Data, code",
    "Data",
    "Data",
    "Data, code"
  ),
  `Sample Size` = c(
    "39",
    "40",
    "164",
    "111",
    "34",
    "160",
    "34",
    "102",
    "96"
  ),
  `Population(s)` = c(
    "Neurotypical",
    "Parkinson’s disease, amyotrophic lateral sclerosis, Huntington’s disease, cerebellar ataxia",
    "Dyslexia, stuttering, neurotypical",
    "Parkinson’s disease, Huntington’s disease, neurotypical",
    "Congenitally blind, sighted",
    "Speech-language pathologists",
    "Brain tumor",
    "Traumatic brain injury, neurotypical",
    "Autism spectrum disorder, neurotypical"
  ),
  `Analysis of Interest` = c(
    "Distribution of laryngeal vestibule residue ratings",
    "Relationship between vowel space area and intelligibility",
    "Group difference in nonword repetition",
    "Relationship between overall perceptual rating and variability of nasality",
    "Group difference in auditory localization",
    "Timepoint difference in lack of/limited internet and technology barriers",
    "Relationship between years of education and reading score",
    "Group x Condition interaction in emotion recognition accuracy",
    "Group difference in non-verbal IQ"
  ),
  `Outcome Type(s)` = c(
    "Continuous",
    "Continuous",
    "Continuous",
    "Continuous",
    "Continuous",
    "Ordinal",
    "Continuous",
    "Binary",
    "Continuous"
  ),
  `Statistics` = c(
    "Descriptive",
    "Hierarchical linear regression",
    "Independent t-test",
    "Pearson correlation",
    "Linear mixed-effects model with 3-way interaction",
    "Chi-square",
    "Spearman’s rank correlation coefficient",
    "Generalized linear mixed-effects model with 3-way interaction",
    "Analysis of Variance"
  )
)

set_flextable_defaults(font.family = "Times New Roman")

t2 <- flextable(cols2) |>
  set_caption("Table 2: Characteristics of included studies by ASHA domain.") |> 
  set_table_properties(layout = "autofit", width = 1)

doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t2)
fileout <- here("manuscript", "tables-figures", "table2.docx") # uncomment to write in your working directory
print(doc, target = fileout)
```

## Generation of Synthetic Datasets and Comparison with Original Dataset

Synthetic data generation and statistical analyses were conducted in R version 4.2.1 [@rcoreteam22]. The *synthpop* R package (version 1.8.0) [@nowok_etal16] was used to generate synthetic data via complete conditional specification [@drechsler_haensch24]. This method synthesizes one variable at a time: the first variable is generated by random sampling from the original dataset, and subsequent variables are synthesized conditionally based on previously synthesized variables. This stepwise approach captures relationships between variables incrementally rather than attempting to synthesize all relationships simultaneously. 

For example, consider a dataset containing three variables: participant ID, age, and weight. The process would begin by synthesizing participant ID through random sampling from its observed distribution. Age would then be synthesized conditionally based on the synthetic participant ID values, with synthetic values drawn from predictions informed by the original data. Finally, weight would be synthesized conditionally on both participant ID and age, with synthetic values similarly sampled from predictions.

Synthpop inherently manages missing data and maintains relationships between missingness and other variables using a tree-based algorithm, specifically classification and regression trees (CART), for data synthesis [@nowok_etal16]. Alternatively, users can select other tree-based methods, such as random forests, or parametric models like linear or logistic regression. This process resembles multiple imputation by chained equations (MICE) for handling missing data [@vanbuuren_vanbuuren12] but with a key distinction: instead of imputing only missing values, synthpop generates entirely synthetic data [@raghunathan21], significantly reducing disclosure risk.
Nowok et al. (2016) provide an in-depth overview of the synthpop package’s features. Briefly, synthesis is largely automated using the syn() function. Users can customize various options, including the modeling approach, choice of predictors, order of synthesized variables, smoothing parameters for continuous variables to enhance privacy, and rules for maintaining logical relationships.

## Evaluation of General and Specific Utility

In the present study, we aimed to explore the feasibility and preliminary utility of synthetic data to promote transparency and reproducibility in CSD. Utility was operationalized as general (does the synthetic data resemble the original data in its statistical properties and distribution?) and specific (is the inferential relationship between variables maintained?). To evaluate general utility, we visually compared univariate (e.g.., bar charts, histograms) and bivariate joint distributions (e.g., scatterplots) between the original and synthetic dataset, and evaluated the predicted probability that a record comes from the synthetic versus original data, known as the standardized propensity mean squared error (*S_pMSE*). Standardized propensity scores closer to zero indicate greater general utility (typically with a standard deviation of one), where a value of zero indicates that the original and synthetic data are identical [@snoke_etal18]. Notably, a value of zero is highly unlikely since synthetic data generation aims to achieve distributional similarity.

To assess specific utility, a statistical analysis was selected from each study and performed separately with the original and the synthetic data. Greater overlap in effect size or coefficient confidence intervals and similar p-value inferences (i.e., significant or non-significant) indicated greater specific utility. Since Curtis et al. (2023) examined median and interquartile ranges (IQR) instead of inferential statistical models, only general utility was examined. The pre-registered analysis plan and corresponding deviations are publicly available on the Open Science Framework (https://osf.io/vhgq2).


##### Table 3 here.

```{r table 3, echo = FALSE, include = TRUE}
cols3 <- tibble(
    `Statistical Test` = c(
    "Hierarchical linear regression",
    "Independent t-test",
    "Correlation (Pearson’s, Spearman’s)",
    "Chi-square",
    "Generalized linear model"
  ),
  `Effect Size Measure` = c(
    "Cohen’s f\n(Cohen, 1988)",
    "Cohen’s d (Cohen, 1988) &\nGlass' Δ (Hedges & Olkin, 1985)",
    "Correlation coefficient\n(Cohen, 1988)",
    "Cohen's 𝜔\n(Cohen, 1988)",
    "√(3/𝜋) x odds ratio\n(Haddock et al., 1998; Hasselblad & Hedges, 1995)"
  ),
  `Small` = c(
    "0.1",
    "0.2",
    "0.1",
    "0.1",
    "0.2"
  ),
  `Medium` = c(
    "0.25",
    "0.5",
    "0.3",
    "0.3",
    "0.5"
  ),
  `Large` = c(
    "0.4",
    "0.8",
    "0.5",
    "0.5",
    "0.8"
  )
)

set_flextable_defaults(font.family = "Times New Roman")

t3 <- flextable(cols3) |>
  set_caption("Table 3: Effect size measures and interpretation by statistical test.") |> 
  set_table_properties(layout = "autofit", width = 1)

doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t3)
fileout <- here("manuscript", "tables-figures", "table3.docx")
print(doc, target = fileout)
```

# Results

### Swallowing
```{r}
# Import original data ----
swallowing_data_original <-
  read.csv(here::here("Data/01_Swallowing/norms_ratings.csv")) |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables
  dplyr::select(c(study_id, bolus_consistency, laryngeal_vestibule_severity_rating)) |> 
  mutate(bolus_consistency = as.factor(bolus_consistency),
         study_id = as.factor(study_id),
         # express laryngeal_vestibule_severity_rating as a percentage
         laryngeal_vestibule_severity_rating = laryngeal_vestibule_severity_rating/100,
         dataset_type = "Original",
         dataset_number = 0)

# Create synthetic datasets ----
swallowing_data_synthetic <- syn(swallowing_data_original, 
             method = "ctree", 
             m = 100,
             seed = 2024)

swallowing_data_synthetic_first <- swallowing_data_synthetic$syn[[1]]

# General Utility ----

# Combine the data frames
swallowing_combined_data <- swallowing_data_synthetic_first |> 
  mutate(dataset_type = "Synthetic") |> 
  rbind(swallowing_data_original)

# Calculate utility with s_pMSE
utility_df <- utility.tables(
  swallowing_data_synthetic_first,
  swallowing_data_original,
  vars = "laryngeal_vestibule_severity_rating",
  tables = "oneway",
  tab.stats = "S_pMSE",
  print.tabs = FALSE
)
    
# Extract s_pMSE value
swallowing_s_pMSE_value <- utility_df$tabs |>
  as.data.frame() |>
  rownames_to_column(var = "rowname") |>
  dplyr::pull(S_pMSE)

# Create a new categorical variable
swallowing_combined_data <- swallowing_combined_data |> 
  mutate(severity_category = ifelse(laryngeal_vestibule_severity_rating == 0, "0%", "> 0"))

# Bar plot for frequency of 0's
p1 <- swallowing_combined_data |> 
  filter(severity_category == "0%") |> 
  ggplot(aes(x = severity_category, fill = dataset_type)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  labs(x = "",
       y = "Frequency") +
  cowplot::theme_cowplot() +
  theme(legend.position = "none",
        # axis.title.x = element_blank(),
        axis.title.y = element_text(margin = margin(r = 8))) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 470))

# Density plot for values > 0
p2 <- swallowing_combined_data |> 
  filter(severity_category == "> 0") |> 
  ggplot(aes(x = laryngeal_vestibule_severity_rating, color = dataset_type, fill = dataset_type)) +
  geom_density(alpha = 0.80) +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_color_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  # scale_x_continuous(limits = c(1, 50)) +
  labs(x = "",
       y = "Density",
       color = "Dataset Type",
       fill = "Dataset Type") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(labels = scales::percent, limits = c(0.01, 0.45),
                     breaks = c(0.01, 0.1, 0.2, 0.3, 0.4)) +
  theme(legend.position = "top",
        legend.title = element_blank(),
        legend.direction = "horizontal",
        legend.justification = "center",
        axis.title.y = element_text(margin = margin(r = 8)))

# Extract the legend from p2
legend_components <-  cowplot::get_plot_component(p2, 'guide-box-top', return_all = TRUE)
legend <- cowplot::ggdraw(legend_components)

# Add title
title <- ggdraw() +
  draw_label_theme("A: Swallowing (Curtis et al., 2023)", 
                   theme = theme_cowplot(), element = "plot.title",
                   x = 0.075, hjust = 0, vjust = 1)

# Combine plots without legends
combined_plots <- cowplot::plot_grid(p1, p2 + theme(legend.position = "none"), rel_widths = c(0.40, 1))

# Arrange the plots and legend
final_plot <- cowplot::plot_grid(
  title,
  combined_plots,
  ncol = 1,
  rel_heights = c(
    0.2, 
    3)
)

# Add a shared x-axis label
swallowing_general_utility_plot <- cowplot::ggdraw(final_plot) +
  cowplot::draw_label("Laryngeal Vestibule Residue Rating", x = 0.5, y = 0.02, 
                      vjust = 0, hjust = 0.5, size = 15)
```

Curtis et al. (2023) provided normative reference values for swallowing outcomes during flexible endoscopic evaluations of swallowing in a sample of 39 community-dwelling adults without dysphagia. In this observational cohort study, each participant completed 15 swallowing trials that varied by bolus size, consistency, contrast agent, and swallowing instructions. Several swallowing outcomes were measured, including the amount of laryngeal vestibule residue, which was rated using the Visual Analysis of Swallowing Efficiency and Safety. No inferential statistics were conducted; instead, the distribution of laryngeal vestibule residue ratings was summarized using median and interquartile ranges.

In the synthetic dataset, the primary outcome (laryngeal vestibule residue ratings) closely mirrored the original data (Figure 1A). The frequency of zero values was nearly identical and the distribution of values greater than zero was also similar, with only minor deviations at higher residue ratings. The *S_pMSE* value was `r round(swallowing_s_pMSE_value, digits = 2)`, indicating strong overall similarity and general utility between the synthetic and original data.

##### Figure 1 here.

### Articulation

```{r echo = FALSE}
# Import original data ----
articulation_data_original <- rio::import(
  file = here::here("Data", "02_Articulation", "data_Acoustic Measures.csv")
) |>
  
  # Remove the reliability trials that have "_rel" in the SpeakerID variable
  dplyr::filter(!str_detect(SpeakerID, "_rel")) |>
  
  # Selecting just the variables we need
  dplyr::select(
    VSA_b, # VSA in Bark
    Int = Int_OT # intelligibility (orthographic transcriptions)
  )

# Create synthetic datasets ----
articulation_data_synthetic <- syn(articulation_data_original, 
             method = "ctree", 
             m = 100,
             seed = 2024)

articulation_data_synthetic_first <- articulation_data_synthetic$syn[[1]]

# General Utility ----

# Calculate utility with s_pMSE
articulation_utility_df <- utility.tables(
  articulation_data_synthetic_first,
  articulation_data_original,
  tables = "oneway",
  tab.stats = "S_pMSE",
  print.tabs = FALSE
)
    
# Extract s_pMSE value
articulation_s_pMSE_value <- articulation_utility_df$tabs |>
  as.data.frame() |>
  rownames_to_column(var = "rowname")

# Extract the data frames
articulation_data_original <- articulation_data_original |> 
  mutate(dataset_type = "Original")

# Combine the data frames
articulation_combined_data <- articulation_data_synthetic_first |> 
  mutate(dataset_type = "Synthetic") |> 
  rbind(articulation_data_original)

# Scatterplot
articulation_p1 <- 
  articulation_combined_data |> 
  ggplot(aes(x = VSA_b, y = Int, color = dataset_type, fill = dataset_type)) +
  geom_point(alpha = 0.80, size = 2.5) +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_color_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_x_continuous(breaks = c(1, 3, 6, 9, 12)) +
  scale_y_continuous(limits = c(0, 100), breaks = c(0, 25, 50, 75, 100)) +
  labs(x = "Vowel Space Area",
       y = "Intelligibility",
       color = "Dataset Type",
       fill = "Dataset Type") +
  cowplot::theme_cowplot() +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(r = 8)),
        axis.title.y = element_text(margin = margin(t = 8)))

# Add title
articulation_title <- ggdraw() +
  draw_label_theme("B: Articulation (Thompson et al., 2023)", 
                   theme = theme_cowplot(), element = "plot.title",
                   x = 0.075, hjust = 0, vjust = 1)

# Combine plot with title
articulation_final_plot <- cowplot::plot_grid(
  articulation_title,
  articulation_p1,
  ncol = 1,
  rel_heights = c(0.2, 3) # Adjust the relative widths as needed
)

# Specific Utility ----

articulation_alpha = .05

## Perform regression between Intelligibility and VSA (bark)
articulation_analyze_data <- function(data) {
  model <- lm(Int ~ VSA_b, data = data) # Creating the model
  model_fit <- summary(model, save=TRUE) # Saving the model
  
  list(
    p_value = model_fit$coefficients[8], # Extracting p-values
    effect_size = effectsize::cohens_f(model, partial = F)$Cohens_f, # Extracting effect size
    conf_int_lower = confint(model)[2, ][1], # Extracting lower CI bound
    conf_int_upper = confint(model)[2, ][2] # Extracting upper CI bound
    )
}

# Reproduce original results ----
articulation_results_original <- (articulation_analyze_data(articulation_data_original)) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < articulation_alpha ~ "sig",
      p_value >= articulation_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.40 ~ "Large",
                               abs(effect_size) >= 0.25 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )


## Perform analysis on each synthetic dataset
articulation_results_synthetic <- articulation_data_synthetic$syn |>
  purrr::map_df(~ articulation_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < articulation_alpha ~ "sig",
                    p_value >= articulation_alpha ~ "non-sig"),
    categorization = case_when(
      abs(effect_size) >= 0.40 ~ "Large",
      abs(effect_size) >= 0.25 ~ "Medium",
      abs(effect_size) >= 0.1 ~ "Small",
      TRUE ~ "Negligible"
    ),
     # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - articulation_results_original$p_value),
    diff_effectSize = abs(effect_size - articulation_results_original$effect_size)
  ) |> 
  add_column(domain = "Articulation")

# Compile results ----
articulation_results_summary <- data_frame(
  Domain = "Articulation",
  Study = "Thompson et al. (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(articulation_data_original),
  p_value = ifelse(
    articulation_results_original$p_value < .001,"<.001",
    sprintf("%0.3f", articulation_results_original$p_value)),
  effect_size_measure = "Cohen’s f",
  effect_size = round(articulation_results_original$effect_size, digits = 2), 
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = articulation_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(articulation_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == articulation_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(articulation_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(articulation_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = articulation_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(articulation_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == articulation_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(articulation_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(articulation_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
articulation <- list(
  data_original = articulation_data_original,
  data_synthetic = articulation_data_synthetic,
  results_original = articulation_results_original,
  results_synthetic = articulation_results_synthetic,
  results_summary = articulation_results_summary,
  alpha = articulation_alpha
)

# Saving the data and results for easy importing later.
base::saveRDS(object = articulation,
              file = here::here("Data","02_Articulation","analysisData.RDS"))
```

Thompson et al. (2023) examined the relationship between vowel space area and speech intelligibility among 40 speakers with dysarthria of varying etiologies (e.g., Parkinson's disease, amyotrophic lateral sclerosis, Huntington's disease, and ataxia). A linear regression model revealed a statistically significant relationship between vowel space area and intelligibility (*p* \< .001) with a Cohen's *f* of 0.59, corresponding to a conventionally "large" effect size (Table 3).

Compared to original data, the synthetic data demonstrated a similar distribution (Figure 1). General utility was high for both variables of vowel space area (*S_pMSE* = `r round(articulation_s_pMSE_value$S_pMSE[1], digits = 2)`) and intelligibility (*S_pMSE* = `r round(articulation_s_pMSE_value$S_pMSE[2], digits = 2)`). Specific utility was high as the statistical model with the synthetic data maintained the direction of statistical significance (*p* = `r fmt_p_value(articulation_results_synthetic[1,]$p_value)`) and effect size magnitude (*f* = `r round(articulation_results_synthetic[1,]$effect_size, digits = 2)`).

### Fluency
```{r echo = FALSE}
# Import original data ----
fluency_data_original <- rio::import(file = here::here("Data", 
                                                       "03_Fluency", 
                                                       "data_Dyslexia Stutter Master.csv")) |>
  dplyr::filter(Rejection == 0) |>
  dplyr::filter(Group != "AWD") |>
  # Selecting just the variables that we need
  dplyr::select(Subject, Group, Nonwordrepetition)

# Establish p_value for study
fluency_alpha <- .05

# Create synthetic datasets ----
fluency_data_synthetic <- syn(fluency_data_original, 
             method = "ctree", 
             m = 100,
             seed = 2024)

fluency_data_synthetic_first <- fluency_data_synthetic$syn[[1]]

# General Utility ----

# Calculate utility with s_pMSE
fluency_utility_df <- utility.tables(
  fluency_data_synthetic_first,
  fluency_data_original,
  tables = "oneway",
  tab.stats = "S_pMSE",
  print.tabs = FALSE
)
    
# Extract s_pMSE value
fluency_s_pMSE_value <- fluency_utility_df$tabs |>
  as.data.frame() |>
  rownames_to_column(var = "rowname")

# Extract the data frames
fluency_data_original <- fluency_data_original |> 
  mutate(dataset_type = "Original")

# Combine the data frames
fluency_combined_data <- fluency_data_synthetic_first |> 
  mutate(dataset_type = "Synthetic") |> 
  rbind(fluency_data_original)

# Data viz
fluency_p1 <- 
  fluency_combined_data |> 
  mutate(Group = case_when(Group == "AWS" ~ "Adults Who Stutter",
                           Group == "NT" ~ "Neurotypical Adults")) |> 
  ggplot(aes(x = Nonwordrepetition, 
             color = dataset_type, fill = dataset_type)) +
  geom_density(alpha = 0.80) +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_color_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_x_continuous(breaks = c(0, 5, 10, 15, 20),
                     limits = c(0, 22)) +
  labs(x = "Non-word Repetition",
       y = "Density",
       color = "Dataset Type",
       fill = "Dataset Type") +
  cowplot::theme_cowplot() +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(r = 8)),
        axis.title.y = element_text(margin = margin(t = 8))) +
  facet_wrap(~Group, nrow = 2)

# Add title
fluency_title <- ggdraw() +
  draw_label_theme("C: Fluency (Elsherifa et al., 2021)", 
                   theme = theme_cowplot(), element = "plot.title",
                   x = 0.075, hjust = 0, vjust = 1)

# Combine plot with title
fluency_final_plot <- cowplot::plot_grid(
  fluency_title,
  fluency_p1,
  ncol = 1,
  rel_heights = c(0.2, 3) # Adjust the relative widths as needed
)

# Specific Utility ----
# Examine the difference between neurotypical and stuttering in non-word repetition
fluency_analyze_data <- function(data) {
  t_test <- stats::t.test(
    Nonwordrepetition ~ Group,
    data = data,
    paired = FALSE,
    var.equal = FALSE
  )

  list(
    p_value = t_test$p.value,# Extracting p-values
    effect_size = effectsize::glass_delta(Nonwordrepetition ~ Group,
                                 data = data,
                                 correction = FALSE,
                                 ci = 0.95)$Glass_delta, # Extracting effect size
    conf_int_lower = t_test$conf.int[1],# Extracting lower CI bound
    conf_int_upper = t_test$conf.int[2] # Extracting upper CI bound
  )
}

# Reproduce original results ----
fluency_results_original <- (fluency_analyze_data(fluency_data_original)) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(p_value < fluency_alpha ~ "sig",
                    p_value >= fluency_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Perform analysis on synthetic dataset
fluency_results_synthetic <- fluency_data_synthetic$syn |>
  purrr::map_df(~ fluency_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < fluency_alpha ~ "sig",
                    p_value >= fluency_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - fluency_results_original$p_value),
    diff_effectSize = abs(effect_size - fluency_results_original$effect_size)
  ) |> 
  add_column(domain = "Fluency")

# Compile results ----
fluency_results_summary <- data_frame(
  Domain = "Fluency",
  Study = "Elsherif et al. (2021)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(fluency_data_original),
  p_value = ifelse(
    fluency_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", fluency_results_original$p_value)),
  effect_size_measure = "Glass' Δ",
  effect_size = round(fluency_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = fluency_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(fluency_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == fluency_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(fluency_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(fluency_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = fluency_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(fluency_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == fluency_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(fluency_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(fluency_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
fluency <- list(
  data_original = fluency_data_original,
  data_synthetic = fluency_data_synthetic,
  results_original = fluency_results_original,
  results_synthetic = fluency_results_synthetic,
  results_summary = fluency_results_summary,
  alpha = fluency_alpha
)

## Cleaning up the environment
# rm(
#   fluency_data_original,
#   fluency_data_synthetic,
#   fluency_results_original,
#   fluency_results_synthetic,
#   fluency_results_summary,
#   fluency_alpha
# )

# Saving the data and results for easy importing later.
base::saveRDS(object = fluency,
              file = here::here("Data","03_Fluency","analysisData.RDS"))
```

Elsherif et al. (2021) compared non-word repetitions between 80 neurotypical adults and 34 adults who stutter. An independent samples t-test demonstrated a statistically significant difference in non-word repetitions between these groups (*p* \< .001) with a large effect size (Δ = 1.26).

Compared to original data, the synthetic data similar distributions (Figure 1C). General utility was high for the outcome of non-word repetitions (*S_pMSE* = `r round(fluency_s_pMSE_value$S_pMSE[3], digits = 2)`) and the statistical model with the synthetic data maintained the direction of statistical significance (*p* = `r fmt_p_value(fluency_results_synthetic[1,]$p_value)`) and effect size magnitude (*f* = `r abs(round(fluency_results_synthetic[1,]$effect_size, digits = 2))`).

### Voice and Resonance
```{r echo = FALSE}
# Load data ----
voice_data_original <-
  readxl::read_excel(here::here("Data/04_Voice_Resonance/RawData.xlsx"), 
                     range = "B3:Z114") |>
  # clean variable names
  janitor::clean_names() |>
  # select only relevant variables
  dplyr::select(c(efn_sd_d_b, median_of_raters)) |> 
  mutate(median_of_raters_cat = case_when(median_of_raters == 0 ~ "Normal",
                                      median_of_raters == 1 ~ "Mild",
                                      median_of_raters == 2 ~ "Moderate",
                                      median_of_raters == 3 ~ "Severe"),
         median_of_raters_cat = factor(median_of_raters_cat,
                                   levels = c("Normal", "Mild", "Moderate"))) |> 
  mutate(dataset_type = "Original") |> 
  as.data.frame()

# Establish p_value for study
voice_alpha <- .05

voice_data_synthetic <- syn(voice_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# General Utility ----

voice_data_synthetic_first <- voice_data_synthetic$syn[[1]] |> 
  as.data.frame()

# Combine the data frames
voice_combined_data <- voice_data_synthetic_first |> 
  mutate(dataset_type = "Synthetic") |> 
  rbind(voice_data_original)

# Calculate utility with s_pMSE
voice_utility_df <- utility.tables(
  voice_data_synthetic_first,
  voice_data_original,
  tables = "oneway",
  tab.stats = "S_pMSE",
  print.tabs = FALSE
)
    
# Extract s_pMSE value
voice_s_pMSE_value <- voice_utility_df$tabs |>
  as.data.frame() |>
  rownames_to_column(var = "rowname")

# Scatterplot
voice_p1 <- voice_combined_data |> 
  ggplot(aes(x = efn_sd_d_b, color = dataset_type, fill = dataset_type)) +
  geom_density(alpha = 0.5) +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_color_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_x_continuous(limits = c(-1, 17)) +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Variability of Nasality",
       y = "Density",
       color = "Dataset Type",
       fill = "Dataset Type") +
  cowplot::theme_cowplot() +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(r = 8)),
        axis.title.y = element_text(margin = margin(t = 8))) +
  facet_wrap(~median_of_raters_cat, nrow = 3, scales = "free_x")

# Add title
voice_title <- ggdraw() +
  draw_label_theme("D: Voice (Novotny et al., 2016)", 
                   theme = theme_cowplot(), element = "plot.title",
                   x = 0.075, hjust = 0, vjust = 1)

# Combine plot with title
voice_final_plot <- cowplot::plot_grid(
  voice_title,
  voice_p1,
  ncol = 1,
  rel_heights = c(0.2, 3) # Adjust the relative widths as needed
)

# Specific Utility ----
# Examine correlation between overall perceptual rating and acoustic EFn SD parameter
voice_analyze_data <- function(data) {
  # Creating the model
  model <- cor.test(data$median_of_raters,
                    data$efn_sd_d_b,
                    method = "pearson")
  
  list(
    p_value = model$p.value, # Extracting p-values
    effect_size = model$estimate, # Extracting effect size
    conf_int_lower = model[["conf.int"]][1], # Extracting lower CI bound
    conf_int_upper = model[["conf.int"]][2] # Extracting upper CI bound
  )
}

# Reproduce original results ----
voice_results_original <- voice_analyze_data(voice_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < voice_alpha ~ "sig",
      p_value >= voice_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Create synthetic data ----
# Perform analysis on each synthetic dataset
voice_results_synthetic <- voice_data_synthetic$syn |>
  purrr::map_df(~ voice_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < voice_alpha ~ "sig",
      p_value >= voice_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - voice_results_original$p_value),
    diff_effectSize = abs(effect_size - voice_results_original$effect_size)
  ) |> 
  add_column(domain = "Voice and resonance")

# Compile results ----
voice_results_summary <- data_frame(
  Domain = "Voice",
  Study = "Novotný et al. (2016)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(voice_data_original),
  p_value = ifelse(
    voice_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", voice_results_original$p_value)),
  effect_size_measure = "Correlation coefficient",
  effect_size = round(voice_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = voice_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(voice_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == voice_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(voice_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(voice_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = voice_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(voice_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == voice_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(voice_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(voice_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
voice <- list(
  data_original = voice_data_original,
  data_synthetic = voice_data_synthetic,
  results_original = voice_results_original,
  results_synthetic = voice_results_synthetic,
  results_summary = voice_results_summary,
  alpha = voice_alpha
)

## Cleaning up the environment
# rm(
#   voice_data_original,
#   voice_data_synthetic,
#   voice_results_original,
#   voice_results_synthetic,
#   voice_results_summary,
#   voice_alpha
# )

# Saving the data and results for easy importing later.
base::saveRDS(object = voice,
              file = here::here("Data","04_Voice_Resonance","analysisData.RDS"))
```

Novotny et al. (2016) examined the relationship between acoustic measures of nasality variability and overall perceptual ratings in a heterogenous cohort of individuals with Parkinson's disease, Huntington's disease, and neurotypical adults. Results indicated a statistically significant relationship (*r* = 0.51, *p* \< .001) between perceptual ratings and acoustic nasality variability. Compared to original data, the synthetic data demonstrated similar values for nasality variability within perceptual ratings of 'Normal' and 'Mild', though values for 'Moderate' perceptual ratings showed larger variance likely due to its low occurrence in the dataset (Figure 1D). General utility was high for both nasality variability (*S_pMSE* = `r round(voice_s_pMSE_value$S_pMSE[1], digits = 2)`) and perceptual rating (*S_pMSE* = `r round(voice_s_pMSE_value$S_pMSE[2], digits = 2)`). The statistical model with the synthetic data maintained the direction of statistical significance (*p* `r fmt_p_value(voice_results_synthetic[1,]$p_value)`) and effect size magnitude (*r* = `r round(voice_results_synthetic[1,]$effect_size, digits = 2)`), indicating adequate specific utility.

### Hearing
```{r echo = FALSE}
# Load packages
library(nlme) # linear mixed models

# Import original data ----
plane.thre.out <- read.table(here::here("Data/05_Hearing/All_Thresholds_outliersmarked-500B.txt"), 
                             sep=",", header=T)

## data wrangling from author's code
# Convert variables to factors
names(plane.thre.out)[4]<-c('head')
plane.thre.out$arm <- factor(plane.thre.out$arm)
plane.thre.out$subj <- factor(plane.thre.out$subj)
plane.thre.out$head <- factor(plane.thre.out$head)
plane.thre.out$planes <- factor(plane.thre.out$planes)
plane.thre.out$group <- ifelse(plane.thre.out$group=='EB', c('CB'), c('SC'))
plane.thre.out$group <-factor(plane.thre.out$group)

#omit all +-3SD outliers 
plane.thre.out<- subset(plane.thre.out, outlier==0)
plane.thre.out[which(plane.thre.out$thre >12),]

#divide back
horizontal.thre.out <- subset(plane.thre.out,planes=='hor')
vertical.thre.out <- subset(plane.thre.out,planes=='ver')

rownames(horizontal.thre.out) <-c(1:nrow(horizontal.thre.out)) #correct the row numbers
horizontal.thre.out[which(horizontal.thre.out$thre >10),] # we know from glmm there's one outlier
horizontal.thre.out <-horizontal.thre.out[-c(188),] 

rownames(vertical.thre.out) <-c(1:nrow(vertical.thre.out))
vertical.thre.out[which(vertical.thre.out$thre >12),] # we know from glmm there's one outlier
vertical.thre.out <-vertical.thre.out[-c(144),] 

# in the horizontal back condition left arm is right area // right arm is left area of the subject
hor.thre.front <- subset(horizontal.thre.out, head ==1)
hor.thre.back <- subset(horizontal.thre.out, head ==2)

hor.thre.back$arm <-as.character(hor.thre.back$arm)
hor.thre.back <- within(hor.thre.back, arm[arm=="left"] <-("Alien"))#just to assign into a temp
hor.thre.back <- within(hor.thre.back, arm[arm=='right'] <- 'left')
hor.thre.back <- within(hor.thre.back, arm[arm=='Alien'] <- 'right')

horizontal.thre.out <-rbind(hor.thre.front,hor.thre.back)

# combine them back
plane.thre.out <- rbind(horizontal.thre.out,vertical.thre.out)
plane.thre.out$arm <- factor(plane.thre.out$arm)
plane.thre.out$subj <- factor(plane.thre.out$subj)
plane.thre.out$head <- factor(plane.thre.out$head)
plane.thre.out$planes <- factor(plane.thre.out$planes)
plane.thre.out$group <-factor(plane.thre.out$group)

#divide back
horizontal.thre.out <- subset(plane.thre.out,planes=='hor')
vertical.thre.out <- subset(plane.thre.out,planes=='ver')

# re-label dataset as 'original_data'
hearing_data_original <- plane.thre.out |> 
  dplyr::select(c(thre, planes, group, head, subj)) |> 
  add_column(dataset_type = "Original")

# Removing unneeded items
rm(plane.thre.out, hor.thre.back, hor.thre.front, horizontal.thre.out, vertical.thre.out)

# Establish p_value for study
hearing_alpha <- .05

# Analysis Function ----
# LMM on planes: Auditory localization performance by plane, group and position (head)
hearing_analyze_data <- function(data) {
  # Creating the model
  model <-
    nlme::lme(
      thre ~ 1 + planes * group * head,
      random = ~ 1 | subj,
      weights = varIdent(form = ~ 1 | planes),
      data = data
    )
  # Saving the model
  model_fit <- summary(model, save=TRUE)
  
  model_results <- anova(model_fit) |> 
    as.data.frame()
  
  random_effects <- unlist(ranef(model))
  
  list(
    p_value = model_results$`p-value`[3], # Extracting p-values
    # Extracting effect size - standardized effect size (Haddock et al., 1998; Hasselblad & Hedges, 1995)
    effect_size = (sqrt(3/pi))*exp(model_fit$coefficients$fixed[3]),
    conf_int_lower = exp(intervals(model)[["fixed"]][3,][1]), # Extracting lower CI bound
    conf_int_upper = exp(intervals(model)[["fixed"]][3,][3]), # Extracting upper CI bound
    random_mean = mean(random_effects), # Extracting random effects mean
    random_conf_int_lower = mean(random_effects) - 1.96 * (sd(random_effects) / length(random_effects)), # Extracting random effects 95% CI lower bound
  random_conf_int_upper = mean(random_effects) + 1.96 * (sd(random_effects) / length(random_effects))
    # Extracting random effects 95% CI upper bound
  )
}


# Reproduce original results ----
hearing_results_original <- hearing_analyze_data(hearing_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < hearing_alpha ~ "sig",
      p_value >= hearing_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )



# Create synthetic data ----
hearing_data_synthetic <- syn(hearing_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
hearing_results_synthetic <- hearing_data_synthetic$syn |>
  purrr::map_df(~ hearing_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < hearing_alpha ~ "sig",
                    p_value >= hearing_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - hearing_results_original$p_value),
    diff_effectSize = abs(effect_size - hearing_results_original$effect_size)
  ) |> 
  add_column(domain = "Hearing")

# Compile results ----
hearing_results_summary <- data_frame(
  Domain = "Hearing",
  Study = "Battal et al. (2019)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(hearing_data_original),
  p_value = ifelse(
    hearing_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", hearing_results_original$p_value)),
  effect_size_measure = "√(3/𝜋) x odds ratio",
  effect_size = round(hearing_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = hearing_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(hearing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == hearing_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(hearing_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(hearing_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = hearing_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(hearing_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == hearing_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(hearing_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(hearing_results_synthetic$diff_effectSize),
)

# General Utility ----
hearing_data_synthetic_first <- hearing_data_synthetic$syn[[1]] |> 
  as.data.frame()

# Combine the data frames
hearing_combined_data <- hearing_data_synthetic_first |> 
  mutate(dataset_type = "Synthetic") |> 
  rbind(hearing_data_original)

# Calculate utility with s_pMSE
hearing_utility_df <- utility.tables(
  hearing_data_synthetic_first,
  hearing_data_original,
  tables = "oneway",
  tab.stats = "S_pMSE",
  print.tabs = FALSE
)
    
# Extract s_pMSE value
hearing_s_pMSE_value <- hearing_utility_df$tabs |>
  as.data.frame() |>
  rownames_to_column(var = "rowname")

# Scatterplot
hearing_p1 <- 
  hearing_combined_data |> 
  ggplot(aes(x = thre, y = subj, color = dataset_type, fill = dataset_type)) +
  geom_point(alpha = 0.80, size = 2.5) +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_color_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  labs(x = "Auditory Localization",
       y = "Subject ID",
       color = "Dataset Type",
       fill = "Dataset Type") +
  cowplot::theme_cowplot() +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(r = 8)),
        axis.title.y = element_text(margin = margin(t = 8)))

hearing_p2 <- 
  hearing_combined_data |> 
  mutate(group = case_when(group == "CB" ~ "Congentially Blind",
                           group == "SC" ~ "Sighted Controls")) |> 
  ggplot(aes(x = thre, color = dataset_type, fill = dataset_type)) +
  geom_density(alpha = 0.8) +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_color_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  labs(x = "Auditory Localization",
       y = "Density",
       color = "Dataset Type",
       fill = "Dataset Type") +
  cowplot::theme_cowplot() +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(r = 8)),
        axis.title.y = element_text(margin = margin(t = 8))) +
  facet_wrap(~group) +
  scale_y_continuous(expand = c(0, 0))

# # Combine plots without legends
# hearing_combined_plots <- cowplot::plot_grid(hearing_p1, 
#                                              hearing_p2, rel_widths = c(1, 0.7))

# Add title
hearing_title <- ggdraw() +
  draw_label_theme("E: Hearing (Battal et al., 2019)", 
                   theme = theme_cowplot(), element = "plot.title",
                   x = 0.075, hjust = 0, vjust = 1)

# Combine plot with title
hearing_final_plot <- cowplot::plot_grid(
  hearing_title,
  hearing_p1,
  ncol = 1,
  rel_heights = c(0.2, 3) # Adjust the relative widths as needed
)

hearing_final_plot2 <- cowplot::plot_grid(
  hearing_title,
  hearing_p2,
  ncol = 1,
  rel_heights = c(0.2, 3) # Adjust the relative widths as needed
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
hearing <- list(
  data_original = hearing_data_original,
  data_synthetic = hearing_data_synthetic,
  results_original = hearing_results_original,
  results_synthetic = hearing_results_synthetic,
  results_summary = hearing_results_summary,
  alpha = hearing_alpha
)

## Cleaning up the environment
# rm(
#   hearing_data_original,
#   hearing_data_synthetic,
#   hearing_results_original,
#   hearing_results_synthetic,
#   hearing_results_summary,
#   hearing_alpha
# )

# Saving the data and results for easy importing later.
base::saveRDS(object = hearing,
              file = here::here("Data","05_Hearing","analysisData.RDS"))
```

Battal et al. (2019) compared auditory localization abilities between 17 congenitally blind and 17 sighted individuals. A linear mixed effects model indicated that congenitally blind individuals had enhanced spatial hearing abilities (*OR* = 1.56, *p* = .016). Compared to original data, the synthetic data showed similar distributions for auditory localization in both sighted and congentially blind individuals, as well as similar auditory localization at the subject-level (Figure 1E). General utility was high for subject (*S_pMSE* = `r round(hearing_s_pMSE_value$S_pMSE[3], digits = 2)`) and group (*S_pMSE* = `r round(hearing_s_pMSE_value$S_pMSE[2], digits = 2)`) variables. Specific utility was high as the statistical model with the synthetic data maintained the direction of statistical significance (*p* `r fmt_p_value(hearing_results_synthetic[1,]$p_value)`) and effect size magnitude (*r* = `r round(hearing_results_synthetic[1,]$effect_size, digits = 2)`). The random effect estimates were stable between the original (mean = `r round(hearing_results_original$random_mean, digits = 3)`, 95% CI: `r round(hearing_results_original$random_conf_int_lower, digits = 3)`, `r round(hearing_results_original$random_conf_int_upper, digits = 3)`) and synthetic (mean = `r round(hearing_results_synthetic$random_mean[1], digits = 3)`, 95% CI: `r round(hearing_results_synthetic$random_conf_int_lower[1], digits = 3)`, `r round(hearing_results_synthetic$random_conf_int_upper[1], digits = 3)`) datasets.

### Communication Modalities
```{r echo = FALSE}
# Import original data ----
## Function to load in AAC data from the author's code
qualtrics_csv = function(file){
  data = read_csv(file, col_names = FALSE, skip = 3)
  names = read_lines(file, n_max = 1) |> 
    str_split(pattern = ",") |> 
    unlist()
  names(data) = names
  labels = read_csv(file, col_names = FALSE, skip = 1, n_max = 1)
  labels = t(labels)
  labels = labels[,1]
  labels = setNames(as.list(labels), names)
  data = labelled::set_variable_labels(data, .labels = labels, .strict = FALSE)
  data = janitor::clean_names(data)
  data
}

# Load original data
aac_data_original <- qualtrics_csv(file = here::here("Data", "06_AAC", "data_Combined CSV data.csv")) |> 
  janitor::remove_empty("cols") |> 
  janitor::remove_empty("rows") |> 
  filter(progress > 98) |> 
  dplyr::select(start_date, participant_id, q2:x2f) |> 
  dplyr::select(q87_1_1, q87_1_2, q87_1_3, q87_1_5, q87_1_6, q87_1_7,
         q87_2_1, q87_2_2, q87_2_3, q87_2_5, q87_2_6, q87_2_7,
         q87_3_1, q87_3_2, q87_3_3, q87_3_5, q87_3_6, q87_3_7,
         q89_1_1, q89_1_2, q89_1_3, q89_1_5, q89_1_6, q89_1_7,
         q89_2_1, q89_2_2, q89_2_3, q89_2_5, q89_2_6, q89_2_7,
         q89_3_1, q89_3_2, q89_3_3, q89_3_5, q89_3_6, q89_3_7) |> 
  tidyr::pivot_longer(everything()) |>
  tidyr::separate(name,
           into = c("var", "time", "question"),
           sep = "_") |>
  dplyr::mutate(time = factor(
    time,
    labels = c(
      "Prior to\nMarch 2020",
      "March to\nJuly 2020",
      "August 2020 to\nSpring 2021"
    )
  ),
  var = factor(var, labels = c("Assessment", "Intervention"))) |>
  tidyr::drop_na(value) |> 
  dplyr::select(-question) |> 
  filter(value == "Lack of/limited internet") %>%
  as.data.frame()

# Establish p_value for study
aac_alpha <- .05

# Analysis Function ----
# Examine the chi-square difference in lack of/limited Internet and technology barriers by timepoint
aac_analyze_data <- function(data) {
  chi_squared <- data |>
    dplyr::group_by(var, value) |>
    dplyr::summarize(prop = list(chisq.test(table(time))),
                     .groups = "drop_last") |>
    dplyr::mutate(
      chi = map_dbl(prop, ~ .x$statistic),
      df = map_dbl(prop, ~ .x$parameter),
      p = map_dbl(prop, ~ .x$p.value)
    ) |>
    dplyr::filter(var == "Intervention") |>
    dplyr::filter(value == "Lack of/limited internet")
  
  list(
    p_value = chi_squared$p,
    # Extracting p-values
    effect_size = chi_squared$chi,
    # There are no confidence intervals for chi-squared tests
    conf_int_lower = NA,
    conf_int_upper = NA 
  )
}

# Reproduce original results ----
aac_results_original <- aac_analyze_data(aac_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < aac_alpha ~ "sig",
      p_value >= aac_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Create synthetic data ----
aac_data_synthetic <- syn(aac_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
aac_results_synthetic <- aac_data_synthetic$syn |>
  purrr::map_df(~ aac_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(p_value < aac_alpha ~ "sig",
                    p_value >= aac_alpha ~ "non-sig"),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - aac_results_original$p_value),
    diff_effectSize = abs(effect_size - aac_results_original$effect_size)
  ) |> 
  add_column(domain = "Communication modalities")

# Compile results ----
aac_results_summary <- data_frame(
  Domain = "Communication modalities",
  Study = "King et al. (2022)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(aac_data_original),
  p_value = ifelse(
    aac_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", aac_results_original$p_value)),
  effect_size_measure = "Cohen's 𝜔",
  effect_size = round(aac_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = aac_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(aac_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == aac_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(aac_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(aac_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = aac_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(aac_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == aac_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(aac_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(aac_results_synthetic$diff_effectSize),
)

# General Utility ----
aac_data_synthetic_first <- aac_data_synthetic$syn[[1]] |> 
  as.data.frame()

aac_data_original2 <- aac_data_original |> 
  mutate(dataset_type = "Original") |> 
  as.data.frame()

# Combine the data frames
aac_combined_data <- aac_data_synthetic_first |> 
  mutate(dataset_type = "Synthetic") |> 
  rbind(aac_data_original2)

# Calculate utility with s_pMSE
aac_utility_df <- synthpop::utility.tables(
  aac_data_synthetic_first,
  aac_data_original,
  tables = "oneway",
  tab.stats = "S_pMSE",
  print.tabs = FALSE
)
    
# Extract s_pMSE value
aac_s_pMSE_value <- aac_utility_df$tabs |>
  as.data.frame() |>
  rownames_to_column(var = "rowname")

# Scatterplot
aac_p1 <- 
  aac_combined_data |> 
  ggplot(aes(x = value, color = dataset_type, fill = dataset_type)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_color_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  labs(x = "Lack of/limited internet",
       y = "Frequency",
       color = "Dataset Type",
       fill = "Dataset Type") +
  cowplot::theme_cowplot() +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(r = 8)),
        axis.title.y = element_text(margin = margin(t = 8)),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  facet_wrap(~var, nrow = 1) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 245))

# Add title
aac_title <- ggdraw() +
  draw_label_theme("F: Communication Modalities (King et al., 2022)", 
                   theme = theme_cowplot(), element = "plot.title",
                   x = 0.075, hjust = 0, vjust = 1)

# Combine plot with title
aac_final_plot <- cowplot::plot_grid(
  aac_title,
  aac_p1,
  ncol = 1,
  rel_heights = c(0.2, 3) # Adjust the relative widths as needed
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
aac <- list(
  data_original = aac_data_original,
  data_synthetic = aac_data_synthetic,
  results_original = aac_results_original,
  results_synthetic = aac_results_synthetic,
  results_summary = aac_results_summary,
  alpha = aac_alpha
)

## Cleaning up the environment
# rm(
#   aac_data_original,
#   aac_data_synthetic,
#   aac_results_original,
#   aac_results_synthetic,
#   aac_results_summary,
#   aac_alpha
# )

# Saving the data and results for easy importing later.
base::saveRDS(object = aac,
              file = here::here("Data","06_AAC","analysisData.RDS"))
```

King et al. (2022) collected survey responses from speech-language pathologists to assess the impact of the COVID-19 pandemic on service provision for emergent bilinguals who use augmentative and alternative communication. Results indicated that speech-language pathologists reporting that a lack of or limited access to internet increased during the initial phase of the pandemic (Cohen's 𝜔 = 34.60, *p* < .001). Compared to original data, the synthetic data showed similar frequencies of responses for the barrier of 'lack of/limited internet' (Figure 1F). General utility was high for assessment type (*S_pMSE* = `r round(aac_s_pMSE_value$S_pMSE[1], digits = 2)`) and time point (*S_pMSE* = `r round(hearing_s_pMSE_value$S_pMSE[2], digits = 2)`) variables. The statistical model with the synthetic data maintained the direction of statistical significance (*p* `r fmt_p_value(aac_results_synthetic[1,]$p_value)`) and effect size magnitude (*r* = `r round(aac_results_synthetic[1,]$effect_size, digits = 2)`), indicating high specific utility.

### Receptive and Expressive Language
```{r echo = FALSE}
# KEARNEY
library(DescTools) # install.packages("DescTools")

# Import original data ----
language_data_original <-
  read.csv(here::here("Data/07_Language/Kearney/demographics_CAT_reading.csv")) |>
  # select only relevant variables
  dplyr::select(c(edu, tpost_cat_read_total))

# Establish p_value for study
language_alpha <- .05

# Analysis Function ----
# Examine correlation between education (years) and reading t-scores
language_analyze_data <- function(data) {
  model <- cor.test(data$edu,
                    data$tpost_cat_read_total,
                    exact = F,
                    method = "spearman") # Creating the model
  
  # Obtaining CI's for SpearmansRho using a bootstrapping method
  model_CI <- DescTools::SpearmanRho(data$edu,
                                     data$tpost_cat_read_total,
                                     conf.level = .95)
  
  list(
    p_value = model$p.value, # Extracting p-values
    effect_size = model$estimate, # Extracting effect size
    # CI for S
    conf_int_lower = model_CI[2],
    conf_int_upper = model_CI[3]
  )
}

# Reproduce original results ----
language_results_original <- language_analyze_data(language_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < language_alpha ~ "sig",
      p_value >= language_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Create synthetic data ----
language_data_synthetic <- syn(language_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
language_results_synthetic <- language_data_synthetic$syn |>
  purrr::map_df(~ language_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < language_alpha ~ "sig",
      p_value >= language_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.5 ~ "Large",
                               abs(effect_size) >= 0.3 ~ "Medium",
                               abs(effect_size) >= 0.1 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - language_results_original$p_value),
    diff_effectSize = abs(effect_size - language_results_original$effect_size)
  ) |> 
  add_column(domain = "Receptive and expressive language")

# Compile results ----
language_results_summary <- data_frame(
  Domain = "Language",
  Study = "Kearney et al. (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(language_data_original),
  p_value = ifelse(
    language_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", language_results_original$p_value)),
  effect_size_measure = "Correlation coefficient",
  effect_size = round(language_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = language_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(language_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == language_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(language_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(language_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = language_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(language_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == language_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(language_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(language_results_synthetic$diff_effectSize),
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
language <- list(
  data_original = language_data_original,
  data_synthetic = language_data_synthetic,
  results_original = language_results_original,
  results_synthetic = language_results_synthetic,
  results_summary = language_results_summary,
  alpha = language_alpha
)

# General Utility ----
language_data_synthetic_first <- language_data_synthetic$syn[[1]] |> 
  as.data.frame()

language_data_original2 <- language_data_original |> 
  mutate(dataset_type = "Original") |> 
  as.data.frame()

# Combine the data frames
language_combined_data <- language_data_synthetic_first |> 
  mutate(dataset_type = "Synthetic") |> 
  rbind(language_data_original2)

# Calculate utility with s_pMSE
language_utility_df <- utility.tables(
  language_data_synthetic_first,
  language_data_original,
  tables = "oneway",
  tab.stats = "S_pMSE",
  print.tabs = FALSE
)
    
# Extract s_pMSE value
language_s_pMSE_value <- language_utility_df$tabs |>
  as.data.frame() |>
  rownames_to_column(var = "rowname")

# Data Viz
language_p1 <- 
  language_combined_data |> 
  ggplot(aes(x = edu, y = tpost_cat_read_total, color = dataset_type, fill = dataset_type)) +
  geom_jitter(alpha = 0.80, size = 2.5, width = 0.15, height = 0.02) +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_color_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  labs(x = "Years of Education",
       y = "Reading Score",
       color = "Dataset Type",
       fill = "Dataset Type") +
  cowplot::theme_cowplot() +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(r = 8)),
        axis.title.y = element_text(margin = margin(t = 8)))

# Add title
language_title <- ggdraw() +
  draw_label_theme("A: Language (Kearney et al., 2023)", 
                   theme = theme_cowplot(), element = "plot.title",
                   x = 0.075, hjust = 0, vjust = 1)

# Combine plot with title
language_final_plot <- cowplot::plot_grid(
  language_title,
  language_p1,
  ncol = 1,
  rel_heights = c(0.2, 3) # Adjust the relative widths as needed
)

# ## Cleaning up the environment
# rm(
#   language_data_original,
#   language_data_synthetic,
#   language_results_original,
#   language_results_synthetic,
#   language_results_summary,
#   language_alpha
# )

# Saving the data and results for easy importing later.
base::saveRDS(object = language,
              file = here::here("Data","07_Language", "Kearney", "analysisData.RDS"))
```

Two studies were included in the domain of Receptive and Expressive Language [@kearney_etal23; @robinaugh_etal24a].

Kearney et al. (2023) examined the relationship between years of education and reading performance among 36 individuals following left-hemisphere tumor resection. Results indicated a large relationship between these variables (*r* = 0.59, *p* < .001). Compared to original data, the synthetic data showed maintained a similar visual relationship between years of education and reading scores (Figure 2A). General utility was high for both years of education (*S_pMSE* = `r round(language_s_pMSE_value$S_pMSE[1], digits = 2)`) and reading scores (*S_pMSE* = `r round(language_s_pMSE_value$S_pMSE[2], digits = 2)`) variables. The statistical model with the synthetic data maintained the direction of statistical significance (*p* = `r fmt_p_value(language_results_synthetic[1,]$p_value)`) and effect size magnitude (*r* = `r round(language_results_synthetic[1,]$effect_size, digits = 2)`), indicating high specific utility.

```{r echo = FALSE}
library(kableExtra)
library(tidybayes)

# Import data
df <- read.csv(here::here("Data", "07_Language", "Robinaugh", 'SDTBI001_Data.csv')) %>%
  janitor::clean_names() %>%
  mutate(condition = ifelse(condition == 0, 'untreated', 'treated'),
         phase = as.factor(phase)) %>%
  mutate_if(is.character, as.factor) 

# Model Setup
df_coded2 <- df %>%
  filter(session < 13) %>%
  group_by(id) %>%
  mutate(phase = ifelse(phase == 'Pre', 0, 1),
         num_baselines = sum(phase == 0),
         # slope change is calculated by (T-(n1 + 1))D
         slope_change = (session - (num_baselines+1))*phase
         ) %>%
  select(id, set, condition, response, session, phase, slope_change) %>%
  mutate(phase = ifelse(condition == "untreated", 0, phase),
         slope_change = ifelse(condition == "untreated", 0, slope_change))

# Model Fitting
m.1b <- brm(response ~ 0 + Intercept + session + phase + slope_change + 
             (session + phase + slope_change | set/id), # 
           family = bernoulli(),  #binomial in lme4
           data = df_coded2, # both trained and untrained sets
           iter = 4000, # number of total samples per chain
           warmup = 1000, # number of samples tossed out per chain
           inits = 'random', # each chain takes random starting values
           chains = 4, # 4 total chains
           cores = 4, # use 4 computer cores to run chains simultaneously
           control = list(adapt_delta = .97), # to aid model convergence
           prior = c(prior(normal(-1, 2), class = b, coef = Intercept),
                     prior(normal(0, 2), class = b), # prior distributions
                     prior(normal(0, 2), class = sd),
                     prior(lkj(2), class = cor)),
           backend = 'cmdstan',
           seed = 42,
           refresh = 0, #for the markdown document 
           file = here::here("Data", "07_Language", "Robinaugh", "new_approach"),
           file_refit = "on_change"
)

pred_d <- m.1b$data %>%
  select(-response, -Intercept) %>% 
  group_by(id) %>% # for each word
  mutate(num_baselines = ifelse(set != 7, sum(phase == 0), 4)) %>% 
  #Untrained Set 7 set to give posterior distributions at session 4 
  #(last baseline without any tx) and session 12 (the last tx phase probe)
  filter(session == num_baselines | session == max(session))

es <- add_epred_draws(m.1b, newdata = pred_d |> filter(set != 7), 
                      pred = 'value', seed = 42) %>% 
  ungroup() %>% #
  mutate(time_point = ifelse(slope_change == 0, 'entry', 'exit')) %>% #
  select(time_point, id, value = .epred, draw = .draw) %>% #
  group_by(draw, time_point) %>% #
  summarize(num_corr = sum(value)) %>% #
  pivot_wider(names_from = time_point, values_from = num_corr) %>% # 
  mutate(effect_size = exit - entry) %>% #
  ungroup() %>%  # 
  select(effect_size)

es %>% # only need one variable
  tidybayes::median_qi(effect_size)

# Create synthetic data ----
robinaugh_data_synthetic <- syn(df_coded2, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# General Utility ----
robinaugh_data_synthetic_first <- robinaugh_data_synthetic$syn[[1]] |> 
  as.data.frame()

robinaugh_data_original <- df_coded2 |> 
  mutate(dataset_type = "Original") |> 
  as.data.frame()

# Combine the data frames
robinaugh_combined_data <- robinaugh_data_synthetic_first |> 
  mutate(dataset_type = "Synthetic") |> 
  rbind(robinaugh_data_original)

# Calculate utility with s_pMSE
robinaugh_utility_df <- utility.tables(
  robinaugh_data_synthetic_first,
  robinaugh_combined_data,
  tables = "oneway",
  tab.stats = "S_pMSE",
  print.tabs = FALSE
)
    
# Extract s_pMSE value
robinaugh_s_pMSE_value <- robinaugh_utility_df$tabs |>
  as.data.frame() |>
  rownames_to_column(var = "rowname")

# Data Viz
robinaugh_p1 <- 
  robinaugh_combined_data |> 
  ggplot(aes(x = factor(session), color = dataset_type, fill = dataset_type)) +
  geom_bar(stat = "count", position = "dodge") +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_color_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  labs(x = "Session",
       y = "Frequency",
       color = "Dataset Type",
       fill = "Dataset Type") +
  cowplot::theme_cowplot() +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(r = 8)),
        axis.title.y = element_text(margin = margin(t = 8))) +
  scale_y_continuous(expand=c(0,0))

robinaugh_p2 <- 
  robinaugh_combined_data |> 
  ggplot(aes(x = factor(response), color = dataset_type, fill = dataset_type)) +
  geom_bar(stat = "count", position = "dodge") +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  scale_color_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  labs(x = "Response",
       y = "Frequency",
       color = "Dataset Type",
       fill = "Dataset Type") +
  cowplot::theme_cowplot() +
  theme(legend.position = "none",
        axis.title.x = element_text(margin = margin(r = 8)),
        axis.title.y = element_text(margin = margin(t = 8))) +
  scale_y_continuous(expand=c(0,0))

# Combine plots
robinaugh_combined <- cowplot::plot_grid(robinaugh_p1,
                                         robinaugh_p2,
                                         ncol = 1,
                                         rel_heights = c(0.5, 0.5))

# Add title
robinaugh_title <- ggdraw() +
  draw_label_theme("B: Language (Robinaugh et al., 2024)", 
                   theme = theme_cowplot(), element = "plot.title",
                   x = 0.075, hjust = 0, vjust = 1)

# Combine plot with title
robinaugh_final_plot <- cowplot::plot_grid(
  robinaugh_title,
  robinaugh_combined,
  ncol = 1,
  rel_heights = c(0.2, 3) # Adjust the relative widths as needed
)

# Specific Utility ----
# Model Fitting
synthetic_model <- brm(response ~ 0 + Intercept + session + phase + slope_change + 
             (session + phase + slope_change | set/id), # 
           family = bernoulli(),  #binomial in lme4
           data = robinaugh_data_synthetic_first, # both trained and untrained sets
           iter = 4000, # number of total samples per chain
           warmup = 1000, # number of samples tossed out per chain
           inits = 'random', # each chain takes random starting values
           chains = 4, # 4 total chains
           cores = 4, # use 4 computer cores to run chains simultaneously
           control = list(adapt_delta = .97), # to aid model convergence
           prior = c(prior(normal(-1, 2), class = b, coef = Intercept),
                     prior(normal(0, 2), class = b), # prior distributions
                     prior(normal(0, 2), class = sd),
                     prior(lkj(2), class = cor)),
           backend = 'cmdstan',
           seed = 42,
           refresh = 0, #for the markdown document 
           file = here::here("Data", "07_Language", "Robinaugh", "synthetic_model"),
           file_refit = "on_change"
)

synthetic_pred_d <- synthetic_model$data %>%
  select(-response, -Intercept) %>% 
  group_by(id) %>% # for each word
  mutate(num_baselines = ifelse(set != 7, sum(phase == 0), 4)) %>% 
  #Untrained Set 7 set to give posterior distributions at session 4 
  #(last baseline without any tx) and session 12 (the last tx phase probe)
  filter(session == num_baselines | session == max(session))

synthetic_es <- add_epred_draws(m.1b, newdata = synthetic_pred_d |> filter(set != 7), 
                      pred = 'value', seed = 42) %>% 
  ungroup() %>% #
  mutate(time_point = ifelse(slope_change == 0, 'entry', 'exit')) %>% #
  select(time_point, id, value = .epred, draw = .draw) %>% #
  group_by(draw, time_point) %>% #
  summarize(num_corr = sum(value)) %>% #
  pivot_wider(names_from = time_point, values_from = num_corr) %>% # 
  mutate(effect_size = exit - entry) %>% #
  ungroup() %>%  # 
  select(effect_size)

synthetic_es_summarized <- synthetic_es %>% # only need one variable
  tidybayes::median_qi(effect_size)
```

Robinaugh et al., (2024) examined the effectiveness of a naming treatment in a single-case experimental design for an individual presenting with semantic variant primary progressive aphasia and a history of traumatic brain injury. An item-level Bayesian generalized mixed-effects model revealed that the treatment resulted in a gain of 35 out of 60 trained words (β = 35.3; 90% CI: 30.6, 39.5). Compared to original data, the synthetic data showed similar frequencies of responses, but not sessions (Figure 2B). General utility was high for id (*S_pMSE* = `r round(robinaugh_s_pMSE_value$S_pMSE[1], digits = 2)`), set (*S_pMSE* = `r round(robinaugh_s_pMSE_value$S_pMSE[2], digits = 2)`), session (*S_pMSE* = `r round(robinaugh_s_pMSE_value$S_pMSE[5], digits = 2)`), and phase (*S_pMSE* = `r round(robinaugh_s_pMSE_value$S_pMSE[6], digits = 2)`) variables. The statistical model with the synthetic data overestimated the effect size (β = `r round(synthetic_es_summarized$effect_size, digits = 2)`; 90% CI: `r round(synthetic_es_summarized$.lower, digits = 2)`, `r round(synthetic_es_summarized$.upper, digits = 2)`), indicating that specific utility was low.

### Cognitive Aspects of Communication
```{r echo = FALSE}
# Load packages
library(lme4) # glmer
library(lmerTest) # p-values for glmer

# Import original data ----
cognition_data_original <-
  read.csv(here::here("Data/08_Cognition/emoji_affect_recognition_data.csv")) |>
  # select only relevant variables
  dplyr::select(c(SubjectID, Correct, Group, Condition, Sex)) |>
  # dummy-code Group (NC = 1, TBI = 0) and Sex (F = 1, M = 0)
  mutate(
    Group_NC = if_else(Group == "NC", 1, 0),
    Sex_F = if_else(Sex == "Female", 1, 0)) |>
  # treat categorical variables as factors and set reference levels
  mutate(
    SubjectID = as.factor(SubjectID),
    Correct = as.factor(Correct),
    Group_NC = relevel(as.factor(Group_NC), '1', '0'),
    Condition = relevel(as.factor(Condition), 'BasicFace', 'BasicEmoji', 'SocialEmoji'),
    Sex_F =  relevel(as.factor(Sex_F), '1', '0')
  ) |> 
  dplyr::select(-c(Group, Sex))

# Set up Helmert contrast coding for Condition
# https://marissabarlaz.github.io/portfolio/contrastcoding/
# Contrast 1: Basic Emotion Faces vs average of Basic Emotion Emoji and Social Emotion Emoji
# Contrast 2: Basic Emotion Emoji vs Social Emotion Emoji
helmert = matrix(c(2/3, -1/3, -1/3, 0, 1/2, -1/2), ncol = 2)
contrasts(cognition_data_original$Condition) = helmert

cognition_alpha <- .05

# Analysis Function ----
## Models: Effect of stimulus condition on emotion recognition accuracy
## Note: Model first failed to converge when using default control settings, so changed optimizer to bobyqa as suggested here: https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
cognition_analyze_data <- function(data) {
  # Creating the model
  m1 <-
    lme4::glmer(Correct ~ Group_NC + Condition + Sex_F + Group_NC * Condition * Sex_F + (1 | SubjectID),
      data = data,
      family = binomial(link = "logit"),
      control = lme4::glmerControl(optimizer = "bobyqa",
                             optCtrl = list(maxfun = 2e5))
    )
  
  # Set group reference level to TBI (0) and rerun model
  data$Group_NC = relevel(data$Group_NC, '0', '1')
  m2 <-
    lme4::glmer(
      Correct ~ Group_NC + Condition + Sex_F + Group_NC * Condition * Sex_F + (1 | SubjectID),
      data = data,
      family = binomial(link = "logit"),
      control = lme4::glmerControl(optimizer = "bobyqa",
                             optCtrl = list(maxfun = 2e5))
    )
  
  model_fit <- summary(m2)
  
  random_effects <- unlist(ranef(m2))
  
  list(
    p_value = model_fit$coefficients[7,4], # Extracting p-values
    # Extracting effect size for Group (NC0) X Condition (2) Interaction - standardized effect size (Haddock et al., 1998; Hasselblad & Hedges, 1995)
    OR = exp(model_fit$coefficients[4,1]),
    effect_size = (sqrt(3/pi))*exp(model_fit$coefficients[4,1]),
    conf_int_lower = exp(confint(m2, method = "Wald", level = .95)[5,][1]), # Extracting lower CI bound
    conf_int_upper = exp(confint(m2, method = "Wald", level = .95)[5,][2]), # Extracting upper CI bound
    random_mean = mean(random_effects), # Extracting random effects mean
    random_conf_int_lower = mean(random_effects) - 1.96 * (sd(random_effects) / length(random_effects)), # Extracting random effects 95% CI lower bound
    random_conf_int_upper = mean(random_effects) + 1.96 * (sd(random_effects) / length(random_effects)) # Extracting random effects 95% CI upper bound
  )
}

# Reproduce original results ----
cognition_results_original <- cognition_analyze_data(cognition_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < cognition_alpha ~ "sig",
      p_value >= cognition_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )


# Create synthetic data ----
cognition_data_synthetic <- syn(cognition_data_original, 
                                   method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
cognition_results_synthetic <- cognition_data_synthetic$syn |>
  purrr::map_df(~ cognition_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < cognition_alpha ~ "sig",
      p_value >= cognition_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - cognition_results_original$p_value),
    diff_effectSize = abs(effect_size - cognition_results_original$effect_size)
  ) |> 
  add_column(domain = "Cognition")

# Compile results ----
cognition_results_summary <- data_frame(
  Domain = "Cognition",
  Study = "Clough et al. (2023))",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(cognition_data_original),
  p_value = ifelse(
    cognition_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", cognition_results_original$p_value)),
  effect_size_measure = "√(3/𝜋) x odds ratio",
  effect_size = round(cognition_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = cognition_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(cognition_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == cognition_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(cognition_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(cognition_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = cognition_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(cognition_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == cognition_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(cognition_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(cognition_results_synthetic$diff_effectSize),
)

# General Utility ----
cognition_data_synthetic_first <- cognition_data_synthetic$syn[[1]] |> 
  as.data.frame()

cognition_data_original <- cognition_data_original |> 
  mutate(dataset_type = "Original") |> 
  as.data.frame()

# Combine the data frames
cognition_combined_data <- cognition_data_synthetic_first |> 
  mutate(dataset_type = "Synthetic") |> 
  rbind(cognition_data_original)

# Calculate utility with s_pMSE
cognition_utility_df <- utility.tables(
  cognition_data_synthetic_first,
  cognition_combined_data,
  tables = "oneway",
  tab.stats = "S_pMSE",
  print.tabs = FALSE
)
    
# Extract s_pMSE value
cognition_s_pMSE_value <- cognition_utility_df$tabs |>
  as.data.frame() |>
  rownames_to_column(var = "rowname")

# Data viz
cognition_p1 <- 
  cognition_combined_data |> 
  mutate(Correct = as.numeric(as.character(Correct)),
         Group_NC = case_when(Group_NC == "BasicFace" ~ "Basic Face",
                              Group_NC == "BasicEmoji" ~ "Basic Emoji",
                              Group_NC == "SocialEmoji" ~ "Social Emoji")) |> 
  group_by(Group_NC, Condition, dataset_type) %>%
  summarize(Proportion_Correct = mean(Correct),
            Count = n(),
            .groups = 'drop') |> 
  ggplot(aes(x = dataset_type, y = Proportion_Correct, fill = as.factor(dataset_type))) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Condition) +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  labs(x = "",
       y = "Proportion Correct",
       fill = "Group NC") +
  cowplot::theme_cowplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank(),
        legend.position = "none") +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1),
                     labels = scales::percent)

# Add title
cognition_title <- ggdraw() +
  draw_label_theme("C: Cognition (Clough  et al., 2023)", 
                   theme = theme_cowplot(), element = "plot.title",
                   x = 0.075, hjust = 0, vjust = 1)

# Combine plot with title
cognition_final_plot <- cowplot::plot_grid(
  cognition_title,
  cognition_p1,
  ncol = 1,
  rel_heights = c(0.2, 3) # Adjust the relative widths as needed
)


# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
cognition <- list(
  data_original = cognition_data_original,
  data_synthetic = cognition_data_synthetic,
  results_original = cognition_results_original,
  results_synthetic = cognition_results_synthetic,
  results_summary = cognition_results_summary,
  s_pMSE_value = cognition_s_pMSE_value,
  alpha = cognition_alpha
)

## Cleaning up the environment
rm(
  cognition_data_original,
  cognition_data_synthetic,
  cognition_results_original,
  cognition_results_synthetic,
  cognition_results_summary,
  cognition_s_pMSE_value,
  cognition_alpha,
  helmert
)

# Saving the data and results for easy importing later.
base::saveRDS(object = cognition,
              file = here::here("Data","08_Cognition","analysisData.RDS"))
```

Clough et al. (2023) examined the interaction between group (traumatic brain injury (TBI) or neurotypical) and condition (basic emotion or social emotion emojis) on the accuracy of emotion recognition. A generalized linear mixed effects model indicated that participants with TBI were more likely to correctly identify basic emotions than social emotions when presented as emoji (*OR* = `r round(cognition$results_original$eOR, digits = 2)`), *p* = `r round(cognition$results_original$p_value, digits = 3)`), whereas neurotypical participants did not differ in their ability to identify these emotions. Compared to original data, the synthetic data showed a similar distribution of responses for both basic and social emotions for the TBI group (Figure X). General utility was high for subject (*S_pMSE* = `r round(cognition$s_pMSE_value$S_pMSE[1], digits = 2)`) and condition (*S_pMSE* = `r round(cognition$s_pMSE_value$S_pMSE[3], digits = 2)`) variables. Specific utility was low as the statistical model with the synthetic data did not maintain the direction of statistical significance (*p* = `r round(cognition$results_synthetic$p_value[1], digits = 3)`)), even though the effect size magnitude was still considered large (*OR* = `r round(cognition$results_synthetic$OR[1], digits = 2)`). The random effect estimates were stable between the original (mean = `r round(cognition$results_original$random_mean, digits = 3)`, 95% CI: `r round(cognition$results_original$random_conf_int_lower, digits = 3)`, `r round(cognition$results_original$random_conf_int_upper, digits = 3)`) and synthetic (mean = `r round(cognition$results_synthetic$random_mean[1], digits = 3)`, 95% CI: `r round(cognition$results_synthetic$random_conf_int_lower[1], digits = 3)`, `r round(cognition$results_synthetic$random_conf_int_upper[1], digits = 3)`) datasets.

### Social Aspects of Communication
```{r echo = FALSE}
# Import original data ----
social_data_original <-
  read.csv(here::here("Data/09_Social_Communication/Study1Comp.csv"),
           fileEncoding = 'UTF-8-BOM') |> 
  plyr::ddply(
    c("ParGroup", "Subj", "Gender"),
    summarise,
    N    = length(Accuracy),
    acc  = sum(Accuracy),
    pct = mean(Accuracy),
    grade = mean(Grade),
    age = mean(Months),
    NVIQ = mean(SSIQ)
  ) |> 
  dplyr::select(c(NVIQ, ParGroup))

# Establish p_value for study
social_alpha <- .05

# Analysis Function ----
# Model: Non-verbal IQ by group (ASD, TD) two-tailed Fisher’s Exact Test
social_analyze_data <- function(data) {
  model <- aov(NVIQ ~ ParGroup,
               data = data) # Creating the model
  model_summary <- summary(model)
  
  list(
    p_value = model_summary[[1]]$'Pr(>F)'[1],
    # Extracting p-values
    effect_size = effectsize::cohens_d(NVIQ ~ ParGroup,
                     data = data,
                     ci = 0.95)$Cohens_d,
    # Extracting effect size
    conf_int_lower = confint(model)[2, ][1],
    # Extracting lower CI bound
    conf_int_upper = confint(model)[2,][2] # Extracting upper CI bound
  )
}

# Reproduce original results ----
social_results_original <- social_analyze_data(social_data_original) |>
  as.data.frame(row.names = NULL) |>
  mutate(
    dataset = "original",
    sig = case_when(
      p_value < social_alpha ~ "sig",
      p_value >= social_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible")
  )

# Create synthetic data ----
social_data_synthetic <- syn(social_data_original, 
                                   # method = "ctree", 
                                   m = 100,
                                   seed = 2024)

# Perform analysis on each synthetic dataset
social_results_synthetic <- social_data_synthetic$syn |>
  purrr::map_df(~ social_analyze_data(.x), .id = "dataset") |>
  dplyr::mutate(
    sig = case_when(
      p_value < social_alpha ~ "sig",
      p_value >= social_alpha ~ "non-sig"
    ),
    categorization = case_when(abs(effect_size) >= 0.8 ~ "Large",
                               abs(effect_size) >= 0.5 ~ "Medium",
                               abs(effect_size) >= 0.2 ~ "Small",
                               TRUE ~ "Negligible"),
    # Calculate the difference between the original p-value/effect size and synthetic p-value/effect sizes.
    diff_pvalue = abs(p_value - social_results_original$p_value),
    diff_effectSize = abs(effect_size - social_results_original$effect_size)
  ) |> 
  add_column(domain = "Social aspects of communication")

# Compile results ----
social_results_summary <- data_frame(
  Domain = "Social aspects of communication",
  Study = "Chanchaochai & Schwarz (2023)",
  
  # Pulling sample size, p value, and effect sizes from the original results
  N = nrow(social_data_original),
  p_value = ifelse(
    social_results_original$p_value < .001, "<.001",
    sprintf("%0.3f", social_results_original$p_value)),
  effect_size_measure = "Cohen’s d",
  effect_size = round(social_results_original$effect_size, digits = 2),
  
  # Creating the % Agreement for p values between the original and the synthetic datasets
  synAgreement_pvalue = social_results_synthetic |>
    count(sig) |>
    dplyr::mutate(percent = n / NROW(social_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(sig == social_results_original$sig) |>
    dplyr::pull(percent),
  absDiff_pvalue_mean = mean(social_results_synthetic$diff_pvalue),
  absDiff_pvalue_sd = sd(social_results_synthetic$diff_pvalue),
  
  # Creating the % Agreement for effect sizes between the original and the synthetic datasets
  synAgreement_effectSize = social_results_synthetic |>
    count(categorization) |>
    dplyr::mutate(percent = n / nrow(social_results_synthetic) * 100,
                  percent = sprintf("%0.0f%%", percent)) |> # Formatting to be a %
    dplyr::filter(categorization == social_results_original$categorization) |>
    dplyr::pull(percent),
  absDiff_effectSize_mean = mean(social_results_synthetic$diff_effectSize),
  absDiff_effectSize_sd = sd(social_results_synthetic$diff_effectSize),
)

# General Utility ----
social_data_synthetic_first <- social_data_synthetic$syn[[1]] |> 
  as.data.frame()

social_data_original <- social_data_original |> 
  mutate(dataset_type = "Original") |> 
  as.data.frame()

# Combine the data frames
social_combined_data <- social_data_synthetic_first |> 
  mutate(dataset_type = "Synthetic") |> 
  rbind(social_data_original)

# Calculate utility with s_pMSE
social_utility_df <- utility.tables(
  social_data_synthetic_first,
  social_combined_data,
  tables = "oneway",
  tab.stats = "S_pMSE",
  print.tabs = FALSE
)
    
# Extract s_pMSE value
social_s_pMSE_value <- social_utility_df$tabs |>
  as.data.frame() |>
  rownames_to_column(var = "rowname")

# Data viz
social_p1 <- 
  social_combined_data |> 
  mutate(ParGroup = case_when(ParGroup == "ASD" ~ "Autism Spectrum Disorder",
                              ParGroup == "TD" ~ "Neurotypical")) |> 
  ggplot(aes(x = NVIQ, fill = as.factor(dataset_type))) +
  geom_density(alpha = 0.80) +
  facet_wrap(~ ParGroup, nrow = 2) +
  scale_fill_manual(values = c("Synthetic" = "#184765", "Original" = "#5EC0D0")) +
  labs(x = "Non-Verbal IQ",
       y = "Density",
       fill = "Group NC") +
  cowplot::theme_cowplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.04)) +
  scale_x_continuous(limits = c(25, 200))

# Add title
social_title <- ggdraw() +
  draw_label_theme("D: Social (Chanchaochai & Schwarz, 2023)", 
                   theme = theme_cowplot(), element = "plot.title",
                   x = 0.075, hjust = 0, vjust = 1)

# Combine plot with title
social_final_plot <- cowplot::plot_grid(
  social_title,
  social_p1,
  ncol = 1,
  rel_heights = c(0.2, 3) # Adjust the relative widths as needed
)

# Tidying the R project environment ----
## Throwing the relevant environment objects into a list for this study.
social <- list(
  data_original = social_data_original,
  data_synthetic = social_data_synthetic,
  results_original = social_results_original,
  results_synthetic = social_results_synthetic,
  results_summary = social_results_summary,
  alpha = social_alpha
)

## Cleaning up the environment
# rm(
#   social_data_original,
#   social_data_synthetic,
#   social_results_original,
#   social_results_synthetic,
#   social_results_summary,
#   social_alpha
# )

# Saving the data and results for easy importing later.
base::saveRDS(object = social,
              file = here::here("Data","09_Social_Communication","analysisData.RDS"))
```

Chanchaochai & Schwarz et al. (2023) compared non-verbal IQ between individuals with autism spectrum disorder and neurotypical peers. An analysis of variance indicated that neurotypical individuals demonstrated higher non-verbal IQ (*d* = -0.85, *p* < .001). Compared to original data, the synthetic data showed similar distributions of non-verbal IQ for both groups (Figure 2D). General utility was high for both group (*S_pMSE* = `r round(social_s_pMSE_value$S_pMSE[2], digits = 2)`) and non-verbal IQ (*S_pMSE* = `r round(social_s_pMSE_value$S_pMSE[1], digits = 2)`) variables. The statistical model with the synthetic data maintained the direction of statistical significance (*p* = `r fmt_p_value(social_results_synthetic[1,]$p_value)`); however, the effect size magnitude (*r* = `r round(social_results_synthetic[1,]$effect_size, digits = 2)`) was lower, indicating a low level of specific utility.

```{r dispersion figures, echo = FALSE}
# Visualize p-value distributions
library(scales)

articulation_pval_viz <- 
  articulation$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(articulation$results_original$p_value))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(limits = c(-35, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("A: Articulation",
          subtitle = "Mean difference of 0.05 (SD = 0.10)") +
  geom_vline(xintercept = log(articulation$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)

fluency_pval_viz <- 
  fluency$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(fluency$results_original$p_value))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(limits = c(-35, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("B: Fluency",
          subtitle = "Mean difference of 0.0000007 (SD = 0.00003)") +
  geom_vline(xintercept = log(fluency$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
voice_pval_viz <- 
  voice$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(voice$results_original$p_value))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(limits = c(-35, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("C: Voice and resonance",
          subtitle = "Mean difference of 0.005 (SD = 0.05)") +
  geom_vline(xintercept = log(voice$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
hearing_pval_viz <- 
  hearing$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(hearing$results_original$p_value))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(limits = c(-35, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("D: Hearing",
          subtitle = "Mean difference of 0.03 (SD = 0.04)") +
  geom_vline(xintercept = log(hearing$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
language_pval_viz <- 
  language$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(language$results_original$p_value))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(limits = c(-35, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("E: Receptive and expressive language",
          subtitle = "Mean difference of 0.0009 (SD = 0.002)") +
  geom_vline(xintercept = log(language$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
aac_pval_viz <- 
  aac$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(aac$results_original$p_value))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(limits = c(-35, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("F: Communication modalities",
          subtitle = "Mean difference of 0.00004 (SD = 0.0002)") +
  geom_vline(xintercept = log(aac$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
cognition_pval_viz <- 
  cognition$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(cognition$results_original$p_value))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(limits = c(-35, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("G: Cognition",
          subtitle = "Mean difference of 0.25 (SD = 0.28)") +
  geom_vline(xintercept = log(cognition$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
social_pval_viz <- 
  social$results_synthetic |> 
  mutate(log_pvalue = log(p_value)) |> 
  ggplot(aes(x = log_pvalue)) +
  geom_density() +
  geom_area(
    aes(x = stage(log_pvalue, after_stat = oob_censor(x, c(-2000, log(social$results_original$p_value))))),
    stat = "density",
    color = "lightgreen",
    fill = "lightgreen",
    alpha = 0.5
  ) +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(limits = c(-35, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "log(p-value)",
       y = "Density") +
  ggtitle("H: Social aspects of communication",
          subtitle = "Mean difference of 0.01 (SD = 0.07)") +
  geom_vline(xintercept = log(social$results_original$p_value), color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)

# Combine p-value figures together
pval_combined <- cowplot::plot_grid(articulation_pval_viz,
                   fluency_pval_viz,
                   voice_pval_viz,
                   hearing_pval_viz,
                   language_pval_viz,
                   aac_pval_viz,
                   cognition_pval_viz,
                   social_pval_viz, 
                   nrow = 4, 
                   align = "hv"
                   )

# save figure
ggsave(
  filename = here::here(
    "Manuscript/tables-figures/figure_3.png"),
    pval_combined,
    height = 10,
    width = 10,
    dpi = 300,
    bg = "white"
  )

# Visualize effect size distributions
articulation_es_viz <- 
  articulation$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=0.40, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.59, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "Cohen's f",
       y = "Count") +
  ggtitle("A: Articulation",
          subtitle = "Mean difference of 0.19 (SD = 0.12)") +
  geom_vline(xintercept = 0.59, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)

fluency_es_viz <- 
  fluency$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=Inf, xmax=-Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(-4, -3, -2.18, -1), limits = c(-4, -1)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "Glass' Δ",
       y = "Count") +
  ggtitle("B: Fluency",
          subtitle = "Mean difference of 0.36 (SD = 0.28)") +
  geom_vline(xintercept = -2.18, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
voice_es_viz <- 
  voice$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=0.50, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(0, 0.20, 0.40, 0.51, 0.60, 0.80), limits = c(0, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "Correlation coefficient",
       y = "Count") +
  ggtitle("C: Voice and resonance",
          subtitle = "Mean difference of 0.09 (SD = 0.07)") +
  geom_vline(xintercept = 0.51, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
hearing_es_viz <- 
  hearing$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(1, 1.2, 1.4, 1.56, 1.8, 2, 2.2), limits = c(0.90, 2.20)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "√(3/𝜋) x odds ratio",
       y = "Count") +
  ggtitle("D: Hearing",
          subtitle = "Mean difference of 0.18 (SD = 0.14)") +
  geom_vline(xintercept = 1.56, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
language_es_viz <- 
  language$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=0.50, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(0.30, 0.4, 0.5, 0.59, 0.7, 0.8), 
                     limits = c(0.30, 0.80)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "Correlation coefficient",
       y = "Count") +
  ggtitle("E: Receptive and expressive language",
          subtitle = "Mean difference of 0.06 (SD = 0.05)") +
  geom_vline(xintercept = 0.59, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
aac_es_viz <- 
  aac$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(10, 20, 30, 34.6, 40, 50), limits = c(10, 50)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "Cohen's 𝜔",
       y = "Count") +
  ggtitle("F: Communication modalities",
          subtitle = "Mean difference of 7.24 (SD = 5.08)") +
  geom_vline(xintercept = 34.6, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
cognition_es_viz <- 
  cognition$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=0.80, xmax=Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(0.80, 1, 1.25, 1.53, 1.75, 2), limits = c(0.70, 2.1)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "√(3/𝜋) x odds ratio",
       y = "Count") +
  ggtitle("G: Cognition",
          subtitle = "Mean difference of 0.28 (SD = 0.19)") +
  geom_vline(xintercept = 1.53, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)
  
social_es_viz <- 
  social$results_synthetic |> 
  ggplot(aes(x = effect_size)) +
  geom_rect(aes(xmin=-0.80, xmax=-Inf, ymin=0, ymax=Inf),
            color = "#e5f5f9", fill = "#e5f5f9") +
  geom_histogram(bins = 50, color = "#66c2a4", fill = "#66c2a4") +
  cowplot::theme_cowplot() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_x_continuous(breaks = c(-2, -1.5, -0.85, -0.5, 0), limits = c(-2, 0)) +
  theme(legend.position = "top",
        legend.justification="center",
        axis.title.x = element_text(margin = margin(t = 4)),
        axis.title.y = element_text(margin = margin(r = 2))) +
  labs(x = "√(3/𝜋) x odds ratio",
       y = "Count") +
  ggtitle("H: Social aspects of communication",
          subtitle = "Mean difference of 0.21 (SD = 0.20)") +
  geom_vline(xintercept = -0.85, color = "black", 
             size = 0.75, linetype = "dashed", alpha = 0.75)

# Combine effect size figures together
es_combined <- cowplot::plot_grid(articulation_es_viz,
                   fluency_es_viz,
                   voice_es_viz,
                   hearing_es_viz,
                   language_es_viz,
                   aac_es_viz,
                   cognition_es_viz,
                   social_es_viz, 
                   nrow = 4, 
                   align = "hv"
                   )

# save figure
ggsave(
  filename = here::here(
    "Manuscript/tables-figures/figure_4.png"),
    es_combined,
    height = 10,
    width = 10,
    dpi = 300,
    bg = "white"
  )
```

##### Figure 3 here.

##### Figure 4 here.

##### Table 4 here.

```{r table 4, echo = FALSE, eval = FALSE}

# This code reads in a template, generates table 3, 
# and then saves it as word doc in the appropriate folder

# Load in the data
swallowing <- base::readRDS(file = here::here("Data","01_Swallowing","analysisData.RDS"))
articulation <- base::readRDS(file = here::here("Data","02_Articulation","analysisData.RDS"))
fluency <- base::readRDS(file = here::here("Data","03_Fluency","analysisData.RDS"))
voice <- base::readRDS(file = here::here("Data","04_Voice_Resonance","analysisData.RDS"))
hearing <- base::readRDS(file = here::here("Data","05_Hearing","analysisData.RDS"))
aac <- base::readRDS(file = here::here("Data","06_AAC","analysisData.RDS"))
language <- base::readRDS(file = here::here("Data","07_Language","analysisData.RDS"))
cognition <- base::readRDS(file = here::here("Data","08_Cognition","analysisData.RDS"))
social <-base::readRDS(file = here::here("Data","09_Social_Communication","analysisData.RDS"))

cols2 <- tibble(
  `Domain` = c(
    "Swallowing",
    "Articulation",
    "Fluency",
    "Voice and resonance",
    "Hearing",
    "Communication modalities",
    "Receptive and expressive language",
    "Cognitive aspects of communication",
    "Social aspects of communication"
  ),
  `Study` = c(
    "Curtis et al. (2023)",
    "Thompson et al. (2023)",
    "Elsherif et al. (2021)",
    "Novotný et al. (2016)",
    "Battal et al. (2019)",
    "King et al. (2022)",
    "Kearney et al. (2023)",
    "Clough et al. (2023)",
    "Chanchaochai & Schwarz (2023)"
  ),
  `Sample Size` = c(
    swallowing$results_summary$N,
    articulation$results_summary$N,
    fluency$results_summary$N,
    voice$results_summary$N,
    hearing$results_summary$N,
    aac$results_summary$N,
    language$results_summary$N,
    cognition$results_summary$N,
    social$results_summary$N
  ),
  `P-value` = c(
    swallowing$results_summary$p_value_mu,
    articulation$results_summary$p_value,
    fluency$results_summary$p_value,
    voice$results_summary$p_value,
    sub("^0", "", hearing$results_summary$p_value),
    aac$results_summary$p_value,
    language$results_summary$p_value,
    sub("^0", "", cognition$results_summary$p_value),
    social$results_summary$p_value
  ),
  `Effect Size Measure` = c(
    swallowing$results_summary$effect_size_measure_mu,
    articulation$results_summary$effect_size_measure,
    fluency$results_summary$effect_size_measure,
    voice$results_summary$effect_size_measure,
    hearing$results_summary$effect_size_measure,
    aac$results_summary$effect_size_measure,
    language$results_summary$effect_size_measure,
    cognition$results_summary$effect_size_measure,
    social$results_summary$effect_size_measure
  ),
  `Effect Size` = c(
    sub("^0", "", swallowing$results_summary$effect_size_mu),
    sub("^0", "", articulation$results_summary$effect_size),
    sub("^0", "", fluency$results_summary$effect_size),
    sub("^0", "", voice$results_summary$effect_size),
    sub("^0", "", hearing$results_summary$effect_size),
    sub("^0", "", aac$results_summary$effect_size),
    sub("^0", "", language$results_summary$effect_size),
    sub("^0", "", cognition$results_summary$effect_size),
    sub("0.", ".", social$results_summary$effect_size)
  ),
  `P-value\nAgreement` = c(
    paste0(swallowing$results_summary$synAgreement_pvalue_nu, 
           " (zero-inflated) & ", 
           swallowing$results_summary$synAgreement_pvalue_mu, 
           " (beta)"),
    articulation$results_summary$synAgreement_pvalue,
    fluency$results_summary$synAgreement_pvalue,
    voice$results_summary$synAgreement_pvalue,
    hearing$results_summary$synAgreement_pvalue,
    aac$results_summary$synAgreement_pvalue,
    language$results_summary$synAgreement_pvalue,
    cognition$results_summary$synAgreement_pvalue,
    social$results_summary$synAgreement_pvalue
  ),
  `Effect Size Categorization\nAgreement` = c(
    paste0(swallowing$results_summary$synAgreement_effectSize_nu, 
           " (zero-inflated) & ", 
           swallowing$results_summary$synAgreement_effectSize_mu, 
           " (beta)"),
    articulation$results_summary$synAgreement_effectSize,
    fluency$results_summary$synAgreement_effectSize,
    voice$results_summary$synAgreement_effectSize,
    hearing$results_summary$synAgreement_effectSize,
    aac$results_summary$synAgreement_effectSize,
    language$results_summary$synAgreement_effectSize,
    cognition$results_summary$synAgreement_effectSize,
    social$results_summary$synAgreement_effectSize
  )
)

set_flextable_defaults(font.family = "Times New Roman")

t2 <- flextable(cols2) |>
  set_caption("Table 4: Stability of synthetic datasets across ASHA domains.") |> 
  set_table_properties(layout = "autofit", width = 1) |> 
  footnote(
    i = 1,
    j = 4,
    value = as_paragraph(
      "Caption: 100 synthetic datasets were generated for each domain. For the Swallowing domain, a zero-inflated beta multilevel model was performed to directly compare original and synthetic datasets. The percentage of non-significant p-values, which is indicative of no statistically significant difference between dataset types, is shown for both zero-inflated and beta portions of the model."
    ),
    ref_symbols = c("a"),
    part = "header"
  )

doc <- read_docx(here("manuscript", "templates-data", "template.docx"))
doc <- body_add_flextable(doc, value = t2)
fileout <- here("manuscript", "tables-figures", "table4.docx") # uncomment to write in your working directory
print(doc, target = fileout)
```

# Discussion

Although computational reproducibility is a core principle of science, data sharing is uncommon in CSD, partly due to concerns regarding disclosure risk [@pfeiffer_etal24]. This study demonstrates the utility of synthetic datasets to protect participant confidentiality while preserving the statistical properties and relationships of the original analysis data. The utility of synthetic data is further strengthened by the range of datasets included in the current study, which varied by domain (across nine ASHA domains), sample size (from 40 to >8,000 data points), statistical models (from simple correlations to multilevel model with 3-way interactions), and effect sizes (from conventionally "small" to "large"). These results suggest that synthetic datasets can be effectively used across a wide range of studies in the field of CSD to preserve participant confidentiality when sharing data.

One key finding is that lower agreement between synthetic and original datasets was not attributed to sample size, despite the *synthpop* package's recommendation of a minimum of 130 observations for generating synthetic datasets [@nowok_etal16]. For example, in the original study from the cognition domain, which included over 8,000 observations, only 35% of synthetic datasets maintained the same inferential result as the original dataset. Instead, *p*-value and effect size agreement between the synthetic and original datasets was influenced by the original data’s proximity to the statistical significance or effect size thresholds. For example, the original cognition study reported a *p*-value of .013, resulting in a 35% agreement rate for synthetic datasets. Conversely, studies that reported an original *p*-value of <.001 showed a *p*-value agreement rate of 97-100%, with the exception of the articulation study, which had a *p*-value agreement of 71%.

These findings highlight the importance of verifying the accuracy of synthetic datasets and providing these comparisons in supplemental manuscript materials. To ensure synthetic data quality, researchers should generate multiple versions of a synthetic dataset and select the one that most closely reproduces the statistical findings of the original analysis. If the synthetic dataset fails to sufficiently maintain these relationships, it should not be shared.

This study is not without limitations. We used predetermined thresholds (e.g., 'significant' *p*-values and effect size categories) to evaluate whether synthetic data maintained the relationships observed in the original study. When the original analyses had *p*-values near the threshold for significance (e.g., .01 < *p* < .05) or effect sizes near the boundary of a category, lower agreement was more likely. This likely reflects the distribution of synthetic data across both sides of these thresholds rather than actual poor agreement (Figures 3 & 4). Additionally, it's important to recognize that synthetic data is inherently a proxy and cannot entirely preserve all statistical properties of the original dataset. Therefore, researchers should provide de-identified (or identifiable when ethical approval is obtained) data whenever possible, as well as evaluate the utility of the synthetic dataset in the context of their own study. Finally, open data alone does not ensure computational reproducibility. Instead, both open data and accompanied code or syntax is required to reproduce analyses. In fact, recent research showed that a high percentage of findings from registered reports that provided open data were unable to be reproduced [@obels_etal20a]. Reproducible workflows in languages like R have been proposed and warrant consideration [@peikert_etal21a].

## Data Sharing Framework

In this framework, we aim to empower researchers who may feel uncertain or unmotivated to consider data sharing for their current or future work. We begin by examining the scientific and ethical implications of closed data, followed by an evaluation of the commonly used “available upon request” approach to data sharing, which we argue is insufficient. Finally, we outline the benefits of open data practices, emphasizing that sharing different types of data (raw, intermediate, analysis, and synthetic) can offer various levels of utility and impact.

### Ethical and Scientific Need for Open Data

Closed data impedes cumulative science and raises ethical concerns. Researchers have an ethical responsibility to maximize the use of clinical data, as participants typically enroll in research with the expectation that their data will help answer important public health questions. In fact, studies on participants' motivations, particularly in non-experimental, observational research where there is no direct benefit, show that altruism is a key factor for participation (i.e., “I signed up because this study might be able to help future patients in my situation”) [@soulemd_etal16]. Thus, it is essential that researchers ensure participants’ data is fully utilized to benefit future patients, clinical outcomes, and scientific knowledge.

Additionally, many research questions cannot be fully answered in a single study due to limitations like small or unrepresentative samples. Sharing data extends the value of collected datasets and allows other researchers to build on previous findings. Closed data practices also place an undue burden on future participants, particularly in studies involving invasive methodologies (e.g., radiation from videofluoroscopic swallow studies or neuroimaging) or require extensive travel and time. If previously collected data is not shared, future participants may undergo unnecessary procedures to duplicate these data.

Researchers conducting publicly funded studies are also ethically obligated to return data to the public that financed their research, a responsibility increasingly emphasized by funding agencies such as the National Institutes of Health and the National Science Foundation [@wilkinson_etal16; @watson_etal23]. Even in cases where research is privately funded, participants arguably have the right to see their data shared and used to its fullest potential. In our experience, when participants are informed during the consent process about the potential for their data to be shared and reused, they overwhelmingly support data sharing to maximize the impact of their contribution.

Although some researchers may consider data availability statements like "available upon reasonable request" as a step toward data sharing, recent research has shown poor compliance with less than half of studies providing requested data [@tedersoo_etal21]. In many cases, researchers do not devote the time to properly organize their data, thereby hindering its availability when requested or may restrict access to protect their data from reuse. Moreover, purposefully vague and unclear data availability statements may exacerbate inequities in the field. This practice limits access, particularly for those with fewer resources or opportunities, and poses a direct barrier to a cumulative and transparent scientific literature.

### Benefits of Open Data

Open data offers substantial benefits for both the scientific community and researcher. For example, studies show that openly shared data is associated with higher citation rates for the original work [@piwowar_etal07; @piwowar_vision13; @drachen_etal16]. However, sharing data is not always straightforward, and researchers must consider when and what types of data to share. Figure 5 proposes a decision tree to guide researchers through the process of deciding which type of data to share. For practical guides on obtaining consent, sharing data, and ensuring FAIR data principles, we direct our readers to [@ohmann_etal17]. Different types of data can be shared, including raw, intermediate, analysis, and synthetic data (Table 1), each providing varying levels of utility and benefit.

##### Figure 5 here.

**Analysis Data.** Sharing analysis data enables others to verify results during peer review or post-publication, promoting a more transparent and reliable scientific record. This practice also allows science to be self-correcting [@vazire_holcombe22]. Analysis data is particularly important for meta-analyses, as these depend on comprehensive reporting of descriptive statistics. Unfortunately, many studies do not report all necessary details (e.g., means, standard deviations, and sample sizes) or use different statistical analyses, making it challenging to synthesize results across studies. Sharing analysis datasets can fill this gap, allowing more studies to be included in meta-analyses and resulting in more robust analyses (e.g., individual participant meta-analysis) [@eisenhauer21; @yu_romero24] and conclusions [@chow_etal23], which is especially valuable for studying low-incidence populations in fields like CSD.

**Raw or Intermediate Data.** Sharing raw or intermediate data further enhances transparency by enabling researchers to reproduce the calculations behind analysis data. Different operational definitions or analysis steps are often a barrier to inclusion in a meta-analysis. Sharing this type of data ensures that secondary analyses can be performed with alternate methodologies or operational definitions, as the field progresses. In this sense, sharing raw or intermediate data facilitates the generation of new knowledge and accelerates scientific discovery. Despite its many benefits, there are instances where sharing raw or intermediate data may not be feasible. For example, researchers may not have obtained consent from participants for data sharing, or the institutional review board may impose project-specific guidelines that restrict sharing this type of data. Even de-identified data may carry risks, as participants could be re-identified through indirect identifiers.

**Synthetic Data.** In such cases, synthetic data offers a viable alternative. Synthetic data maintains the statistical properties of the original data while protecting participant privacy, thus facilitating computational reproducibility. In this study, we demonstrated how synthetic data can be generated using the *synthpop* package in R across a wide range of datasets in the field of CSD. Recognizing that coding expertise may be a barrier for some researchers, we have also developed a free Shiny website that interfaces with *synthpop*, allowing researchers to easily generate synthetic versions of their data (https://csdsynthetic.shinyapps.io/synthetic_data_generation/).

## Moving Forward

Current training models and incentive structures are not well-equipped to promote data sharing. To encourage open science practices like data sharing, larger systemic changes are likely necessary at both the organizational level (e.g., ASHA, societies) and within academic institutions. For example, doctoral programs should offer coursework that introduces these concepts and educates future researchers on best practices for data sharing. Fortunately, many resources are available for current researchers to familiarize themselves with these practices [@lewis24]. Additionally, institutions must incentivize data sharing and recognize open science efforts as valuable scholarly contributions. Although ASHA has introduced open science badges to acknowledge these efforts, it remains unclear if this is enough to encourage large-scale participation. Ultimately, a broader cultural shift is needed in the field - from the current individualistic, siloed approach to a more collaborative and pro-social view of science.

## Conclusions

This study assessed the utility of the *synthpop* package in R for generating synthetic data in situations where sharing original data poses risks of participant re-identification. We demonstrated that synthetic data can be effectively applied across various data types and research areas within the CSD field. In most cases, the synthetic data closely matched the *p*-values and effect sizes of the original data, though a few instances showed lower agreement. Therefore, researchers using synthetic data should verify its accuracy in reproducing their original findings before sharing. Finally, we provide a framework for data sharing, emphasizing that whether researchers share raw, intermediate, analysis, or synthetic data, some form of data sharing is achievable for all. Our overarching goal is to establish data sharing as the standard practice, rather than the exception, in CSD.

\newpage

# Acknowledgements

We would like to thank the authors of the studies included in this manuscript for making their data publicly available.

<br>

**Funding**: None.

<br>

**Study Preregistration and Data Availability**: The study preregistration (https://osf.io/vhgq2) and associated data and analysis scripts (https://osf.io/yhkqf/) are publicly available on the Open Science Framework.

\newpage

# References

::: {#refs}
:::

\newpage

::: {custom-style="noIndentParagraph"}
# Table and Figure Captions

Table 1: Description of types of data.

<br>

Table 2: Characteristics of included studies by ASHA domain.

<br>

Table 3: Effect size measures and interpretation by statistical test.

<br>

Table 4: Stability of synthetic datasets across ASHA domains.

<br>

Figure 1. Visualization of data distributions from synthetic and original data for Study #1 (Curtis et al., 2023). 

*Caption*: Panel A displays the overall distribution of laryngeal vestibule residue. Panel B displays the frequency of values by bolus consistency.

<br>

Figure 2. Visualization of data distributions from synthetic and original data for Study #2 (Thompson et al., 2023). 

*Caption*: Panel A displays the distribution of vowel space area and panel B displays the distribution of speech intelligibility.

<br>

Figure 3. Distribution of log-transformed *p*-values in synthetic datasets across ASHA domains.

*Caption*: Each panel displays the distribution of log-transformed *p*-values across 100 synthetic datasets for a given ASHA domain. The dashed line indicates the threshold for statistical significance from the original study. Shaded green areas indicate synthetic *p*-values that maintained the statistical inferential result of the original study. The mean difference and standard deviation of raw *p*-values compared to the *p*-value reported in the original study is shown below each panel's title.

<br>

Figure 4. Distribution of effect sizes in synthetic datasets across ASHA domains.

*Caption*: Each panel displays the distribution of effect sizes across 100 synthetic datasets for a given ASHA domain. The dashed line indicates the effect size reported in the original study and the light blue shaded area indicates the range of the effect size categorization. The mean difference and standard deviation of the effect size compared to the result reported in the original study is shown below each panel's title.

<br>

Figure 5. Decision tree for data sharing.
:::
